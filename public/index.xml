<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DNA confesses Data speak on DNA confesses Data speak</title>
    <link>/</link>
    <description>Recent content in DNA confesses Data speak on DNA confesses Data speak</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Ming Tang</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Run Rstudio server with singularity on HPC</title>
      <link>/post/run-rstudio-server-with-singularity-on-hpc/</link>
      <pubDate>Sun, 09 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/run-rstudio-server-with-singularity-on-hpc/</guid>
      <description>

&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;

&lt;p&gt;Please read the following before go ahead:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;what is &lt;a href=&#34;https://www.docker.com/&#34; target=&#34;_blank&#34;&gt;docker&lt;/a&gt;?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;what is &lt;a href=&#34;https://www.rocker-project.org/&#34; target=&#34;_blank&#34;&gt;Rocker&lt;/a&gt;?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;what is &lt;a href=&#34;https://www.sylabs.io/docs/&#34; target=&#34;_blank&#34;&gt;singularity&lt;/a&gt;?&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;from Harvard Research computing website: &lt;a href=&#34;https://www.rc.fas.harvard.edu/resources/documentation/software/singularity-on-odyssey/&#34; target=&#34;_blank&#34;&gt;Odyssey has singularity installed&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why Singularity?
There are some important differences between Docker and Singularity:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Docker and Singularity have their own container formats.&lt;/li&gt;
&lt;li&gt;Docker containers may be imported to run via Singularity.&lt;/li&gt;
&lt;li&gt;Docker containers need root privileges for full functionality which is not suitable for a shared HPC environment.&lt;/li&gt;
&lt;li&gt;Singularity allows working with containers as a regular user. No sudo is required,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our &lt;a href=&#34;https://informatics.fas.harvard.edu/&#34; target=&#34;_blank&#34;&gt;informatics group&lt;/a&gt; has several big memory (1TB) computing nodes that allow us to run interactive jobs. I want to have big memory to run Rstudio for my scRNAseq data.&lt;/p&gt;

&lt;h3 id=&#34;run-rstudio-server-with-singularity&#34;&gt;Run Rstudio server with singularity&lt;/h3&gt;

&lt;p&gt;I basically followed this tutorial &lt;a href=&#34;https://www.rocker-project.org/use/singularity/&#34; target=&#34;_blank&#34;&gt;https://www.rocker-project.org/use/singularity/&lt;/a&gt; written by my colleague Nathan Weeks sitting in the same office with me. Thanks!&lt;/p&gt;

&lt;p&gt;First, go to &lt;a href=&#34;https://www.rocker-project.org/images/&#34; target=&#34;_blank&#34;&gt;https://www.rocker-project.org/images/&lt;/a&gt; choose the image you want. I use &lt;code&gt;tidyverse&lt;/code&gt; heavily, so I downloaded the &lt;code&gt;tidyverse&lt;/code&gt; image buit upon &lt;code&gt;Rstudio&lt;/code&gt; image&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## ssh to remote HPC and pull the docker image by singularity
ssh bio1
mkdir singularity-images; cd !$
singularity pull --name rstudio.simg docker://rocker/tidyverse:latest


# This example bind mounts the /project directory on the host into the Singularity container.
# By default the only host file systems mounted within the container are $HOME, /tmp, /proc, /sys, and /dev.
# type in the password you want to set, make it more complicated than this dummy one
PASSWORD=&#39;xyz&#39; singularity exec --bind=/project  rstudio.simg rserver --auth-none=0  --auth-pam-helper-path=pam-helper --www-address=127.0.0.1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;back to my mac and connect to it via &lt;a href=&#34;https://www.ssh.com/ssh/tunneling/&#34; target=&#34;_blank&#34;&gt;SSH tunnel&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;Nathan explained by drawing the following.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/ssh_tunnel.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh -Nf -L 8787:localhost:8787 bio1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;on my local mac, open &lt;code&gt;localhost:8787&lt;/code&gt; in web browser, type in the Odyssey (HPC) user name and password (xyz in this dummy example). Rstudio server now is ready for me! Magic!!!&lt;/p&gt;

&lt;p&gt;Note: if mulitple people using the same node for Rstudio sever, you will need to pick a different
port than &lt;code&gt;8787&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;one-more-note-on-r-packages&#34;&gt;One more note on R packages&lt;/h3&gt;

&lt;p&gt;create an &lt;code&gt;.Renviron&lt;/code&gt; file in your home diretory&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# User-installed R packages go into their home directory
echo &#39;R_LIBS_USER=~/R/%p-library/%v&#39; &amp;gt;&amp;gt; ${HOME}/.Renviron
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The platform and version will be replaced by the corresponding R versions&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls ~/R/x86_64-pc-linux-gnu-library/
3.6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;install packages inside Rstudio and the packages will be installed to &lt;code&gt;~/R/x86_64-pc-linux-gnu-library/3.6&lt;/code&gt;. R version in this singularity image is R3.6. Note that if you use R on command line at the remote machine and use the same version of R. the library may not be compatible. e.g. singularity container is based on debian （Ubuntu) and HPC is based on RPM (CentOS). One may need to have mulitiple &lt;code&gt;.Renviron&lt;/code&gt; file and switch back and forth depending on which R one is using. If you have better options, please let me know!&lt;/p&gt;

&lt;h3 id=&#34;jump-to-other-folders&#34;&gt;Jump to other folders&lt;/h3&gt;

&lt;p&gt;by default, Rstudio opens the home directory. if you want to go to other folders, you can click &lt;code&gt;...&lt;/code&gt; in the file pane.
You can then type in the path you want to jump to.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/change_path.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;submit-a-slurm-job&#34;&gt;Submit a slurm job&lt;/h3&gt;

&lt;p&gt;If you do not have a big computing node that you can run interactive job, you can follow Nathan&amp;rsquo;s &lt;a href=&#34;https://www.rocker-project.org/use/singularity/&#34; target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt; on how to submit slurm job to run Rstudio server with singularity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calculate scATACseq TSS enrichment score</title>
      <link>/post/calculate-scatacseq-tss-enrichment-score/</link>
      <pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/calculate-scatacseq-tss-enrichment-score/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://www.encodeproject.org/data-standards/terms/#enrichment&#34; target=&#34;_blank&#34;&gt;TSS enrichment score&lt;/a&gt; serves as an important quality control metric for ATACseq data. I want to write a script for single cell ATACseq data.&lt;/p&gt;

&lt;p&gt;From the Encode page:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Transcription Start Site (TSS) Enrichment Score - The TSS enrichment calculation is a signal to noise calculation. The reads around a reference set of TSSs are collected to form an aggregate distribution of reads centered on the TSSs and extending to 1000 bp in either direction (for a total of 2000bp). This distribution is then normalized by taking the average read depth in the 100 bps at each of the end flanks of the distribution (for a total of 200bp of averaged data) and calculating a fold change at each position over that average read depth. This means that the flanks should start at 1, and if there is high read signal at transcription start sites (highly open regions of the genome) there should be an increase in signal up to a peak in the middle. We take the signal value at the center of the distribution after this normalization as our TSS enrichment metric. Used to evaluate ATAC-seq.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It was not so clear to me from the definition on how &lt;strong&gt;EXACTLY&lt;/strong&gt; this score is calculated.&lt;/p&gt;

&lt;p&gt;I inspected the &lt;a href=&#34;https://github.com/jianhong/ATACseqQC/blob/master/R/TSSEscore.R#L80&#34; target=&#34;_blank&#34;&gt;source code&lt;/a&gt; of  &lt;code&gt;ATACseqQC&lt;/code&gt; which calculates the TSS enrichment score for bulk ATACseq data, but I think it is not calculating it the right way as described by the ENCODE page.&lt;/p&gt;

&lt;p&gt;I reached out to &lt;a href=&#34;https://twitter.com/Satpathology&#34; target=&#34;_blank&#34;&gt;Ansu Satpathy&lt;/a&gt; (thanks!), and got a script written by Jeffrey Granja, who are the authors of this recent scATACseq paper:
&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/610550v1&#34; target=&#34;_blank&#34;&gt;Massively parallel single-cell chromatin landscapes of human immune cell development and intratumoral T cell exhaustion (2019)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I studied the script and also got the confirmation from ENCODE how they calculate the TSS enrichment score
&lt;a href=&#34;https://github.com/ENCODE-DCC/atac-seq-pipeline/issues/50&#34; target=&#34;_blank&#34;&gt;https://github.com/ENCODE-DCC/atac-seq-pipeline/issues/50&lt;/a&gt; by a python script.&lt;/p&gt;

&lt;p&gt;To work with this coverage type of data in R, I want to take advantage of the data structure &lt;code&gt;View&lt;/code&gt; in bioconductor, so I borrowed some codes from &lt;a href=&#34;https://bioconductor.org/packages/release/bioc/vignettes/genomation/inst/doc/GenomationManual.html&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;genomation::ScoreMatrix&lt;/code&gt;&lt;/a&gt; instead of using the script sent by Ansu. It is a very nice package by &lt;a href=&#34;https://twitter.com/AltunaAkalin&#34; target=&#34;_blank&#34;&gt;Altuna Akalin&lt;/a&gt;. A side note, he has a very nice book you might be interested in: &lt;a href=&#34;http://compgenomr.github.io/book/how-to-contribute.html&#34; target=&#34;_blank&#34;&gt;Computational Genomics with R&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Below, I ended up with a hybrid script from multiple sources.
Now it works with the 10x cellranger-atac output &lt;code&gt;fragment.tsv.gz&lt;/code&gt;. One can tweak it to work with the bam file. However, the bam file is 25G, R takes a long time to parse it.&lt;/p&gt;

&lt;p&gt;I explain what the script does:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;for each TSS, get per base coverage for the 1000 bp flanking region(flank = 1000).&lt;/li&gt;
&lt;li&gt;do this for all TSSs, We get a matrix of #TSS x 2000 bp dimension.&lt;/li&gt;
&lt;li&gt;do a column sum of the matrix.&lt;/li&gt;
&lt;li&gt;sum of the coverage of the endFlank (100bp) at both ends and divide by 200 bp to get a
normalization factor.&lt;/li&gt;
&lt;li&gt;divide the the normalization factor for -1900 to + 1900 bp to get per base normalized coverage.&lt;/li&gt;
&lt;li&gt;do a smoothing with a defined window (50bp by default) using &lt;code&gt;zoo::rollmean&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;select the highest value within a window (highest_tss_flank, 50 bp by default) around the TSS because the highest peak is not necessary at exactly the TSS site (position 0)&lt;/li&gt;
&lt;li&gt;repeat 1-7 for all cells.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;extra-technical-notes&#34;&gt;Extra technical notes:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;One thing to note is that one needs to filter out the TSSs which are not within the coverage. e.g. A TSS with 1000 bp flanking regions fall out of the coverage.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;use only the common chromosomes between coverage and the txs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;convert GRanges to IntergerRangesList does not maintain the order of the GRanges.
so a unique id was given for each Ranges, and the matrix can be reordered according to this unique id. That&amp;rsquo;s what &lt;code&gt;constrainRanges()&lt;/code&gt; does. read this thread for more &lt;a href=&#34;https://stat.ethz.ch/pipermail/bioc-devel/2016-June/009433.html&#34; target=&#34;_blank&#34;&gt;https://stat.ethz.ch/pipermail/bioc-devel/2016-June/009433.html&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-long-it-takes&#34;&gt;How long it takes.&lt;/h3&gt;

&lt;p&gt;It took me around ~15 seconds to calculate the TSS enrichment score for a single cell.
1.213291 hours for 5000 PBMC cells using 15 workers (not too bad :).&lt;/p&gt;

&lt;h3 id=&#34;r-code&#34;&gt;R code&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(GenomicRanges)
library(dplyr)

#&#39; checkClass function
#&#39; 
#&#39; check whether the x object corresponds to the given class
#&#39;
#&#39; @param x object
#&#39; @param class.name class name
#&#39; @param var.name uses x object
#&#39; @keywords internal
checkClass = function(x, class.name, var.name = deparse(substitute(x))){
  
  fun.name = match.call(call=sys.call(sys.parent(n=1)))[[1]]
  if(!class(x) %in% class.name)
    stop(paste(fun.name,&#39;: &#39;, 
               var.name, 
               &#39; is not of class: &#39;, 
               paste(class.name, collapse=&#39; &#39;), 
               &#39;\n&#39;, sep=&#39;&#39;))
}

### remove the tss that do not have coverage
### I took some code from the ScoreMatrix.R function in the genomation package.
### give the credit due :)
### see https://github.com/BIMSBbioinfo/genomation/blob/master/R/scoreMatrix.R#L113
constrainRanges = function(target, windows){
  
  checkClass(target, c(&#39;SimpleRleList&#39;,&#39;RleList&#39;,&#39;CompressedRleList&#39;))
  checkClass(windows, &#39;GRanges&#39;)
  
  mcols(windows)$X_rank = 1:length(windows)
  r.chr.len = elementNROWS(target)
  constraint = GRanges(seqnames=names(r.chr.len),
                       IRanges(start=rep(1,length(r.chr.len)),
                               end=as.numeric(r.chr.len)))
  # suppressWarnings is done becuause GenomicRanges function give warnings 
  #if you don&#39;t have the same seqnames in both objects
  win.list.chr = suppressWarnings(subsetByOverlaps(windows, 
                                                   constraint,
                                                   type = &amp;quot;within&amp;quot;,
                                                   ignore.strand = TRUE))
  
  if(length(win.list.chr) == 0)
    stop(&#39;All windows fell have coordinates outside windows boundaries&#39;)
  return(win.list.chr)
}



#&#39; Calculate tss enrichment score from 10xscATAC fragment file
#&#39;
#&#39; @param frag_gz_file  fragment.tsv.gz file from 10x cellranger-atac output or 
#&#39; anyother tool but in the same format.
#&#39; @param txs  a txdb object
#&#39; @param flank flanking bp of tss (upstream and downstream)
#&#39; @param endFlank  bp end flanks of flank for local noise control
#&#39;     flank               flank
#&#39;  ---------------|-----------------
#&#39;                tss
#&#39;  ---                           ---
#&#39;  endFlank                     endFlank
#&#39;  
#&#39; @param highest_tss_flank bp flanking tss windown for choosing the highest tss score.
#&#39; The highest tss enrichment score is not always exactly at tss.
#&#39; @param barcodeList valid barcode list, a file with one column 
#&#39; @param smooth window size to smooth 
#&#39; @param strand.aware consider tss strandness when calculating 
#&#39;
#&#39; @return
#&#39; @export
#&#39;
#&#39; @examples
#&#39; library(TxDb.Hsapiens.UCSC.hg19.knownGene)
#&#39; library(dplyr); library(readr); library(BiocParallel)
#&#39; txs &amp;lt;- transcripts(TxDb.Hsapiens.UCSC.hg19.knownGene)
#&#39; scores&amp;lt;- TssEnrichmentFromFrags(&amp;quot;fragment.tsv.gz&amp;quot;, txs = txs)

TssEnrichmentFromFrags &amp;lt;- function(frag_gz_file,
                               txs,
                               flank = 1000,
                               endFlank = 100,
                               highest_tss_flank= 50,
                               smooth = 50,
                               strand.aware = TRUE,
                               workers = 1,
                               barcodeList = NULL){
        
        # Make GRanges of fragments that are solid for the cells that we care about
        frags_valid &amp;lt;- data.table::fread(paste0(&amp;quot;zcat &amp;lt; &amp;quot;, frag_gz_file)) %&amp;gt;% 
                data.frame() %&amp;gt;% 
                mutate(V2 = V2 + 1) %&amp;gt;% # make it 1 based for R
                GenomicRanges::makeGRangesFromDataFrame(seqnames.field = &amp;quot;V1&amp;quot;, start.field = &amp;quot;V2&amp;quot;, end.field = &amp;quot;V3&amp;quot;, keep.extra.columns = TRUE)
        if (!is.null(barcodeList)){
                validBarcodes&amp;lt;- read_tsv(barcodeList, col_names = F)
                frags_valid&amp;lt;- frags_valid[frags_valid$V4 %in% validBarcodes$X1]
        }
        
        # common chromosome names, do it per cell instead, see TssEnrichmentSingleCell
        seqlev&amp;lt;- intersect(seqlevels(frags_valid), seqlevels(txs))
        frags_valid&amp;lt;- keepSeqlevels(frags_valid, seqlev, pruning.mode=&amp;quot;coarse&amp;quot;)
        
        # calculate coverage per cell
        frags_valid_per_cell&amp;lt;- split(frags_valid, frags_valid$V4)
        
       
        # this step can take minutes 
        multicoreParam &amp;lt;- BiocParallel::MulticoreParam(workers = workers)
        # can add the chromosome length as additional argument for `coverage`
        # to get 0 coverages if there are no reads there. 
        cvgs&amp;lt;- bplapply(frags_valid_per_cell, function(x) coverage(x), BPPARAM = multicoreParam)
        
        txs &amp;lt;- unique(txs)
        
        txs.flanks&amp;lt;- promoters(txs, upstream = flank, 
                            downstream = flank)
        txs.length&amp;lt;- length(txs.flanks)
        
        TssEnrichmentScores&amp;lt;- BiocParallel::bplapply(cvgs, TssEnrichmentSingleCell, txs.flanks, BPPARAM = multicoreParam)

        enrichment&amp;lt;- do.call(&amp;quot;rbind&amp;quot;, TssEnrichmentScores)
        return(enrichment)
}    

TssEnrichmentSingleCell&amp;lt;- function(cvg, txs.flanks){
        ## remove tss not in the coverage and assign a unique id for each tss: X_rank
        txs.flanks&amp;lt;- constrainRanges(cvg, txs.flanks)
        txs.length&amp;lt;- length(txs.flanks)
        if(length(txs.flanks)!=txs.length){
              warning(paste0(txs.length-length(txs.flanks),
                             &amp;quot; Tss removed because they fall out of the coverage&amp;quot;))
            }
        # common chromosomes
        chrs&amp;lt;- sort(intersect(names(cvg), as.character(unique(seqnames(txs.flanks)))))
        
        # convert GRanges to IntergerRangesList does not maintain the order
        # a unique id was given for each Ranges
        myViews&amp;lt;- Views(cvg[chrs],as(txs.flanks,&amp;quot;IntegerRangesList&amp;quot;)[chrs]) # get subsets of RleList
        mat = lapply(myViews,function(x) t(viewApply(x,as.vector)) )
        mat = do.call(&amp;quot;rbind&amp;quot;,mat)
        
        r.list=split(mcols(txs.flanks)[,&amp;quot;X_rank&amp;quot;], as.vector(seqnames(txs.flanks))  )
        r.list=r.list[order(names(r.list))]
        ranks=do.call(&amp;quot;c&amp;quot;,r.list)
        rownames(mat) = ranks
        
        if(strand.aware == TRUE){
              orig.rows=txs.flanks[strand(txs.flanks) == &#39;-&#39;,]$X_rank
              mat[rownames(mat) %in% orig.rows,] = mat[rownames(mat) %in% 
                                                         orig.rows, ncol(mat):1]
        }
        
        # reorder according to the original Granges (txs)
        mat = mat[order(ranks),]
        
  
        ### normlization by the endFlank local noise
        profile &amp;lt;- colSums(mat)
        profile_norm &amp;lt;- profile/mean(profile[c(1:endFlank,(flank*2-endFlank+1):(flank*2))])

        #smooth
        profile_norm_smooth &amp;lt;- zoo::rollmean(profile_norm, smooth, fill = 1)
        

        #enrichment
        max_finite &amp;lt;- function(x){
        suppressWarnings(max(x[is.finite(x)], na.rm=TRUE))
        }
        
        e &amp;lt;- max_finite(profile_norm_smooth[(flank-highest_tss_flank):(flank+highest_tss_flank)])
        return(e)
}


&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>clustering scATACseq data: the TF-IDF way</title>
      <link>/post/clustering-scatacseq-data-the-tf-idf-way/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/clustering-scatacseq-data-the-tf-idf-way/</guid>
      <description>&lt;p&gt;scATACseq data are very sparse. It is sparser than scRNAseq. To do clustering of
scATACseq data, there are some preprocessing steps need to be done.&lt;/p&gt;
&lt;p&gt;I want to reproduce what has been done after reading the method section of these two recent scATACseq paper:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/30078704&#34;&gt;A Single-Cell Atlas of In Vivo Mammalian Chromatin Accessibility&lt;/a&gt;
Darren et.al Cell 2018&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Latent Semantic Indexing Cluster Analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to get an initial sense of the relationship between individual cells, &lt;strong&gt;we first broke the genome into 5kb windows and then scored each cell for any insertions in these windows, generating a large, sparse, binary matrix of 5kb windows by cells for each tissue.&lt;/strong&gt; Based on this matrix, we retained the top 20,000 most commonly used sites in each tissue (this number could extend a little above 20,000 because we included tied sites at the threshold) and then filtered out the bottom 5% of cells in terms of the number of 5kb windows with any insertions. We then reduced the dimensionality of these large binary matrices using a term &lt;strong&gt;frequency-inverse document frequency (‘‘TF-IDF’’) transformation.&lt;/strong&gt; To do this, we first weighted all the sites for individual cells by the total number of sites accessible in that cell (‘‘term frequency’’). We then multiplied these weighted values by log(1 + the inverse frequency of each site across all cells), the ‘‘inverse document frequency.’’ We then used singular value decomposition on the TF-IDF matrix to generate a lower dimensional representation of the data by only retaining the 2nd through 10th dimensions (because the first dimension tends to be highly correlated with read depth). These LSI-transformed scores of accessibility were then standardized by row (i.e., mean subtracted and divided by standard deviation), capped at ± 1.5, and used to bi-cluster cells and windows based on cosine distances using the ward algorithm in R. Visual examination of the resulting heatmaps identified between 2 and 7 distinct clusters of cells, de- pending on the tissue. These relatively crude groups of cells were used for peak calling (described below) to maintain enough cells in each group for identifying peaks while also retaining sufficient sensitivity to identify peaks that were restricted to subset of cells.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;t-distributed Stochastic Neighbor Embedding and Iterative Cluster Analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To take a more holistic approach to understanding the relationships of different cell types across the entire dataset, we combined all cells from all tissues and used the t-distributed stochastic neighbor embedding dimensionality reduction technique to visualize the full dataset and identify clusters of cells representing individual cell types. &lt;strong&gt;As with the LSI analysis above, we started by generating a large binary matrix of sites by cells, but instead of scoring cells for reads overlapping 5kb windows in the genome we scored cells for reads overlapping the master list of potential regulatory elements we had previously identified based on LSI clusters.&lt;/strong&gt; Starting with all cells that passed our nucleosome signal and read depth thresholds, we again wanted to remove the most sparsely sampled sites and cells to more clearly define differences between cell types. To do so, we first filtered out any sites that were not observed as accessible in at least 5% of cells in at least one LSI cluster and then filtered out cells that were more than 1 standard deviation below the mean number of sites observed. We then transformed this matrix with the TF-IDF algorithm described above. Finally, we generated a lower dimen- sional representation of the data by including the first 50 dimensions of the singular value decomposition of this TF-IDF-transformed matrix. This representation was then used as input for the Rtsne package in R (Krijthe, 2015). To identify clusters of cells in this two dimensional representation of the data, we used the Louvain clustering algorithm implemented in Seurat (Satija et al., 2015). Resolu- tion and K parameters for Louvain clustering were chosen for each major cluster to produce reasonable groupings of cells that are well- separated in each t-SNE embedding. This analysis identified 30 distinct clusters of cells, but to get at even finer structure, we subset TF-IDF normalized data on each of these 30 clusters of cells and repeated SVD and t-SNE to identify subclusters, again using Louvain clustering. Through this round of ‘‘iterative’’ t-SNE, we identified a total of 85 distinct clusters. Note that for one major cluster, major cluster 12, we found that Monocle 20s implementation of density peak clustering (Qiu et al., 2017; Trapnell et al., 2014) seemed to produce more reasonable clusters. Rho and delta parameters were set in the same manner as for Louvain clustering.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/610550v1&#34;&gt;Massively parallel single-cell chromatin landscapes of human immune cell development and intratumoral T cell exhaustion&lt;/a&gt;
Ansuman et.al 2019 biorxiv&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;ATAC-seq-centric Latent Semantic Indexing clustering and visualization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We clustered scATAC-seq data using an approach that does not require bulk data or prior
knowledge. To achieve this, we adopted the strategy by Cusanovich et. al9, to compute
the term frequency-inverse document frequency (“TF-IDF”) transformation. Briefly we
divided each index by the colSums of the matrix to compute the cell “term frequency.”
Next, we multiplied these values by log(1 + ncol(matrix) / rowSums(matrix)), which
represents the “inverse document frequency.” This resulted in a TF-IDF matrix that was
used as input to irlba’s singular value decomposition (SVD) implementation in R. We then
used the first 50 reduced dimensions as input into a Seurat object and then crude clusters
were identified by using Seurat’s (v2.3) SNN graph clustering “FindClusters” with a default
resolution of 0.8. We found that there was detectable batch effect that confounded further
analyses. To attenuate this batch effect, we calculated the cluster sums from the binarized
accessibility matrix and then log-normalized by using edgeR’s “cpm(matrix , log = TRUE,
prior.count = 3)” in R. Next, we identified the top 25,000 varying peaks across all clusters
using “rowVars” in R. This was done on the cluster log-normalized matrix vs the sparse
binary matrix because: (1) it reduced biases due to cluster cell sizes, and (2) it attenuated
the mean-variability relationship by converting to log space with a scaled prior count.
These 25,000 variable peaks were then used to subset the sparse binarized accessibility
matrix and recomputed the “TF-IDF” transform. We used singular value decomposition
on the TF-IDF matrix to generate a lower dimensional representation of the data by
retaining the first 50 dimensions. We then used these reduced dimensions as input into
a Seurat object and then crude clusters were identified by using Seurat’s (v2.3) SNN
graph clustering “FindClusters” with a default resolution of 0.8. These same reduced
dimensions were used as input to Seurat’s “RunUMAP” with default parameters and
plotted in ggplot2 using R&lt;/p&gt;
&lt;p&gt;Both papers used the so called &lt;code&gt;Latent Semantic Indexing&lt;/code&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Latent_semantic_analysis&#34;&gt;LSI method&lt;/a&gt; and used a transformation of the
binarized scATAC count matrix called ’TF-IDF` (term frequency–inverse document frequency) which is
used in text mining. TF-IDF can be used for scRNAseq data as well. see &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6101073/&#34;&gt;Single cell RNA-seq data clustering using TF-IDF based methods&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The transformation is not complicated as described above:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Briefly we divided each index by the colSums of the matrix to compute the cell “term frequency.”
Next, we multiplied these values by log(1 + ncol(matrix) / rowSums(matrix)), which
represents the “inverse document frequency.” This resulted in a TF-IDF matrix&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;Seurat&lt;/code&gt; version 3 has a function called &lt;a href=&#34;https://github.com/satijalab/seurat/blob/master/R/preprocessing.R#L1253&#34;&gt;&lt;code&gt;TF.IDF&lt;/code&gt;&lt;/a&gt; for that purpose.&lt;/p&gt;
&lt;p&gt;But note that, it does not do the log transformation in this function, but do it at &lt;a href=&#34;https://github.com/satijalab/seurat/blob/master/R/dimensional_reduction.R#L669&#34; class=&#34;uri&#34;&gt;https://github.com/satijalab/seurat/blob/master/R/dimensional_reduction.R#L669&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I will first show you the long way to do the clustering through which I want to gain some more
deep understanding of the whole process and I will show you how to use &lt;code&gt;Seurat&lt;/code&gt; V3 for that.&lt;/p&gt;
&lt;p&gt;I am going to use the 10k pbmc scATAC data from 10x for demonstration. You can download the data from
&lt;a href=&#34;https://support.10xgenomics.com/single-cell-atac/datasets/1.1.0/atac_v1_pbmc_10k&#34; class=&#34;uri&#34;&gt;https://support.10xgenomics.com/single-cell-atac/datasets/1.1.0/atac_v1_pbmc_10k&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;the-long-way&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The long way&lt;/h3&gt;
&lt;p&gt;read in the sparse matrix&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Matrix)
library(readr)
library(dplyr)
mat&amp;lt;- readMM(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/matrix.mtx&amp;quot;)
peaks&amp;lt;- read_tsv(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/peaks.bed&amp;quot;, col_names = F)
peaks&amp;lt;- peaks %&amp;gt;%
        mutate(id1 = paste(X2, X3, sep = &amp;quot;-&amp;quot;)) %&amp;gt;%
        mutate(id = paste(X1, id1, sep = &amp;quot;:&amp;quot;))
        
barcodes&amp;lt;- read_tsv(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/barcodes.tsv&amp;quot;, col_names =F)

rownames(mat)&amp;lt;- peaks$id1
colnames(mat)&amp;lt;- barcodes$X1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;binarize the data and do TF-IDF transformation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# binarize the matrix 
mat@x[mat@x &amp;gt;0]&amp;lt;- 1 

TF.IDF.custom &amp;lt;- function(data, verbose = TRUE) {
  if (class(x = data) == &amp;quot;data.frame&amp;quot;) {
    data &amp;lt;- as.matrix(x = data)
  }
  if (class(x = data) != &amp;quot;dgCMatrix&amp;quot;) {
    data &amp;lt;- as(object = data, Class = &amp;quot;dgCMatrix&amp;quot;)
  }
  if (verbose) {
    message(&amp;quot;Performing TF-IDF normalization&amp;quot;)
  }
  npeaks &amp;lt;- Matrix::colSums(x = data)
  tf &amp;lt;- t(x = t(x = data) / npeaks)
  # log transformation
  idf &amp;lt;- log(1+ ncol(x = data) / Matrix::rowSums(x = data))
  norm.data &amp;lt;- Diagonal(n = length(x = idf), x = idf) %*% tf
  norm.data[which(x = is.na(x = norm.data))] &amp;lt;- 0
  return(norm.data)
}


mat&amp;lt;- TF.IDF.custom(mat)

# what&amp;#39;s the range after transformation?
range(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.00000000 0.01111942&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 89796  8728&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dimension reduction with SVD, use &lt;code&gt;irlba::irlba&lt;/code&gt; for approximated calculation.&lt;/p&gt;
&lt;p&gt;Note: &lt;code&gt;svd&lt;/code&gt; singular value decomposition gives the same results as &lt;code&gt;prcomp&lt;/code&gt;for exact PC calculation.
see my previous &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/post/pca-in-action/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(irlba)
set.seed(123)
mat.lsi&amp;lt;- irlba(mat, 50)

d_diagtsne &amp;lt;- matrix(0, 50, 50)
diag(d_diagtsne) &amp;lt;- mat.lsi$d
mat_pcs &amp;lt;- t(d_diagtsne %*% t(mat.lsi$v))
dim(mat_pcs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8728   50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## PCA plot PC1 vs PC2
plot(mat_pcs[,1], mat_pcs[,2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rownames(mat_pcs)&amp;lt;- colnames(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;clustering in the PCA space using KNN.&lt;/p&gt;
&lt;p&gt;I took some code from &lt;a href=&#34;https://jef.works/blog/2017/09/13/graph-based-community-detection-for-clustering-analysis/&#34;&gt;Jean Fan’s blog post&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RANN)
knn.info&amp;lt;- RANN::nn2(mat_pcs, k = 30)

## convert to adjacancy matrix
knn &amp;lt;- knn.info$nn.idx

adj &amp;lt;- matrix(0, nrow(mat_pcs), nrow(mat_pcs))
rownames(adj) &amp;lt;- colnames(adj) &amp;lt;- rownames(mat_pcs)

for(i in seq_len(nrow(mat_pcs))) {
    adj[i,rownames(mat_pcs)[knn[i,]]] &amp;lt;- 1
}

## convert to graph
library(igraph)
g &amp;lt;- igraph::graph.adjacency(adj, mode=&amp;quot;undirected&amp;quot;)
g &amp;lt;- simplify(g) ## remove self loops

## identify communities, many algorithums. Use the Louvain clustering

km &amp;lt;- igraph::cluster_louvain(g)

com &amp;lt;- km$membership
names(com) &amp;lt;- km$names

# cluster id for each barcode
head(com)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AAACGAAAGAGCGAAA-1 AAACGAAAGAGTTTGA-1 AAACGAAAGCGAGCTA-1 
##                  7                 14                  2 
## AAACGAAAGGCTTCGC-1 AAACGAAAGTGCTGAG-1 AAACGAACAAGGGTAC-1 
##                 11                  1                 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## total 13 clusters
table(com)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## com
##    1    2    3    4    5    6    7    8    9   10   11   12   13   14 
## 1776  389  482   34 1100  520  491  640  781  487  204  888  173  763&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;t-SNE for visualization&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rtsne)
library(ggplot2)
library(tibble)
set.seed(345)

mat_tsne&amp;lt;- Rtsne(mat_pcs,  dims = 2, perplexity = 30, verbose = TRUE, 
               max_iter = 1000, check_duplicates = FALSE, is_distance = FALSE, 
               theta = 0.5, pca = FALSE, exaggeration_factor = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Read the 8728 x 50 data matrix successfully!
## OpenMP is working. 1 threads.
## Using no_dims = 2, perplexity = 30.000000, and theta = 0.500000
## Computing input similarities...
## Building tree...
## Done in 1.81 seconds (sparsity = 0.015574)!
## Learning embedding...
## Iteration 50: error is 94.027001 (50 iterations in 1.63 seconds)
## Iteration 100: error is 80.430931 (50 iterations in 1.34 seconds)
## Iteration 150: error is 77.384844 (50 iterations in 1.10 seconds)
## Iteration 200: error is 76.435871 (50 iterations in 1.12 seconds)
## Iteration 250: error is 75.985857 (50 iterations in 1.16 seconds)
## Iteration 300: error is 2.655848 (50 iterations in 1.02 seconds)
## Iteration 350: error is 2.321504 (50 iterations in 1.02 seconds)
## Iteration 400: error is 2.140627 (50 iterations in 1.05 seconds)
## Iteration 450: error is 2.024543 (50 iterations in 1.06 seconds)
## Iteration 500: error is 1.944114 (50 iterations in 1.06 seconds)
## Iteration 550: error is 1.884803 (50 iterations in 1.10 seconds)
## Iteration 600: error is 1.840703 (50 iterations in 1.14 seconds)
## Iteration 650: error is 1.806387 (50 iterations in 1.06 seconds)
## Iteration 700: error is 1.780991 (50 iterations in 1.07 seconds)
## Iteration 750: error is 1.761708 (50 iterations in 1.07 seconds)
## Iteration 800: error is 1.747014 (50 iterations in 1.10 seconds)
## Iteration 850: error is 1.735953 (50 iterations in 1.07 seconds)
## Iteration 900: error is 1.728716 (50 iterations in 1.11 seconds)
## Iteration 950: error is 1.725798 (50 iterations in 1.13 seconds)
## Iteration 1000: error is 1.724810 (50 iterations in 1.21 seconds)
## Fitting performed in 22.62 seconds.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_tsne&amp;lt;- as.data.frame(mat_tsne$Y)
colnames(df_tsne)&amp;lt;- c(&amp;quot;tSNE1&amp;quot;, &amp;quot;tSNE2&amp;quot;)
df_tsne$barcode&amp;lt;- rownames(mat_pcs)

df_tsne&amp;lt;- left_join(df_tsne, enframe(com), by = c(&amp;quot;barcode&amp;quot; = &amp;quot;name&amp;quot;)) %&amp;gt;%
        dplyr::rename(cluster = value) %&amp;gt;%
        mutate(cluster = as.factor(cluster))


ggplot(df_tsne, aes(x = tSNE1, y = tSNE2)) + 
        geom_point(aes(col = cluster), size = 0.5) +
        theme_bw(base_size = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks pretty good :)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-easier-way-use-seurat&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The easier way: use Seurat&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Seurat)
peaks &amp;lt;- Read10X_h5(filename = &amp;quot;/Users/mingtang/github_repos/blogdown_data/atac_v1_pbmc_10k_filtered_peak_bc_matrix.h5&amp;quot;)

# binarize the matrix
peaks@x[peaks@x &amp;gt;0]&amp;lt;- 1 

## create a seurat object
atac.lsi &amp;lt;- CreateSeuratObject(counts = peaks, assay = &amp;#39;ATAC&amp;#39;, project = &amp;#39;10k_pbmc&amp;#39;)

atac.lsi &amp;lt;- RunLSI(object = atac.lsi, n = 50, scale.max = NULL)

# atac.lsi@reductions

atac.lsi&amp;lt;- FindNeighbors(atac.lsi, reduction = &amp;quot;lsi&amp;quot;, dims = 1:50)
atac.lsi&amp;lt;- FindClusters(atac.lsi, resolution = 0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck
## 
## Number of nodes: 8728
## Number of edges: 246454
## 
## Running Louvain algorithm...
## Maximum modularity in 10 random starts: 0.9129
## Number of communities: 20
## Elapsed time: 0 seconds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;atac.lsi &amp;lt;- RunTSNE(object = atac.lsi, reduction = &amp;quot;lsi&amp;quot;, dims = 1:50)
DimPlot(object = atac.lsi, reduction = &amp;#39;tsne&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You may argue those two t-SNE graphs look very different in terms of number of clusters
and the shape of the clusters. And I agree. There are many reasons for that.
I hope &lt;code&gt;Seurat&lt;/code&gt; team can give some insights.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The &lt;code&gt;TF-IDF&lt;/code&gt; function in &lt;code&gt;Seurat&lt;/code&gt; does not do log transformation
as in the papers: &lt;code&gt;idf &amp;lt;- log(1+ ncol(x = data) / Matrix::rowSums(x = data))&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;but rather do a log transformation &lt;a href=&#34;https://github.com/satijalab/seurat/blob/master/R/dimensional_reduction.R#L669&#34;&gt;later&lt;/a&gt;: &lt;code&gt;tf.idf &amp;lt;- LogNorm(data = tf.idf, display_progress = verbose, scale_factor = 1e4)&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I am not an expert in the graph clustering, but the clustering algorithm in
&lt;code&gt;Seurat&lt;/code&gt; is probably not exactly the same with &lt;code&gt;igraph::cluster_louvain&lt;/code&gt;.
Moreover, one can always tweak the k.param and resolution parameters, and the cluster number changes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can compare the cell identities for each cluster&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# https://github.com/crazyhottommy/scclusteval
library(scclusteval)

# takes two named vector, and calculate the pairwise Jaccard similarity score
# for all clusters
PairWiseJaccardSetsHeatmap(com, Idents(atac.lsi))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-other-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some other notes&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;It is known that first dimension is correlated with sequencing depth (although Ansuman et.al did not find such). Nevertheless, if you see such correlation, when cluster
cells in the PC space, you can exclude the first PC. e.g. &lt;code&gt;atac.lsi&amp;lt;- FindNeighbors(atac.lsi, reduction = &amp;quot;lsi&amp;quot;, dims = 2:50)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I did not do &lt;code&gt;LSI&lt;/code&gt; first for crude clustering using the titled 5kb genome bin matrix and call peaks for each crude cluster and then get the count matrix per peak per cell. I am not sure how much this extra work can benefit the clustering.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It turns out the &lt;code&gt;TF-IDF&lt;/code&gt; transformation is critical for this sparse matrix. If you do not do it, you will find your t-SNE plot looks really funky! do not trust me, try it yourself:)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for clustering scATAC data, one can use the peak x cell matrix or derive a gene activity score by tools such as &lt;a href=&#34;https://cole-trapnell-lab.github.io/cicero-release/&#34;&gt;&lt;code&gt;Cicero&lt;/code&gt;&lt;/a&gt; to generate a gene x cell matrix. This is useful when you want to transfer the RNAseq cell type labels to the scATACseq data. see more details in the &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/460147v1&#34;&gt;Seurat V3 paper&lt;/a&gt;. The question is then, which matrix should we use for clustering? The clustering of these two different matrix can be different but there should be no surprise. We can use the gene activity score matrix as a label transferring mediator and get the cell labels and then super-impose the cluster id to the t-SNE plot clustered by the peak x cell matrix.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Acknowledgements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I want to thank 10x genomics for making the data publicly available.&lt;/li&gt;
&lt;li&gt;I want to thank &lt;a href=&#34;https://jef.works/blog/2017/09/13/graph-based-community-detection-for-clustering-analysis/&#34;&gt;Jean Fan&lt;/a&gt; for putting up some nice posts.&lt;/li&gt;
&lt;li&gt;I want to thank Tim Stuart for answering questions with &lt;code&gt;Seurat&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;I got some ideas from &lt;a href=&#34;https://github.com/jaychung10010/Mammary_snATAC-seq&#34; class=&#34;uri&#34;&gt;https://github.com/jaychung10010/Mammary_snATAC-seq&lt;/a&gt; as well. Thanks for posting the codes.&lt;/li&gt;
&lt;li&gt;I want to thank everyone else who give help and suggestions along my adventure of analyzing scATACseq data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;update&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;UPDATE&lt;/h3&gt;
&lt;p&gt;Do the IF-IDF Seurat way&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Matrix)
library(readr)
library(dplyr)
mat&amp;lt;- readMM(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/matrix.mtx&amp;quot;)
peaks&amp;lt;- read_tsv(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/peaks.bed&amp;quot;, col_names = F)
peaks&amp;lt;- peaks %&amp;gt;%
        mutate(id1 = paste(X2, X3, sep = &amp;quot;-&amp;quot;)) %&amp;gt;%
        mutate(id = paste(X1, id1, sep = &amp;quot;:&amp;quot;))
        
barcodes&amp;lt;- read_tsv(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/barcodes.tsv&amp;quot;, col_names =F)

rownames(mat)&amp;lt;- peaks$id1
colnames(mat)&amp;lt;- barcodes$X1
# binarize the matrix 
mat@x[mat@x &amp;gt;0]&amp;lt;- 1 
# Seurat version TF-IDF
mat&amp;lt;- TF.IDF(mat)
mat&amp;lt;- LogNormalize(mat,scale_factor = 1e4)

### SVD
library(irlba)
set.seed(123)
mat.lsi&amp;lt;- irlba(mat, 50)

d_diagtsne &amp;lt;- matrix(0, 50, 50)
diag(d_diagtsne) &amp;lt;- mat.lsi$d
mat_pcs &amp;lt;- t(d_diagtsne %*% t(mat.lsi$v))
dim(mat_pcs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8728   50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## PCA plot PC1 vs PC2
plot(mat_pcs[,1], mat_pcs[,2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rownames(mat_pcs)&amp;lt;- colnames(mat)

library(RANN)
knn.info&amp;lt;- RANN::nn2(mat_pcs, k = 30)

## convert to adjacancy matrix
knn &amp;lt;- knn.info$nn.idx

adj &amp;lt;- matrix(0, nrow(mat_pcs), nrow(mat_pcs))
rownames(adj) &amp;lt;- colnames(adj) &amp;lt;- rownames(mat_pcs)

for(i in seq_len(nrow(mat_pcs))) {
    adj[i,rownames(mat_pcs)[knn[i,]]] &amp;lt;- 1
}

## convert to graph
library(igraph)
g &amp;lt;- igraph::graph.adjacency(adj, mode=&amp;quot;undirected&amp;quot;)
g &amp;lt;- simplify(g) ## remove self loops

## identify communities, many algorithums. Use the Louvain clustering

km &amp;lt;- igraph::cluster_louvain(g)

com &amp;lt;- km$membership
names(com) &amp;lt;- km$names

# cluster id for each barcode
head(com)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AAACGAAAGAGCGAAA-1 AAACGAAAGAGTTTGA-1 AAACGAAAGCGAGCTA-1 
##                 13                  7                 12 
## AAACGAAAGGCTTCGC-1 AAACGAAAGTGCTGAG-1 AAACGAACAAGGGTAC-1 
##                 14                 13                 16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## total 13 clusters
table(com)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## com
##    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 
##  801  376  617  629  572   56  607  435  280  390  131  417 2554  490  241 
##   16 
##  132&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### T-sne visualization
library(Rtsne)
library(ggplot2)
library(tibble)
set.seed(345)

mat_tsne&amp;lt;- Rtsne(mat_pcs,  dims = 2, perplexity = 30, verbose = TRUE, 
               max_iter = 1000, check_duplicates = FALSE, is_distance = FALSE, 
               theta = 0.5, pca = FALSE, exaggeration_factor = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Read the 8728 x 50 data matrix successfully!
## OpenMP is working. 1 threads.
## Using no_dims = 2, perplexity = 30.000000, and theta = 0.500000
## Computing input similarities...
## Building tree...
## Done in 2.57 seconds (sparsity = 0.014984)!
## Learning embedding...
## Iteration 50: error is 94.871477 (50 iterations in 1.48 seconds)
## Iteration 100: error is 84.409610 (50 iterations in 1.50 seconds)
## Iteration 150: error is 82.319098 (50 iterations in 1.18 seconds)
## Iteration 200: error is 81.831573 (50 iterations in 1.35 seconds)
## Iteration 250: error is 81.608255 (50 iterations in 1.40 seconds)
## Iteration 300: error is 3.039995 (50 iterations in 1.20 seconds)
## Iteration 350: error is 2.691975 (50 iterations in 1.14 seconds)
## Iteration 400: error is 2.508723 (50 iterations in 1.24 seconds)
## Iteration 450: error is 2.390684 (50 iterations in 1.14 seconds)
## Iteration 500: error is 2.308249 (50 iterations in 1.16 seconds)
## Iteration 550: error is 2.248218 (50 iterations in 1.12 seconds)
## Iteration 600: error is 2.201765 (50 iterations in 1.27 seconds)
## Iteration 650: error is 2.166028 (50 iterations in 1.21 seconds)
## Iteration 700: error is 2.137659 (50 iterations in 1.13 seconds)
## Iteration 750: error is 2.115987 (50 iterations in 1.11 seconds)
## Iteration 800: error is 2.098913 (50 iterations in 1.16 seconds)
## Iteration 850: error is 2.086752 (50 iterations in 1.08 seconds)
## Iteration 900: error is 2.079435 (50 iterations in 1.07 seconds)
## Iteration 950: error is 2.078012 (50 iterations in 1.14 seconds)
## Iteration 1000: error is 2.076638 (50 iterations in 1.12 seconds)
## Fitting performed in 24.21 seconds.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_tsne&amp;lt;- as.data.frame(mat_tsne$Y)
colnames(df_tsne)&amp;lt;- c(&amp;quot;tSNE1&amp;quot;, &amp;quot;tSNE2&amp;quot;)
df_tsne$barcode&amp;lt;- rownames(mat_pcs)

df_tsne&amp;lt;- left_join(df_tsne, enframe(com), by = c(&amp;quot;barcode&amp;quot; = &amp;quot;name&amp;quot;)) %&amp;gt;%
        dplyr::rename(cluster = value) %&amp;gt;%
        mutate(cluster = as.factor(cluster))

ggplot(df_tsne, aes(x = tSNE1, y = tSNE2)) + 
        geom_point(aes(col = cluster), size = 0.5) +
        theme_bw(base_size = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, it still looks different from the &lt;code&gt;Seurat&lt;/code&gt; output. Any comments?&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>plot 10x scATAC coverage by cluster/group</title>
      <link>/post/plot-10x-scatac-coverage-by-cluster-group/</link>
      <pubDate>Mon, 29 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/plot-10x-scatac-coverage-by-cluster-group/</guid>
      <description>&lt;p&gt;This post was inspired by &lt;a href=&#34;https://twitter.com/ahill_tweets&#34;&gt;Andrew Hill&lt;/a&gt;’s &lt;a href=&#34;http://andrewjohnhill.com/blog/2019/04/12/streamlining-scatac-seq-visualization-and-analysis/&#34;&gt;recent blog post&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Inspired by some nice posts by &lt;a href=&#34;https://twitter.com/timoast?ref_src=twsrc%5Etfw&#34;&gt;@timoast&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/tangming2005?ref_src=twsrc%5Etfw&#34;&gt;@tangming2005&lt;/a&gt; and work from &lt;a href=&#34;https://twitter.com/10xGenomics?ref_src=twsrc%5Etfw&#34;&gt;@10xGenomics&lt;/a&gt;. Would still definitely have to split BAM files for other tasks, so easy to use tools for that are super useful too!&lt;/p&gt;&amp;mdash; Andrew J Hill (@ahill_tweets) &lt;a href=&#34;https://twitter.com/ahill_tweets/status/1116875339303493634?ref_src=twsrc%5Etfw&#34;&gt;April 13, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Andrew wrote that blog post in light of my other &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/post/split-a-10xscatac-bam-file-by-cluster/&#34;&gt;recent blog post&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/timoast&#34;&gt;Tim&lt;/a&gt;’s (developer of the almighty &lt;a href=&#34;https://satijalab.org/seurat/&#34;&gt;Seurat&lt;/a&gt; package) &lt;a href=&#34;https://timoast.github.io/blog/sinto/&#34;&gt;blog post&lt;/a&gt;. Writing blog post is fun and I am happy to see so many new ideas can be generated through online communications.&lt;/p&gt;
&lt;p&gt;I took Andrew’s idea of reading in the reads in a certain window by taking advantages of tabix indexed file fragment.tsv.gz which is an output from 10x &lt;code&gt;cellranger-atac&lt;/code&gt;. I then split the reads by a grouping file which specifies which group each cell belongs to and a total number of reads in each cell. For visualization, instead of using ggplot2, I used the awesome &lt;a href=&#34;https://bernatgel.github.io/karyoploter_tutorial/&#34;&gt;karyoploteR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I experimented a bit and came up with a function below. Note the script is fast as only the reads fall in the specified region are read into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(tidyr)
library(dplyr)
library(tibble)
library(Rsamtools)
library(karyoploteR)
library(org.Hs.eg.db)
library(org.Mm.eg.db)
library(TxDb.Hsapiens.UCSC.hg19.knownGene)
library(TxDb.Mmusculus.UCSC.mm10.knownGene)


extend &amp;lt;- function(x, upstream=0, downstream=0)     
{
        if (any(strand(x) == &amp;quot;*&amp;quot;))
                warning(&amp;quot;&amp;#39;*&amp;#39; ranges were treated as &amp;#39;+&amp;#39;&amp;quot;)
        on_plus &amp;lt;- strand(x) == &amp;quot;+&amp;quot; | strand(x) == &amp;quot;*&amp;quot;
        new_start &amp;lt;- start(x) - ifelse(on_plus, upstream, downstream)
        new_end &amp;lt;- end(x) + ifelse(on_plus, downstream, upstream)
        ranges(x) &amp;lt;- IRanges(new_start, new_end)
        trim(x)
}


addGeneNameToTxdb&amp;lt;- function(txdb = TxDb.Hsapiens.UCSC.hg19.knownGene, 
                             eg.db = org.Hs.eg.db){
        gene&amp;lt;- genes(txdb)
        ## 1: 1 mapping
        ss&amp;lt;- AnnotationDbi::select(eg.db, keys = gene$gene_id,  
                              keytype=&amp;quot;ENTREZID&amp;quot;, columns = &amp;quot;SYMBOL&amp;quot; )
        gene$symbol&amp;lt;- ss[, 2]
        return(gene)
}


plotCoverageByGroup&amp;lt;- function(chrom = NULL, start = NULL, end = NULL, gene_name = NULL, upstream = 2000,
                               downstream = 2000, fragment, grouping,
                               genome =&amp;#39;hg19&amp;#39;, txdb = TxDb.Hsapiens.UCSC.hg19.knownGene,
                               eg.db = org.Hs.eg.db,
                               ymax = NULL, label_cex = 1, 
                               yaxis_cex = 1, track_col = &amp;quot;cadetblue2&amp;quot;,
                               tick.dist = 10000, minor.tick.dist = 2000,
                               tick_label_cex = 1){
        grouping&amp;lt;- readr::read_tsv(grouping)
        if(! all(c(&amp;quot;cell&amp;quot;, &amp;quot;cluster&amp;quot;, &amp;quot;depth&amp;quot;) %in% colnames(grouping))) {
                stop(&amp;#39;Grouping dataframe must have cell, cluster, and depth columns.&amp;#39;)
        }
        ## get number of reads per group for normalization. 
        ## not furthur normalize by the cell number in each group.
        grouping&amp;lt;-  grouping %&amp;gt;%
                group_by(cluster) %&amp;gt;%
                dplyr::mutate(cells_in_group = n(), total_depth_in_group = sum(depth)) %&amp;gt;%
                # reads per million (RPM)
                dplyr::mutate(scaling_factor = 1e6/(total_depth_in_group)) %&amp;gt;%
                ungroup() %&amp;gt;%
                dplyr::select(cell, cluster, scaling_factor)
        
        
        if (is.null(chrom) &amp;amp; is.null(start) &amp;amp; is.null(end) &amp;amp; !is.null(gene_name)){
                gene &amp;lt;- genes(txdb)
                gene &amp;lt;- addGeneNameToTxdb(txdb = txdb, eg.db = eg.db)
                gr&amp;lt;- gene[which(gene$symbol == gene_name)]
                if (length(gr) == 0){
                        stop(&amp;quot;gene name is not found in the database&amp;quot;)
                } else if (length(gr) &amp;gt; 1) {
                        gr&amp;lt;- gr[1]
                        warning(&amp;quot;multiple GRanges found for the gene, using the first one&amp;quot;)
                } else {
                        gr&amp;lt;- extend(gr, upstream = upstream, downstream = downstream)
                } 
                
        } else if (!is.null(chrom) &amp;amp; !is.null(start) &amp;amp; !is.null(end)){
                gr&amp;lt;- GRanges(seq = chrom, IRanges(start = start, end = end ))
        }
        
        
        ## read in the fragment.tsv.gz file
        ## with &amp;quot;chr&amp;quot;, &amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;, &amp;quot;cell&amp;quot;, &amp;quot;duplicate&amp;quot; columns. output from cellranger-atac
        # this returns a list
        reads&amp;lt;- scanTabix(fragment, param = gr)
        
        reads&amp;lt;- reads[[1]] %&amp;gt;% 
                tibble::enframe() %&amp;gt;% 
                dplyr::select(-name) %&amp;gt;%
                tidyr::separate(value, into = c(&amp;quot;chr&amp;quot;, &amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;, &amp;quot;cell&amp;quot;, &amp;quot;duplicate&amp;quot;), sep = &amp;quot;\t&amp;quot;) %&amp;gt;%
                dplyr::mutate_at(.vars = c(&amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;), as.numeric) %&amp;gt;% 
                # make it 1 based for R, the fragment.tsv is 0 based
                dplyr::mutate(start = start + 1) %&amp;gt;% 
                inner_join(grouping) %&amp;gt;%
                makeGRangesFromDataFrame(keep.extra.columns = TRUE)
        # GRangesList object by group/cluster
        reads_by_group&amp;lt;- split(reads, reads$cluster)
        
        ## plotting
        pp &amp;lt;- getDefaultPlotParams(plot.type=1)
        pp$leftmargin &amp;lt;- 0.15
        pp$topmargin &amp;lt;- 15
        pp$bottommargin &amp;lt;- 15
        pp$ideogramheight &amp;lt;- 5
        pp$data1inmargin &amp;lt;- 10
        kp &amp;lt;- plotKaryotype(genome = genome, zoom = gr, plot.params = pp)
        kp&amp;lt;- kpAddBaseNumbers(kp, tick.dist = tick.dist, minor.tick.dist = minor.tick.dist,
                              add.units = TRUE, cex= tick_label_cex, digits = 6)
        ## calculate the normalized coverage
        normalized_coverage&amp;lt;- function(x){
                if (!is(x, &amp;quot;GRangesList&amp;quot;))
                        stop(&amp;quot;&amp;#39;x&amp;#39; must be a GRangesList object&amp;quot;)
                # specify the width to the whole chromosome to incldue the 0s
                cvgs&amp;lt;- lapply(x, function(x) coverage(x, width = kp$chromosome.lengths) * x$scaling_factor[1])
                return(cvgs)
        }
        
        coverage_norm&amp;lt;- normalized_coverage(reads_by_group)
        
        ## calculate the max coverage if not specified 
        if (is.null(ymax)) {
                yaxis_common&amp;lt;- ceiling(max(lapply(coverage_norm, max) %&amp;gt;% unlist()))
        } else {
                yaxis_common&amp;lt;- ymax
        }
        ## add gene information
        genes.data &amp;lt;- makeGenesDataFromTxDb(txdb,
                                            karyoplot=kp,
                                            plot.transcripts = TRUE, 
                                            plot.transcripts.structure = TRUE)
        genes.data &amp;lt;- addGeneNames(genes.data)
        genes.data &amp;lt;- mergeTranscripts(genes.data)
        
        kp&amp;lt;- kpPlotGenes(kp, data=genes.data, r0=0, r1=0.05, gene.name.cex = 1)
        
        for(i in seq_len(length(coverage_norm))) {
                read &amp;lt;- coverage_norm[[i]]
                at &amp;lt;- autotrack(i, length(coverage_norm), r0=0.1, r1=1, margin = 0.1)
                kp &amp;lt;- kpPlotCoverage(kp, data=read,
                                     r0=at$r0, r1=at$r1, col = track_col, ymax = yaxis_common)
                kpAxis(kp, ymin=0, ymax=yaxis_common, numticks = 2, r0=at$r0, r1=at$r1, cex = yaxis_cex, labels = c(&amp;quot;&amp;quot;, yaxis_common))
                kpAddLabels(kp, labels = names(coverage_norm)[i], r0=at$r0, r1=at$r1, 
                            cex=label_cex, label.margin = 0.005)
        }
        
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;usage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Usage&lt;/h3&gt;
&lt;p&gt;The example files can be downloaded from &lt;a href=&#34;https://osf.io/q5dwj/&#34; class=&#34;uri&#34;&gt;https://osf.io/q5dwj/&lt;/a&gt;.
&lt;code&gt;atac_v1_pbmc_10k_fragments.tsv.gz&lt;/code&gt; is the 10k pbmc atac data downloaded from 10x website. Thanks for making the
data public available. Make sure put the tabix index &lt;code&gt;atac_v1_pbmc_10k_fragments.tsv.gz.tbi&lt;/code&gt; in the same folder.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;grouping.txt&lt;/code&gt; is a 3 column tsv file containing header: cell, cluster, and depth. The cluster label was transferred from the 10x pbmc scRNAseq data set using &lt;code&gt;Seurat&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;LYZ gene is a marker for CD16+ cells.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## LYZ gene 
chrom&amp;lt;-  &amp;quot;chr12&amp;quot;
start&amp;lt;-  69730394
end&amp;lt;- 69760971

plotCoverageByGroup(chrom = chrom, start = start, end = end, fragment = &amp;quot;atac_v1_pbmc_10k_fragments.tsv.gz&amp;quot;,
                    grouping = &amp;quot;grouping.txt&amp;quot;, track_col = &amp;quot;red&amp;quot;, label_cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-29-plot-10x-scatac-coverage-by-cluster-group_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;NKG7 is a marker for NK cells.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plotCoverageByGroup(gene_name = &amp;quot;NKG7&amp;quot;, fragment = &amp;quot;atac_v1_pbmc_10k_fragments.tsv.gz&amp;quot;,
                    grouping = &amp;quot;grouping.txt&amp;quot;, tick_label_cex = 1, tick.dist = 5000,
                    minor.tick.dist = 1000, label_cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-29-plot-10x-scatac-coverage-by-cluster-group_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;what I did for some extra work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I normalized each track by total number of reads in that group in reads per million. I did not do
any further normalization on the cell number of each group as Andrew did. I am open to discussion on how to
best normalize the tracks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I calculated the max value of all the tracks and set a common y-axis for all the tracks. Users can set a customized &lt;code&gt;ymax&lt;/code&gt; as well.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I added some functionality of specifying only a gene name, and one can extend that gene ranges by padding upstream (from transcription start site) and downstream (from transcription end site) bps.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One can either plot Human or Mouse data. Other organisms can be easily supported by modifying the script.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;what-to-do-next&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;what to do next&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;customer specified cluster (a subset of clusters in a certain order) to plot.
When one has a lot of clusters (e.g. over 50), one probably does not want to plot all of them.&lt;/li&gt;
&lt;li&gt;specify color for each cluster track.&lt;/li&gt;
&lt;li&gt;bam support by using&lt;code&gt;Rsamtools::ScanBam&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;smooth window? The plots showed above were not smoothed and they look good to me. Not sure needed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Any suggestions/discussions are welcomed!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reproducible research in bioinformatics</title>
      <link>/talk/2019-bunkerhill-talk/</link>
      <pubDate>Thu, 28 Mar 2019 14:30:00 -0400</pubDate>
      
      <guid>/talk/2019-bunkerhill-talk/</guid>
      <description>&lt;p&gt;I was invited to give a talk on reproducible bioinformatics research to the students in the &lt;a href=&#34;https://www.bhcc.edu/&#34; target=&#34;_blank&#34;&gt;Bunker Hill Community College&lt;/a&gt; in Boston, MA. I was so glad to introduce bioinformatics to the students and share my own perspectives on reproducible research.&lt;/p&gt;

&lt;p&gt;The movie &lt;a href=&#34;https://en.wikipedia.org/wiki/Good_Will_Hunting&#34; target=&#34;_blank&#34;&gt;Good Will Hunting&lt;/a&gt; was shot there :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/bunkerhill-talk.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KRAS-IRF2 Axis Drives Immune Suppression and Immune Therapy Resistance in Colorectal Cancer</title>
      <link>/publication/2019-03-20-irf2/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 -0400</pubDate>
      
      <guid>/publication/2019-03-20-irf2/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Use docopt to write command line R utilities </title>
      <link>/post/use-docopt-to-write-command-line-r-utilities/</link>
      <pubDate>Fri, 22 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/use-docopt-to-write-command-line-r-utilities/</guid>
      <description>&lt;p&gt;I was writing an R script to plot the ATACseq fragment length distribution and wanted
to turn the R script to a command line utility.&lt;/p&gt;

&lt;p&gt;I then (re)discovered this awesome &lt;a href=&#34;https://github.com/docopt/docopt.R&#34; target=&#34;_blank&#34;&gt;docopt.R&lt;/a&gt;.
One just needs to write the help message the you want to display and &lt;code&gt;docopt()&lt;/code&gt; will
parse the options, arguments and return a named list which can be accessed inside the
R script. check &lt;a href=&#34;http://docopt.org/&#34; target=&#34;_blank&#34;&gt;http://docopt.org/&lt;/a&gt; for more information as well.&lt;/p&gt;

&lt;p&gt;See below for an example. You can download it at &lt;a href=&#34;https://github.com/crazyhottommy/scATACutils/blob/master/R/plot_atac_frag_distribution.R&#34; target=&#34;_blank&#34;&gt;https://github.com/crazyhottommy/scATACutils/blob/master/R/plot_atac_frag_distribution.R&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
#! /usr/bin/env Rscript
&#39;Plot fragment length distribution from ATACseq data
Usage:
    plot_atac_fragment_len_distribution.R (--poly | --hist) (--pdf | --png) [--width=&amp;lt;width&amp;gt; --height=&amp;lt;height&amp;gt; --bin=&amp;lt;bp&amp;gt;] &amp;lt;input&amp;gt; &amp;lt;output&amp;gt;
    
Options:
    -h --help  Show this screen.
    -v --version  Show version.
    --bin=&amp;lt;bp&amp;gt;  Bin size [default: 5]
    --poly  Plot frequency polygon.
    --hist  Plot histogram.
    --pdf  Save to pdf.
    --png  Save to png.
    --width=&amp;lt;width&amp;gt;  Width of the output [default: 4]
    --height=&amp;lt;height&amp;gt; Height of the output [default: 4]

Arguments:
    input  fragment length in a one column dataframe without header or stdin
    output  output filename
&#39; -&amp;gt; doc

suppressMessages(library(ggplot2))
# check this awesome docoptR https://github.com/docopt/docopt.R
## make sure use the development version, the CRAN version not working for me
# library(devtools) 
# devtools::install_github(&amp;quot;docopt/docopt.R&amp;quot;)
suppressMessages(library(docopt))
suppressMessages(library(dplyr))

# this will give error if try interactively, because no input and output argument are given
# https://github.com/docopt/docopt.R/issues/27
arguments &amp;lt;- docopt(doc, version = &#39;plot_atac_frag_distribution v1.0\n\n&#39;)

# for testing interactively
#arguments &amp;lt;- docopt(doc, version = &#39;FragmentSizeDistribution v1.0&#39;, args = c(&amp;quot;scripts/fragment3.txt&amp;quot;,&amp;quot;my.pdf&amp;quot;))
#print(arguments)

## File Read ##
# taken from https://stackoverflow.com/questions/26152998/how-to-make-r-script-takes-input-from-pipe-and-user-given-parameter
# if the input is stdin one can do 
# cat fragment.txt | ./plot_atac_frag_distribution.R --poly --pdf stdin  out.pdf
# cat fragment.txt | ./plot_atac_frag_distribution.R --poly --pdf - out.pdf
# ./plot_atac_frag_distribution.R --poly --pdf &amp;lt;(cat fragment.txt)  out.pdf


OpenRead &amp;lt;- function(arg) {
    if (arg %in% c(&amp;quot;-&amp;quot;, &amp;quot;/dev/stdin&amp;quot;)) {
        file(&amp;quot;stdin&amp;quot;, open = &amp;quot;r&amp;quot;)
    } else if (grepl(&amp;quot;^/dev/fd/&amp;quot;, arg)) {
        fifo(arg, open = &amp;quot;r&amp;quot;)
    } else {
        file(arg, open = &amp;quot;r&amp;quot;)
    }
}

dat.con &amp;lt;- OpenRead(arguments$input)
fragment &amp;lt;- read.table(dat.con, header = FALSE)

#fragment&amp;lt;- read.table(arguments$input, header = F)

names(fragment)&amp;lt;- c(&amp;quot;length&amp;quot;)

plot_hist&amp;lt;- function(fragment, bin) {
        g&amp;lt;- ggplot(fragment %&amp;gt;% filter(length &amp;lt;=2000), aes(x = length)) + 
                geom_histogram(binwidth = bin, aes(y=..density..), fill = &amp;quot;red&amp;quot;) +
                geom_density(alpha=.2, fill=&amp;quot;#FF6666&amp;quot;, col = &amp;quot;black&amp;quot;) +
                coord_cartesian(xlim = c(0,1000)) +
                scale_x_continuous(breaks = c(0, 100, 200, 300, 400, 800)) +
                theme_minimal(base_size = 14)
        return(g)
        
}

plot_polygon&amp;lt;- function(fragment, bin){
        g&amp;lt;- ggplot(fragment %&amp;gt;% filter(length &amp;lt;=2000), aes(x = length, stat(density))) + 
                geom_freqpoly(binwidth = bin, col = &amp;quot;blue&amp;quot;) +
                coord_cartesian(xlim = c(0,1000)) +
                scale_x_continuous(breaks = c(0, 100, 200, 300, 400, 800)) +
                theme_minimal(base_size = 14)
        return(g)
}


main&amp;lt;- function(fragment, arguments){
    if (arguments$poly){
        g&amp;lt;- plot_polygon(fragment, as.numeric(arguments$bin))
    } else if (arguments$hist){
        g&amp;lt;- plot_hist(fragment, as.numeric(arguments$bin))
    }
    device&amp;lt;- ifelse(arguments$pdf, &amp;quot;pdf&amp;quot;, &amp;quot;png&amp;quot;)
    
    ggsave(arguments$output, plot = g,  device = device, width =as.numeric(arguments$width), 
           height = as.numeric(arguments$height) )
    
}

main(fragment, arguments)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;save it to &lt;code&gt;plot_atac_frag_distribution.R&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;on command line, one can do:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;
./plot_atac_frag_distribution.R -h
Plot fragment length distribution from ATACseq data
Usage:
    plot_atac_fragment_len_distribution.R (--poly | --hist) (--pdf | --png) [--width=&amp;lt;width&amp;gt; --height=&amp;lt;height&amp;gt; --bin=&amp;lt;bp&amp;gt;] &amp;lt;input&amp;gt; &amp;lt;output&amp;gt;

Options:
    -h --help  Show this screen.
    -v --version  Show version.
    --bin=&amp;lt;bp&amp;gt;  Bin size [default: 5]
    --poly  Plot frequency polygon.
    --hist  Plot histogram.
    --pdf  Save to pdf.
    --png  Save to png.
    --width=&amp;lt;width&amp;gt;  Width of the output [default: 4]
    --height=&amp;lt;height&amp;gt; Height of the output [default: 4]

Arguments:
    input  fragment length in a one column dataframe without header or stdin
    output  output filename

./plot_atac_frag_distribution.R --poly --png  --bin 10 fragment.txt out.png
cat fragment.txt | ./plot_atac_frag_distribution.R --poly --pdf stdin  out.pdf
cat fragment.txt | ./plot_atac_frag_distribution.R --hist --pdf - out.pdf
./plot_atac_frag_distribution.R --hist --pdf &amp;lt;(cat fragment.txt)  out.pdf

samtools view my.bam | awk &#39;$9&amp;gt;0&#39; | cut -f 9 |./plot_atac_frag_distribution.R --poly --pdf - out.pdf

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;histogram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/out2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;polygon:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/out3.png&#34; alt=&#34;&#34; /&gt;
Pretty cool!!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important notes:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[default: 4]  The space after &lt;code&gt;:&lt;/code&gt; is needed.&lt;/li&gt;
&lt;li&gt;use two spaces to separate the option and the explanation&lt;/li&gt;
&lt;li&gt;use four spaces to indent&lt;/li&gt;
&lt;li&gt;use the development version of optdoc.R&lt;/li&gt;
&lt;li&gt;when testing interactively. docopt() may give error when the mandatory arguments
are not specified, but running on command line is fine.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;see &lt;a href=&#34;https://github.com/docopt/docopt.R/issues/24&#34; target=&#34;_blank&#34;&gt;https://github.com/docopt/docopt.R/issues/24&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also check &lt;a href=&#34;http://dirk.eddelbuettel.com/code/littler.examples.html&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;littler&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;littler provides the r program, a simplified command-line interface for GNU R. This allows direct execution of commands, use in piping where the output of one program supplies the input of the next, as well as adding the ability for writing hash-bang scripts, i.e. creating executable files starting with, say, #!/usr/bin/r.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Split a 10xscATAC bam file by cluster</title>
      <link>/post/split-a-10xscatac-bam-file-by-cluster/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/split-a-10xscatac-bam-file-by-cluster/</guid>
      <description>

&lt;p&gt;I want to split the PBMC scATAC bam from 10x by cluster id. So, I can then make a bigwig for each cluster to visualize in &lt;code&gt;IGV&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The first thing I did was googling to see if anyone has written such a tool (Do not reinvent the wheels!). People have done that because I saw figures from the scATAC papers. I just could not find it. Maybe I need to refine my googling skills.&lt;/p&gt;

&lt;p&gt;I decided to write one myself. The following is my journey for this small task.&lt;/p&gt;

&lt;p&gt;download the 5k pbmc scATAC data from &lt;a href=&#34;https://support.10xgenomics.com/single-cell-atac/datasets/1.0.1/atac_v1_pbmc_5k&#34; target=&#34;_blank&#34;&gt;https://support.10xgenomics.com/single-cell-atac/datasets/1.0.1/atac_v1_pbmc_5k&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;split-the-cell-barcodes-by-cluster-id&#34;&gt;split the cell barcodes by cluster id&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd analysis/clustering/graphclust
head clusters.csv
Barcode,Cluster
AAACGAAAGCGCAATG-1,1
AAACGAAAGGGTATCG-1,4
AAACGAAAGTAACATG-1,8
AAACGAAAGTTACACC-1,1
AAACGAACAGAGATGC-1,4
AAACGAACATGCTATG-1,5
AAACGAAGTGCATCAT-1,3
AAACGAAGTGGACGAT-1,3
AAACGAAGTGGCCTCA-1,7

# there are ^M characters at the end of the line if you do cat -A you will see it.
# change it to unix
dos2unix clusters.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;awk -F&amp;quot;,&amp;quot; &#39;NR&amp;gt;1{print $1 &amp;gt;&amp;gt; &amp;quot;cluster_&amp;quot;$2&amp;quot;.csv&amp;quot;}&#39; clusters.csv
wc -l *csv
   330 cluster_10.csv
   322 cluster_11.csv
   258 cluster_12.csv
   191 cluster_13.csv
   608 cluster_1.csv
   563 cluster_2.csv
   559 cluster_3.csv
   532 cluster_4.csv
   483 cluster_5.csv
   425 cluster_6.csv
   366 cluster_7.csv
   360 cluster_8.csv
   338 cluster_9.csv
  5336 clusters.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;use-bamtools&#34;&gt;use bamtools&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;time bamtools filter -tag CB:Z:AAACTGCAGAGCAGCT-1 -in atac_v1_pbmc_5k_possorted_bam.bam -out AAACTGCAGAGCAGCT-1.bam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This takes a little over 1 hour for one barcode! And there is no easy way
to specify a group of barcodes.&lt;/p&gt;

&lt;h3 id=&#34;use-the-linux-tricks&#34;&gt;use the linux tricks&lt;/h3&gt;

&lt;p&gt;inspired partly by this post &lt;a href=&#34;https://www.biostars.org/p/263346/&#34; target=&#34;_blank&#34;&gt;https://www.biostars.org/p/263346/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wc -l clusters.csv
5336 clusters.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and let&amp;rsquo;s see how fast each regular expression takes for &lt;code&gt;awk&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;time samtools view atac_v1_pbmc_5k_possorted_bam.bam | awk -v tag=&amp;quot;CB:Z:AAACTGCAGAGCAGCT-1&amp;quot; &#39;index($0,tag)&amp;gt;0&#39; &amp;gt;&amp;gt; AAACTGCAGAGCAGCT-1.sam

real    27m14.332s
user    48m36.883s
sys     4m37.908s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is not too bad, but if we loops over the &lt;code&gt;clusters.csv&lt;/code&gt; files for 5335 times,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;samtools view -H atac_v1_pbmc_5k_possorted_bam.bam &amp;gt; header.txt

cat clusters.csv \
| sed &#39;1d&#39; \
| while IFS=&#39;,&#39; read -r barcode cluster
    do samtools view atac_v1_pbmc_5k_possorted_bam.bam |  awk -v tag=&amp;quot;CB:Z:$barcode&amp;quot; &#39;index($0,tag)&amp;gt;0&#39; &amp;gt;&amp;gt; &amp;quot;$cluster.sam&amp;quot;
    done

## then cat the header with the sam.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;it will take ~30min * 5335 = ~100 days to finish.&lt;/p&gt;

&lt;p&gt;we can do better to parallize by GNU parallel&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## not tested...
cat clusters.csv \
| sed &#39;1d&#39; \
| parallel --colsep &#39;,&#39; -j 40 &#39;samtools view atac_v1_pbmc_5k_possorted_bam.bam |awk -v tag=&amp;quot;CB:Z:{1}&amp;quot; &#39;index(\$0,tag)&amp;gt;0&#39; &amp;gt;&amp;gt; {2}.sam&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, using 40 cores may reduce our time to &lt;sup&gt;100&lt;/sup&gt;&amp;frasl;&lt;sub&gt;40&lt;/sub&gt; = 5 days.&lt;/p&gt;

&lt;h3 id=&#34;use-pysam&#34;&gt;use pysam&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s only loop over the sam file once&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pysam
import csv

cluster_dict = {}
with open(&#39;clusters.csv&#39;) as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=&#39;,&#39;)
    #skip header
    header = next(csv_reader)
    for row in csv_reader:
        cluster_dict[row[0]] = row[1]

clusters = set(x for x in cluster_dict.values())


fin = pysam.AlignmentFile(&amp;quot;atac_v1_pbmc_5k_possorted_bam.bam&amp;quot;, &amp;quot;rb&amp;quot;)

# open the number of bam files as the same number of clusters, and map the out file handler to the cluster id, write to a bam with wb
fouts_dict = {}
for cluster in clusters:
    fout = pysam.AlignmentFile(&amp;quot;cluster&amp;quot; + cluster + &amp;quot;.bam&amp;quot;, &amp;quot;wb&amp;quot;, template = fin)
    fouts_dict[cluster] = fout

for read in fin:
    tags = read.tags
    CB_list = [ x for x in tags if x[0] == &amp;quot;CB&amp;quot;]
    if CB_list:
        cell_barcode = CB_list[0][1]
    # the bam files may contain reads not in the final clustered barcodes
    # will be None if the barcode is not in the clusters.csv file
    else: 
        continue
    cluster_id = cluster_dict.get(cell_barcode)
    if cluster_id:
        fouts_dict[cluster_id].write(read)

## do not forget to close the files
fin.close()
for fout in fouts_dict.values():
    fout.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;real    172m58.758s
user    172m10.678s
sys     0m46.071s&lt;/p&gt;

&lt;p&gt;Note, some read record in the bam file do not have &lt;code&gt;CB&lt;/code&gt; but only &lt;code&gt;CR&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;from &lt;a href=&#34;https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/output/bam&#34; target=&#34;_blank&#34;&gt;https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/output/bam&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CB&lt;/td&gt;
&lt;td&gt;Chromium cellular barcode sequence that is error-corrected and confirmed against a list of known-good barcode sequences.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;CR&lt;/td&gt;
&lt;td&gt;Chromium cellular barcode sequence as reported by the sequencer.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;How many of those reads with &lt;code&gt;CR&lt;/code&gt;?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# every read has a CR tag
samtools view atac_v1_pbmc_5k_possorted_bam.bam| grep -v &amp;quot;CR&amp;quot; | wc -l
0 

# not every read has a CB tag.
samtools view atac_v1_pbmc_5k_possorted_bam.bam| grep -v &amp;quot;CB&amp;quot; | wc -l
10647804

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;use-pybam&#34;&gt;use pybam&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.biostars.org/p/186732/&#34; target=&#34;_blank&#34;&gt;https://www.biostars.org/p/186732/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/JohnLonginotto/pybam&#34; target=&#34;_blank&#34;&gt;https://github.com/JohnLonginotto/pybam&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note that pybam is python2.x&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source activate py27
cd ~/apps
git clone https://github.com/JohnLonginotto/pybam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;inside python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pybam
import csv

cluster_dict = {}
with open(&#39;clusters.csv&#39;) as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=&#39;,&#39;)
    #skip header
    header = next(csv_reader)
    for row in csv_reader:
        cluster_dict[row[0]] = row[1]

clusters = set(x for x in cluster_dict.values())


# open the number of bam files as the same number of clusters, and map the out file handler to the cluster id

header = pybam.read(&#39;atac_v1_pbmc_5k_possorted_bam.bam&#39;).file_header
fouts_dict = {}
for cluster in clusters:
    fout = open(&amp;quot;cluster&amp;quot; + cluster + &amp;quot;.sam&amp;quot;, &amp;quot;w&amp;quot;)
    fout.write(header)
    fouts_dict[cluster] = fout

for read in pybam.read(&#39;possorted_bam.bam&#39;):
        ## not always the same position in the list for the CB tag
        ## there could be no CB tag for a certian read as well
        ## it will return empty list
        CB_list = [ x for x in read.sam_tags_list if x[0] == &amp;quot;CB&amp;quot;]
        if CB_list:
            cell_barcode = CB_list[0][2]
            cluster_id = cluster_dict.get(cell_barcode)
            if cluster_id:
                fouts_dict[cluster_id].write(read.sam + &#39;\n&#39;)
        
## do not forget to close the files
for fout in fouts_dict.values():
    fout.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;real    1262m30.849s
user    1240m11.906s
sys     22m9.325s&lt;/p&gt;

&lt;p&gt;Did not find how to write to a bam file, so I have to write to a sam file. I asked on github issues but no responses. The author is not actively maintaining the library anymore.&lt;/p&gt;

&lt;h3 id=&#34;use-hts-nim&#34;&gt;use hts-nim&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/brentp/hts-nim-tools/issues/5&#34; target=&#34;_blank&#34;&gt;https://github.com/brentp/hts-nim-tools/issues/5&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks Brent for providing the code.&lt;/p&gt;

&lt;h4 id=&#34;htslib-need-to-be-in-ld-library-path&#34;&gt;htslib need to be in &lt;code&gt;$LD_LIBRARY_PATH&lt;/code&gt;:&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/samtools/htslib/releases/download/1.6/htslib-1.6.tar.bz2
tar xjf htslib-1.6.tar.bz2
cd htslib-1.6
./configure ~/bin/

make

# add this to .bashrc and source ~/.bashrc
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/n/home02/mtang/apps/htslib-1.6
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;install-nim-and-hts-nim&#34;&gt;install &lt;code&gt;nim&lt;/code&gt; and &lt;code&gt;hts-nim&lt;/code&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://raw.githubusercontent.com/brentp/hts-nim/master/scripts/simple-install.sh

chmod u+x simple-install.sh
./simple-install.sh

# add nim to PATH

git clone https://github.com/brentp/hts-nim
cd hts-nim
nimble install -y
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import hts
import os
import strutils
import tables
var ibam:Bam

# lookup from cb -&amp;gt; cluster
var clusterTbl = initTable[string,string]()
# lookup from cluster -&amp;gt; bam
var tbl = initTable[string, Bam]()

for x in paramStr(1).lines:
  var toks = x.strip().split(&amp;quot;,&amp;quot;)
  clusterTbl[toks[0]] = toks[1]

if not open(ibam, paramStr(2)):
   quit &amp;quot;couldn&#39;t open bam&amp;quot;

for aln in ibam:
  var cb = tag[string](aln, &amp;quot;CB&amp;quot;).get
  if cb.isNullOrEmpty: continue
  if cb notin clusterTbl: continue
  var cluster = clusterTbl[cb]
  if cluster notin tbl:
    var obam: Bam
    if not open(obam, &amp;quot;out-cluster-&amp;quot; &amp;amp; cluster &amp;amp; &amp;quot;.bam&amp;quot;, mode=&amp;quot;w&amp;quot;):
      quit &amp;quot;couldn&#39;t open bam for writing&amp;quot;
    obam.write_header(ibam.hdr)
    tbl[cluster] = obam
  tbl[cluster].write(aln)

for k, bam in tbl:
  bam.close()
ibam.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;compile&#34;&gt;compile&lt;/h4&gt;

&lt;p&gt;save it to &lt;code&gt;split_scATAC_bam.nim&lt;/code&gt; and compile:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nim compile -d:release scATAC_split_scATAC_bam.nim
split_scATAC_bam clusters.csv atac_v1_pbmc_5k_possorted_bam.bam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;real    105m17.140s
user    102m17.214s
sys     2m58.312s&lt;/p&gt;

&lt;p&gt;it is &lt;sup&gt;172&lt;/sup&gt;&amp;frasl;&lt;sub&gt;105&lt;/sub&gt; &lt;strong&gt;~1.6 times faster&lt;/strong&gt; in &lt;code&gt;hts-nim&lt;/code&gt; than in &lt;code&gt;pysam&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;speed-up&#34;&gt;speed up&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;parallize by chromosome&lt;/li&gt;
&lt;li&gt;pysam parallization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hts-nim&lt;/code&gt; from Brent:&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;you can add &lt;code&gt;threads=2&lt;/code&gt; (or 3) to the &lt;code&gt;open&lt;/code&gt; calls to get a bit more speed on de/compressing the bam which will be the most CPU time&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I tested using &lt;code&gt;threads = 3&lt;/code&gt; for the same bam file, it took&lt;/p&gt;

&lt;p&gt;real    92m11.205s
user    100m11.622s
sys     6m3.067s&lt;/p&gt;

&lt;p&gt;one saved another 105-92 = 13 mins using multi-thread &lt;code&gt;hts-nim&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;C htslib, I expect the speed will be similar to &lt;code&gt;hts-nim&lt;/code&gt; since &lt;code&gt;hts-nim&lt;/code&gt; is a wrapper around it.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;lessons-learned&#34;&gt;Lessons learned&lt;/h3&gt;

&lt;p&gt;I had &lt;a href=&#34;https://github.com/brentp/hts-nim-tools/issues/5#issuecomment-464114496&#34; target=&#34;_blank&#34;&gt;a bug&lt;/a&gt; in my &lt;code&gt;pysam&lt;/code&gt; code and it pulls out some reads without the &lt;code&gt;CB&lt;/code&gt; tag. Thanks Brent for catching it. I spent some time to debug and could not find it.&lt;/p&gt;

&lt;p&gt;Lessons that I have learned:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How to make sure the output of the software is correct is very difficult. unit testing is important.&lt;/li&gt;
&lt;li&gt;It is good to have someone else with more programming experience to look at the code for you. You are so used to the code that you write and can not find the &amp;ldquo;obvious&amp;rdquo; problem.&lt;/li&gt;
&lt;li&gt;Do not use libraries that are not well maintained. The &lt;code&gt;pybam&lt;/code&gt; author is not maintaining the library now and it is written in python2.x. I am writing all my python code in python3.x&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have put the python and nim code at &lt;a href=&#34;https://github.com/crazyhottommy/scATACutils&#34; target=&#34;_blank&#34;&gt;scATACutils&lt;/a&gt;. The &lt;code&gt;pysam&lt;/code&gt; code and &lt;code&gt;hts-nim&lt;/code&gt; code generate exactly the same results.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>understand 10x scRNAseq and scATAC fastqs</title>
      <link>/post/understand-10x-scrnaseq-and-scatac-fastqs/</link>
      <pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/understand-10x-scrnaseq-and-scatac-fastqs/</guid>
      <description>

&lt;h3 id=&#34;single-cell-rnaseq&#34;&gt;single cell RNAseq&lt;/h3&gt;

&lt;p&gt;Please read the following posts by Dave Tang. When I google, I always find his posts on top of the pages. Thanks for sharing your knowledge.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://davetang.org/muse/2018/06/06/10x-single-cell-bam-files/&#34; target=&#34;_blank&#34;&gt;https://davetang.org/muse/2018/06/06/10x-single-cell-bam-files/&lt;/a&gt;
&lt;a href=&#34;https://davetang.org/muse/2018/08/09/getting-started-with-cell-ranger/&#34; target=&#34;_blank&#34;&gt;https://davetang.org/muse/2018/08/09/getting-started-with-cell-ranger/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From the 10x manual:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The final Single Cell 3’ Libraries contain the P5 and P7 primers used in Illumina bridge amplification PCR. The 10x Barcode and Read 1 (primer site for sequencing read 1) is added to the molecules during the GEMRT incubation. The P5 primer, Read 2 (primer site for sequencing read 2), Sample Index and P7 primer will be added during library construction&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/rnaseq_library.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A Single Cell 3’ Library comprises standard Illumina paired-end constructs which begin and end with P5 and P7. The Single Cell 3’ v2 16 bp 10x Barcodes are encoded at the start of Read 1, while sample index sequences are incorporated as the i7 index read. Read 1 and Read 2 are standard Illumina sequencing primer sites used in paired-end sequencing. Read 1 is used to sequence the 16 bp 10x Barcode and 10 bp UMI, while Read 2 is used to sequence the cDNA fragment.&lt;/p&gt;

&lt;p&gt;Each sample index provided in the Chromium i7 Sample Index Kit combines 4 different sequences in order to balance across all 4 nucleotides.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After &lt;code&gt;cellranger mkfastq&lt;/code&gt;, three &lt;code&gt;fastq.gz&lt;/code&gt; files will be produced: &lt;code&gt;I1&lt;/code&gt;, &lt;code&gt;R1&lt;/code&gt; and &lt;code&gt;R2&lt;/code&gt;. &lt;code&gt;I1&lt;/code&gt; is the 8 bp sample barcode, &lt;code&gt;R1&lt;/code&gt; is the 16bp &lt;code&gt;feature barcode&lt;/code&gt; + 10 bp &lt;code&gt;UMI&lt;/code&gt;, &lt;code&gt;R2&lt;/code&gt; is the reads mapped to the transcriptome.&lt;/p&gt;

&lt;p&gt;Feature barcode whitelist can be found at the cellranger installation path:  &lt;code&gt;cellranger-2.1.0/cellranger-cs/2.1.0/lib/python/cellranger/barcodes/737K-august-2016.txt&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;single-cell-atac&#34;&gt;single cell ATAC&lt;/h3&gt;

&lt;p&gt;after &lt;code&gt;cellranger-atac mkfastq&lt;/code&gt;, there four &lt;code&gt;fastq.gz&lt;/code&gt; files will be generated. &lt;code&gt;I1&lt;/code&gt;, &lt;code&gt;R1&lt;/code&gt;, &lt;code&gt;R2&lt;/code&gt; and &lt;code&gt;R3&lt;/code&gt;.
&lt;code&gt;I1&lt;/code&gt; is the 8 bp sample barcode, &lt;code&gt;R1&lt;/code&gt; is the forward read, &lt;code&gt;R2&lt;/code&gt; is the 16 bp &lt;code&gt;10x feature barcode&lt;/code&gt; and &lt;code&gt;R3&lt;/code&gt; is the reverse read. Thanks &lt;a href=&#34;https://twitter.com/Itti_Q&#34; target=&#34;_blank&#34;&gt;Aditi Qamra&lt;/a&gt; for pointing it out.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/atac_library.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The 16bp feature barcode whitelist can be found at cellranger-atac installation path:&lt;code&gt;cellranger-atac-1.0.1/cellranger-atac-cs/1.0.1/lib/python/barcodes/737K-cratac-v1.txt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that I have put the sample barcodes and feature barcodes files at &lt;a href=&#34;https://osf.io/2z9gj/files/&#34; target=&#34;_blank&#34;&gt;https://osf.io/2z9gj/files/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;are-the-feature-barcode-whitelist-the-same-for-scrnaseq-and-scatac&#34;&gt;Are the feature barcode whitelist the same for scRNAseq and scATAC?&lt;/h3&gt;

&lt;p&gt;In theory, 16bp barcode can have &lt;code&gt;4^16&lt;/code&gt; (4,294,967,296) combinations, but you will want some diversities of the sequences to better distinguish 2 barcodes.Keep in mind that there are PCR or sequencing errors for the barcodes.&lt;/p&gt;

&lt;p&gt;Both are 737K, but are the sequences the same?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat 737K-august-2016.txt | sort | uniq &amp;gt; scRNAseq_barcode.txt
cat 737K-cratac-v1.txt | sort | uniq &amp;gt; scATAC_barcode.txt

wc -l scRNAseq_barcode.txt
737280 scRNAseq_barcode.txt

wc -l scATAC_barcode.txt
737280 scATAC_barcode.txt

## only 10812 cell barcodes are common  
comm -12 scRNAseq_barcode.txt scATAC_barcode.txt  | wc -l
10812
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, 10x uses quite different cell barcodes for scRNAseq and scATACseq applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluating single cell RNAseq cluster stability</title>
      <link>/project/evaluating-scrnaseq-cluster/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 -0500</pubDate>
      
      <guid>/project/evaluating-scrnaseq-cluster/</guid>
      <description>&lt;p&gt;The goal of scclusteval(Single Cell Cluster Evaluation) is to evaluate the single cell clustering stability by boostrapping/subsampling the cells and provide many visualization methods for comparing clusters.&lt;/p&gt;

&lt;p&gt;for Theory behind the method, see Christian Henning, “Cluster-wise assessment of cluster stability,” Research Report 271, Dept. of Statistical Science, University College London, December 2006)&lt;/p&gt;

&lt;p&gt;You can find the package at my &lt;a href=&#34;https://github.com/crazyhottommy/scclusteval&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/raincloud_cluster.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to make a transcript to gene mapping file</title>
      <link>/post/how-to-make-a-transcript-to-gene-mapping-file/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-make-a-transcript-to-gene-mapping-file/</guid>
      <description>&lt;p&gt;I need a transcript to gene mapping file for &lt;code&gt;Salmon&lt;/code&gt;. I am aware of annotation &lt;code&gt;bioconductor&lt;/code&gt; packages that can do this job. However, I was working on a species which does not have the annotation in a package format (I am going to use Drosphila as an example for this blog post). I had to go and got the gtf file and made such a file from scratch.&lt;/p&gt;
&lt;p&gt;Please read the &lt;a href=&#34;https://useast.ensembl.org/info/website/upload/gff.html&#34;&gt;specifications&lt;/a&gt; of those two file formats.&lt;/p&gt;
&lt;div id=&#34;download-drosophila-gtf-file-from-ensemble-and-gff-file-from-ncbi&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Download drosophila gtf file from ENSEMBLE and gff file from NCBI&lt;/h3&gt;
&lt;p&gt;Find the &lt;code&gt;gff&lt;/code&gt; file at &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/genome/?term=drosophila+melanogaster&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/genome/?term=drosophila+melanogaster&lt;/a&gt;&lt;br /&gt;
Find the &lt;code&gt;gtf&lt;/code&gt; file at &lt;a href=&#34;ftp://ftp.ensembl.org/pub/release-95/gtf/drosophila_melanogaster/&#34; class=&#34;uri&#34;&gt;ftp://ftp.ensembl.org/pub/release-95/gtf/drosophila_melanogaster/&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;#gtf file
zless -S ~/Downloads/Drosophila_melanogaster.BDGP6.95.gtf.gz | grep -v &amp;quot;#&amp;quot; | cut -f3 | sort | uniq -c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 160859 CDS
##    4 Selenocysteine
## 187373 exon
## 46299 five_prime_utr
## 17737 gene
## 30492 start_codon
## 33892 three_prime_utr
## 34767 transcript&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;#gff file
zless -S ~/Downloads/GCF_000001215.4_Release_6_plus_ISO1_MT_genomic.gff.gz| grep -v &amp;quot;#&amp;quot; | cut -f3 | sort | uniq -c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 160949 CDS
##    1 RNase_MRP_RNA
##    2 RNase_P_RNA
##    2 SRP_RNA
##  584 antisense_RNA
## 187809 exon
## 17421 gene
## 2275 lnc_RNA
## 30480 mRNA
##  479 miRNA
## 5416 mobile_genetic_element
##   77 ncRNA
##  263 primary_transcript
##  308 pseudogene
##  134 rRNA
## 1870 region
##    1 sequence_feature
##   32 snRNA
##  289 snoRNA
##  319 tRNA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;use-unix-command-to-make-a-transcripts-to-gene-mapping-file-from-gtf-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use unix command to make a transcripts to gene mapping file from gtf file&lt;/h3&gt;
&lt;p&gt;We see the feature types are quite different although they are both annotation files for the same species.
The &lt;code&gt;gtf&lt;/code&gt; file is relatively well formatted, and we can make a transcripts to gene mapping file easily using
unix command line.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;zless -S ~/Downloads/Drosophila_melanogaster.BDGP6.95.gtf.gz | grep -v &amp;quot;#&amp;quot; | awk &amp;#39;$3==&amp;quot;transcript&amp;quot;&amp;#39; | cut -f9 | tr -s &amp;quot;;&amp;quot; &amp;quot; &amp;quot; | awk &amp;#39;{print$4&amp;quot;\t&amp;quot;$2}&amp;#39; | sort | uniq |  sed &amp;#39;s/\&amp;quot;//g&amp;#39; | tee tx2gene_ensemble.tsv| head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## FBgn0013687  FBgn0013687
## FBtr0005088  FBgn0260439
## FBtr0006151  FBgn0000056
## FBtr0070000  FBgn0031081
## FBtr0070001  FBgn0052826
## FBtr0070002  FBgn0031085
## FBtr0070003  FBgn0062565
## FBtr0070006  FBgn0031089
## FBtr0070007  FBgn0031092
## FBtr0070008  FBgn0031094&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hmm…why the first line has both genes in the two columns?…
sanity check:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;zless -S ~/Downloads/Drosophila_melanogaster.BDGP6.95.gtf.gz | grep &amp;quot;FBgn0013687&amp;quot; | less -S&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## mitochondrion_genome FlyBase gene    14917   19524   .   +   .   gene_id &amp;quot;FBgn0013687&amp;quot;; gene_name &amp;quot;mt:ori&amp;quot;; gene_source &amp;quot;FlyBase&amp;quot;; gene_biotype &amp;quot;pseudogene&amp;quot;;
## mitochondrion_genome FlyBase transcript  14917   19524   .   +   .   gene_id &amp;quot;FBgn0013687&amp;quot;; transcript_id &amp;quot;FBgn0013687&amp;quot;; gene_name &amp;quot;mt:ori&amp;quot;; gene_source &amp;quot;FlyBase&amp;quot;; gene_biotype &amp;quot;pseudogene&amp;quot;; transcript_source &amp;quot;FlyBase&amp;quot;; transcript_biotype &amp;quot;pseudogene&amp;quot;;
## mitochondrion_genome FlyBase exon    14917   19524   .   +   .   gene_id &amp;quot;FBgn0013687&amp;quot;; transcript_id &amp;quot;FBgn0013687&amp;quot;; exon_number &amp;quot;1&amp;quot;; gene_name &amp;quot;mt:ori&amp;quot;; gene_source &amp;quot;FlyBase&amp;quot;; gene_biotype &amp;quot;pseudogene&amp;quot;; transcript_source &amp;quot;FlyBase&amp;quot;; transcript_biotype &amp;quot;pseudogene&amp;quot;; exon_id &amp;quot;FBgn0013687-E1&amp;quot;;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed it is in the original gtf file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-gffutilsto-make-a-transcripts-to-gene-mapping-file-from-gff-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use &lt;code&gt;gffutils&lt;/code&gt;to make a transcripts to gene mapping file from gff file&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;gff&lt;/code&gt; file is not that well defined. One may still be able to use some unix tricks to get the tx2gene.tsv file from a gff file, but it can be rather awkward especially for gff files from other not well annotated species. Instead, let’s use &lt;code&gt;gffutils&lt;/code&gt;, a python package to do the same.&lt;/p&gt;
&lt;p&gt;install &lt;code&gt;gffutils&lt;/code&gt; in terminal:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;source activate snakemake&lt;/code&gt;&lt;br /&gt;
&lt;code&gt;conda install gffutils&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note, I am running python through Rsutdio/ First read how to set python path for &lt;code&gt;reticulate&lt;/code&gt; at &lt;a href=&#34;https://rstudio.github.io/reticulate/articles/versions.html&#34; class=&#34;uri&#34;&gt;https://rstudio.github.io/reticulate/articles/versions.html&lt;/a&gt;
read more on &lt;a href=&#34;https://cran.r-project.org/web/packages/reticulate/vignettes/versions.html&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/reticulate/vignettes/versions.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Somehow, I have to create a &lt;code&gt;.Rprofile&lt;/code&gt; in the same folder of &lt;code&gt;.Rproj&lt;/code&gt; file with the following line to use my snakemake conda environment which is python3:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Sys.setenv(PATH = paste(&amp;quot;/anaconda3/envs/snakemake/bin/&amp;quot;, Sys.getenv(&amp;quot;PATH&amp;quot;), sep=&amp;quot;:&amp;quot;))&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reticulate)

# check which python I am using
py_discover_config()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## python:         /anaconda3/envs/snakemake/bin//python
## libpython:      /anaconda3/envs/snakemake/lib/libpython3.6m.dylib
## pythonhome:     /anaconda3/envs/snakemake:/anaconda3/envs/snakemake
## version:        3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 14:01:38)  [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
## numpy:          /anaconda3/envs/snakemake/lib/python3.6/site-packages/numpy
## numpy_version:  1.15.3
## 
## python versions found: 
##  /anaconda3/envs/snakemake/bin//python
##  /usr/bin/python
##  /anaconda3/envs/py27/bin/python
##  /anaconda3/envs/snakemake/bin/python&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these did not work for me...
# use_condaenv(&amp;quot;snakemake&amp;quot;, required = TRUE)
# use_python(&amp;quot;/anaconda3/envs/snakemake/bin/python&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import sys
print(sys.version)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 14:05:31) 
## [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import gffutils
import itertools
import os
os.listdir()
db = gffutils.create_db(&amp;quot;GCF_000001215.4_Release_6_plus_ISO1_MT_genomic.gff.gz&amp;quot;, &amp;quot;:memory:&amp;quot;, force = True,merge_strategy=&amp;quot;merge&amp;quot;, id_spec={&amp;#39;gene&amp;#39;: &amp;#39;Dbxref&amp;#39;})
list(db.featuretypes())
# one can do it for one type of features, say mRNA
for mRNA in itertools.islice(db.features_of_type(&amp;#39;mRNA&amp;#39;), 10):
        print(mRNA[&amp;#39;transcript_id&amp;#39;][0], mRNA[&amp;#39;gene&amp;#39;][0])
        #print(mRNA.attributes.items())
        
## but I then have to do the same for lnc_RNA and others.        
## instead, loop over all features in the database&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NM_001103384.3 CG17636
## NM_001258513.2 CG17636
## NM_001258512.2 CG17636
## NM_001297796.1 RhoGAP1A
## NM_001297795.1 RhoGAP1A
## NM_001103385.2 RhoGAP1A
## NM_001103386.2 RhoGAP1A
## NM_001169155.1 RhoGAP1A
## NM_001297797.1 RhoGAP1A
## NM_001297801.1 tyn&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tx_and_gene=[]
with open(&amp;quot;tx2gene_NCBI.tsv&amp;quot;, &amp;quot;w&amp;quot;) as f:
        for feature in db.all_features():
                transcript = feature.attributes.get(&amp;#39;transcript_id&amp;#39;, [None])[0]
                gene = feature.attributes.get(&amp;#39;gene&amp;#39;, [None])[0]
                if gene and transcript and ([transcript, gene] not in tx_and_gene):
                        tx_and_gene.append([transcript, gene])
                        f.write(transcript + &amp;quot;\t&amp;quot; + gene + &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These lines of codes are not hard to write. It takes more time to read the package documentation and understand how to use the package. One problem with bioinFORMATics is that there are so many different file formats. To make things worse, even for gff file format, many files do not follow the exact specification. You can have a taste of that at &lt;a href=&#34;http://daler.github.io/gffutils/examples.html&#34; class=&#34;uri&#34;&gt;http://daler.github.io/gffutils/examples.html&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding p value, multiple comparisons, FDR and q value</title>
      <link>/post/understanding-p-value-multiple-comparisons-fdr-and-q-value/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/understanding-p-value-multiple-comparisons-fdr-and-q-value/</guid>
      <description>&lt;p&gt;UPDATE 01/29/2019.
Read this awesome paper &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877414/&#34;&gt;Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This was an &lt;a href=&#34;http://crazyhottommy.blogspot.com/2015/03/understanding-p-value-multiple.html&#34;&gt;old post&lt;/a&gt; I wrote 3 years ago after I took HarvardX: &lt;a href=&#34;https://courses.edx.org/courses/course-v1:HarvardX+PH525.3x+1T2018/0b42cffa7c6e4c559bf74f93fb864a59/&#34;&gt;PH525.3x Advanced Statistics for the Life Sciences on edx&lt;/a&gt; taught by &lt;a href=&#34;http://rafalab.github.io/&#34;&gt;Rafael Irizarry&lt;/a&gt;. It is still one of the best courses to get you started using R for genomics. I am very thankful to have those high quality classes available to me when I started to learn. I am reposting it here using blogdown to give myself a refresh.&lt;/p&gt;
&lt;p&gt;I am writing this post for my own later references. Deep understanding of p-value, FDR and q-value is not trivial, and many biologists are misusing and/or misinterpreting them. Please also read this Nature Biotech primer &lt;a href=&#34;https://www.nature.com/articles/nbt1209-1135&#34;&gt;How does multiple testing correction work?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For biologists’ sake, I will use an example of gene expression. Suppose we have two groups of cells: control and treatment (can be anything like chemical treatment, radiation treatment etc..). We are looking if Gene A is deferentially expressed or not under treatment. Each group we have 12 replicates.&lt;/p&gt;
&lt;p&gt;What we usually do is take the average of 12 replicates of each group and do a t-test to compare if the difference is significant or not (assume normal distribution). We then get a p-value, say p = 0.035. We know it is smaller than 0.05 (a threshold we set), and we conclude that after treatment, expression of Gene A is significantly changed. However, what does it mean by saying a p value of 0.035?&lt;/p&gt;
&lt;p&gt;Everything starts with a null hypothesis:&lt;br /&gt;
&lt;strong&gt;H0 : There are no difference of gene expression for Gene A after treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;and an alternative hypothesis:&lt;br /&gt;
&lt;strong&gt;H1: After treatment, expression of Gene A changes.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The definition of every P value begins by assuming a null hypothesis is True. &lt;span class=&#34;citation&#34;&gt;Motulsky (&lt;a href=&#34;#ref-motulsky2014intuitive&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; Third edition page 127. With a p-value of 0.035, it means that under the Null, the probability that we see the difference of gene expression after treatment is 0.035, which is very low. If we choose a significant level of alpha=0.05, we then reject the Null hypothesis and accept the alternative hypothesis. So, if you can not state what the null hypothesis is, you can not understand the P value. &lt;span class=&#34;citation&#34;&gt;Motulsky (&lt;a href=&#34;#ref-motulsky2014intuitive&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; Third edition page 127.&lt;/p&gt;
&lt;p&gt;For a typical genomic study, there are thousands of genes we want to compare. How do we report the gene list containing the genes that are differentially expressed? We can perform a-test for each single gene and if the p-value is smaller than 0.05, we report it. However, it will give us a lot of false positives because we did not consider multiple tests.&lt;/p&gt;
&lt;p&gt;Let’s start using a microarray data set in which thousands of genes are assayed at the same time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### This part is from the Edx online Harvard course 
## HarvardX: PH525.3x Advanced Statistics for the Life Sciences, week1

library(devtools)
library(qvalue)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;qvalue&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install_github(&amp;quot;genomicsclass/GSE5859Subset&amp;quot;)

library(GSE5859Subset)
data(GSE5859Subset)
dim(geneExpression)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8793   24&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Have a look at the data and objects available&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;geneExpression[1:6, 1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           GSM136508.CEL.gz GSM136530.CEL.gz GSM136517.CEL.gz
## 1007_s_at         6.543954         6.401470         6.298943
## 1053_at           7.546708         7.263547         7.201699
## 117_at            5.402622         5.050546         5.024917
## 121_at            7.892544         7.707754         7.461886
## 1255_g_at         3.242779         3.222804         3.185605
## 1294_at           7.531754         7.090270         7.466018
##           GSM136576.CEL.gz GSM136566.CEL.gz GSM136574.CEL.gz
## 1007_s_at         6.837899         6.470689         6.450220
## 1053_at           7.052761         6.980207         7.096195
## 117_at            5.304313         5.214149         5.173731
## 121_at            7.558130         7.819013         7.641136
## 1255_g_at         3.195363         3.251915         3.324934
## 1294_at           7.122145         7.058973         6.992396&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(sampleInfo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 24  4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(sampleInfo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     ethnicity       date         filename group
## 107       ASN 2005-06-23 GSM136508.CEL.gz     1
## 122       ASN 2005-06-27 GSM136530.CEL.gz     1
## 113       ASN 2005-06-27 GSM136517.CEL.gz     1
## 163       ASN 2005-10-28 GSM136576.CEL.gz     1
## 153       ASN 2005-10-07 GSM136566.CEL.gz     1
## 161       ASN 2005-10-07 GSM136574.CEL.gz     1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sampleInfo$filename&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;GSM136508.CEL.gz&amp;quot; &amp;quot;GSM136530.CEL.gz&amp;quot; &amp;quot;GSM136517.CEL.gz&amp;quot;
##  [4] &amp;quot;GSM136576.CEL.gz&amp;quot; &amp;quot;GSM136566.CEL.gz&amp;quot; &amp;quot;GSM136574.CEL.gz&amp;quot;
##  [7] &amp;quot;GSM136575.CEL.gz&amp;quot; &amp;quot;GSM136569.CEL.gz&amp;quot; &amp;quot;GSM136568.CEL.gz&amp;quot;
## [10] &amp;quot;GSM136559.CEL.gz&amp;quot; &amp;quot;GSM136565.CEL.gz&amp;quot; &amp;quot;GSM136573.CEL.gz&amp;quot;
## [13] &amp;quot;GSM136523.CEL.gz&amp;quot; &amp;quot;GSM136509.CEL.gz&amp;quot; &amp;quot;GSM136727.CEL.gz&amp;quot;
## [16] &amp;quot;GSM136510.CEL.gz&amp;quot; &amp;quot;GSM136515.CEL.gz&amp;quot; &amp;quot;GSM136522.CEL.gz&amp;quot;
## [19] &amp;quot;GSM136507.CEL.gz&amp;quot; &amp;quot;GSM136524.CEL.gz&amp;quot; &amp;quot;GSM136514.CEL.gz&amp;quot;
## [22] &amp;quot;GSM136563.CEL.gz&amp;quot; &amp;quot;GSM136564.CEL.gz&amp;quot; &amp;quot;GSM136572.CEL.gz&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(geneAnnotation)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      PROBEID  CHR     CHRLOC SYMBOL
## 1  1007_s_at chr6   30852327   DDR1
## 30   1053_at chr7  -73645832   RFC2
## 31    117_at chr1  161494036  HSPA6
## 32    121_at chr2 -113973574   PAX8
## 33 1255_g_at chr6   42123144 GUCA1A
## 34   1294_at chr3  -49842638   UBA7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;let’s look at one single gene&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g&amp;lt;- sampleInfo$group

e&amp;lt;- geneExpression[25,]

# t-test, expression should be normal distribution
qqnorm(e[g==1])
qqline(e[g==1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqnorm(e[g==0])
qqline(e[g==1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# perform t-test
t.test(e[g==1], e[g==0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  e[g == 1] and e[g == 0]
## t = 0.28382, df = 21.217, p-value = 0.7793
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.1431452  0.1884244
## sample estimates:
## mean of x mean of y 
##  10.52505  10.50241&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;do t-test for all the genes&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mytest&amp;lt;- function(x) t.test(x[g==1], x[g==0], var.equal=T)$p.value

## or we can use the genefilter package from bioconductor
## library(genefilter)
## results&amp;lt;- rowttests(geneExpression, factor(g))

pvals&amp;lt;- apply(geneExpression, 1, mytest)

sum(pvals&amp;lt; 0.05)  # how many pvalues are smaller than 0.05&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1383&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;have a look at the p-value distribution&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# there are 1383 genes with p value smaller than 0.05
# are all of them statistically different?
hist(pvals)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;simulate-multiple-comparisons-with-random-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;simulate multiple comparisons with random data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m&amp;lt;- nrow(geneExpression)
n&amp;lt;- ncol(geneExpression)

# generate random numbers
randomData&amp;lt;- matrix(rnorm(n*m), m, n)
nullpvalues&amp;lt;- apply(randomData, 1, mytest)
hist(nullpvalues)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;compare this histogram with the histogram above. what do you see?
Even if we randomly generated the data, you still see some pvalues are smaller than 0.05!! We randomly generated data, there should be no genes that deferentially expressed. However, we see a flat line across different p values.&lt;/p&gt;
&lt;p&gt;p values are random variables. Mathematically, one can &lt;a href=&#34;https://joyeuserrance.wordpress.com/2011/04/22/proof-that-p-values-under-the-null-are-uniformly-distributed/&#34;&gt;demonstrate&lt;/a&gt; that under the null hypothesis (and some assumptions are met, in this case, the test statistic T follows standard normal distribution), p-values follow a uniform (0,1) distribution, which means that P(p &amp;lt; p1) = p1. This means that the probability see a p value smaller than p1 is equal to p1. That being said, with a 100 t-tests, under the null (no difference between control and treatment), we will see 1 test with a p value smaller than 0.01. And we will see 2 tests with a p value smaller than 0.02 etc…
This explains why we see some p-values are smaller than 0.05 in our randomly generated numbers.&lt;/p&gt;
&lt;p&gt;In fact, checking the p-value distribution by histogram is a very important step during data analysis.
You may want to read a blog post by David Robinson &lt;a href=&#34;http://varianceexplained.org/statistics/interpreting-pvalue-histogram/&#34;&gt;How to interpret a p-value histogram&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-do-we-control-the-false-positives-for-multiple-comparisons&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How do we control the false positives for multiple comparisons?&lt;/h3&gt;
&lt;p&gt;One way is to use the Bonferroni correction to correct the familywise error rate (FWER):
define a particular comparison as statistically significant only when the P value is less than alpha(often 0.05) divided by the number of comparisons (p &amp;lt; alpha/m) &lt;span class=&#34;citation&#34;&gt;Motulsky (&lt;a href=&#34;#ref-motulsky2014intuitive&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; Third edition page 187. Say we computed 100 t-tests, and got 100 p values, we only consider the genes with a p value smaller than 0.05/100 as significant. This approach is very conservative and is used in Genome-wide association studies (GWAS). Since we often compare millions of genetic variations between (tens of thousands) cases and controls, this threshold will be very small! &lt;span class=&#34;citation&#34;&gt;Motulsky (&lt;a href=&#34;#ref-motulsky2014intuitive&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; Third edition page 188.&lt;/p&gt;
&lt;p&gt;Alternatively, we can use False Discovery Rate (FDR) to report the gene list.
&lt;strong&gt;FDR = #false positives/# called significant.&lt;/strong&gt;&lt;br /&gt;
This approach does not use the term statistically significant but instead use the term discovery.
Let’s control FDR for a gene list with &lt;code&gt;FDR = 0.05&lt;/code&gt;.
&lt;strong&gt;It means that of all the discoveries, 5% of them is expected to be false positives.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Benjamini &amp;amp; Hochberg (BH method) in 1995 proposed a way to control FDR:
Let k be the largest i such that &lt;code&gt;p(i) &amp;lt;= (i/m) * alpha&lt;/code&gt;, (m is the number of comparisons)
then reject H(i) for i =1, 2, …k&lt;/p&gt;
&lt;p&gt;This process controls the FDR at level alpha. The method sets a different threshold p value for each comparison. Say we computed 100 t-tests, and got 100 p values, and we want to control the FDR =0.05. We then rank the p values from small to big.
if p(1) &amp;lt;= 1/100 * 0.05, we then reject null hypothesis and accept the alternative.
if p(2) &amp;lt; = 2/100 * 0.05, we then reject the null and accept the alternative..
…..&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## order the pvals computed above and plot it.
alpha = 0.05
m = length(pvals)
#m is the number of 8793 comparisons 

plot(x=seq(1,100), y=pvals[order(pvals)][1:100])
abline(a=0, b=alpha/m)
title(&amp;quot;slop is alpha/m&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# let&amp;#39;s zoom in to look at the first 15 p values from small to big

plot(x=seq(1,100), y=pvals[order(pvals)][1:100], xlim=c(1,15))
abline(a=0, b=alpha/m)
title(&amp;quot;slop is alpha/m&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we can see that the 14th p value is bigger than its own threshold 
# which is computed by (0.05/m) * 14 = 7.960878e-05

# we will use p.adjust function and the method &amp;quot;fdr&amp;quot; or &amp;quot;BH&amp;quot; to
# correct the p value, what the p.adjust function does to to
# recalculate the p-value. ?p.adjust to see more
# p(i)&amp;lt;= (i/m) * alpha 
# p(i) * m/i &amp;lt;= alpha
# we can then only accept the returned if p.adjust(pvals) &amp;lt;= alpha
# number of p values smaller than their own thresholds after controlling FDR=0.05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we can see that the 14th p value is bigger than its own threshold ,which is computed by (0.05/m) * 14 = 7.960878e-05
we will use p.adjust function and the method “fdr” or “BH” to correct the p value, what the p.adjust function does is to recalculate the p-values.
p(i)&amp;lt;= (i/m) * alpha
p(i) * m/i &amp;lt;= alpha
we can then only accept the returned the p values if p.adjust(pvals) &amp;lt;= alpha&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum( p.adjust(pvals, method=&amp;quot;fdr&amp;quot;) &amp;lt; 0.05 )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;it is 13, the same as we saw from the figure.&lt;/p&gt;
&lt;p&gt;Another method by Storey in 2002 is the direct approach to FDR:
Let K be the largest i such that pi_0 * p(i) &amp;lt; (i/m) * alpha
then reject H(i) for i =1,2,…k
pi_0 is the estimate of the proportion of null hypothesis in the gene list is true, range from 0 to 1.
so when pi_0 is 1, then we have the Benjamini &amp;amp; Hochberg correction.
This method is less conservative than the BH method.
Use the qvalue function in the bioconductor package “qvalue”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum( qvalue(pvals)$qvalues &amp;lt; 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 22&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;it is 22, less conservative than the BH method.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note that FDR is a property of a list of genes. q value is defined for a specific gene:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;HarvardX: PH525.3x Advanced Statistics for the Life Sciences, week1, video lecture for FDR.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“But if you do want to assign a number to each gene, a simple thing you can do, is you can go gene by gene, and decide what would be the smallest FDR I would consider, that would include this gene in the list. And once you do that, then you have defined a q-value. And this is something that is very often reported in the list of genes”[4]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;HarvardX: PH525.3x Advanced Statistics for the Life Sciences, week1, quiz for FDR:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“To define the q-value we order features we tested by p-value then compute the FDRs for a list with the most significant, the two most significant, the three most significant, etc… The FDR of the list with the, say, m most significant tests is defined as the q-value of the m-th most significant feature. In other words, the q-value of a feature, is the FDR of the biggest list that includes that gene” [5]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I hope this post helps you better understand p values, FDR and q values. Sadly, many biologists do not understand them well and try to do p-hacking.&lt;/p&gt;
&lt;p&gt;Further read &lt;a href=&#34;https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106&#34;&gt;The Extent and Consequences of P-Hacking in Science&lt;/a&gt; and &lt;a href=&#34;https://www.thermofisher.com/blog/proteomics/whats-true-whats-false-proteostats-and-the-fdr/&#34;&gt;What’s True? What’s False? ProteoStats and the FDR&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-motulsky2014intuitive&#34;&gt;
&lt;p&gt;Motulsky, Harvey. 2014. &lt;em&gt;Intuitive Biostatistics: A Nonmathematical Guide to Statistical Thinking&lt;/em&gt;. Oxford University Press, USA.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>permutation test for PCA components</title>
      <link>/post/permute-test-for-pca-components/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/permute-test-for-pca-components/</guid>
      <description>&lt;p&gt;PCA is a critical method for dimension reduction for high-dimensional data.
High-dimensional data are data with features (p) a lot more than observations (n).
However, this is changing with single-cell RNAseq data. Now, we can sequence millions (n)
of single cells and each cell has ~20,000 genes/features (p).&lt;/p&gt;
&lt;p&gt;I suggest you read my &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/post/pca-in-action/&#34;&gt;previous blog post&lt;/a&gt; on using &lt;code&gt;svd&lt;/code&gt; to calculate PCs.&lt;/p&gt;
&lt;div id=&#34;single-cell-expression-data-pca&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Single-cell expression data PCA&lt;/h3&gt;
&lt;p&gt;In single-cell RNAseq analysis, feature selection will be performed first. e.g. In &lt;a href=&#34;https://github.com/satijalab/seurat&#34;&gt;&lt;code&gt;Seruat&lt;/code&gt;&lt;/a&gt;, most variable genes will be calculated by &lt;code&gt;FindVariableGenes&lt;/code&gt; and will be used for downstream analysis. The number of variable genes is in
the range of a couple of thousands (~2000). This further reduced number of features(p).&lt;/p&gt;
&lt;p&gt;Let’s take a look at the &lt;a href=&#34;https://github.com/satijalab/seurat/blob/master/R/dimensional_reduction.R#L70&#34;&gt;source code of &lt;code&gt;Seurat&lt;/code&gt;&lt;/a&gt; for
PCA:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if (rev.pca) {
    pcs.compute &amp;lt;- min(pcs.compute, ncol(x = data.use)-1)
    pca.results &amp;lt;- irlba(A = data.use, nv = pcs.compute, ...)
    sdev &amp;lt;- pca.results$d/sqrt(max(1, nrow(data.use) - 1))
    if(weight.by.var){
      gene.loadings &amp;lt;- pca.results$u %*% diag(pca.results$d)
    } else{
      gene.loadings &amp;lt;- pca.results$u
    }
    cell.embeddings &amp;lt;- pca.results$v
  }
  else {
    pcs.compute &amp;lt;- min(pcs.compute, nrow(x = data.use)-1)
    pca.results &amp;lt;- irlba(A = t(x = data.use), nv = pcs.compute, ...)
    gene.loadings &amp;lt;- pca.results$v
    sdev &amp;lt;- pca.results$d/sqrt(max(1, ncol(data.use) - 1))
    if(weight.by.var){
      cell.embeddings &amp;lt;- pca.results$u %*% diag(pca.results$d)
    } else {
      cell.embeddings &amp;lt;- pca.results$u
    }
  }
  rownames(x = gene.loadings) &amp;lt;- rownames(x = data.use)
  colnames(x = gene.loadings) &amp;lt;- paste0(reduction.key, 1:pcs.compute)
  rownames(x = cell.embeddings) &amp;lt;- colnames(x = data.use)
  colnames(x = cell.embeddings) &amp;lt;- colnames(x = gene.loadings)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the help page for &lt;code&gt;{Seruat::RunPCA}&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pc.genes    
Genes to use as input for PCA. Default is object@var.genes

rev.pca 
By default computes the PCA on the cell x gene matrix. Setting to true will compute it on gene x cell matrix.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Seurat&lt;/code&gt; uses &lt;a href=&#34;https://cran.r-project.org/web/packages/irlba/index.html&#34;&gt;irlba&lt;/a&gt; (Fast Truncated Singular Value Decomposition and Principal Components Analysis for Large Dense and Sparse Matrices) for PCA.
The &lt;code&gt;irlba&lt;/code&gt; is both faster and more memory efficient than the usual R &lt;code&gt;svd&lt;/code&gt; function for computing a few of the largest singular vectors and corresponding singular values of a matrix.&lt;/p&gt;
&lt;p&gt;By default, &lt;code&gt;RunPCA&lt;/code&gt; computes the PCA on the &lt;code&gt;cell (n) x gene (p)&lt;/code&gt; matrix.
One thing to note is that in linear algebra, a matrix is coded as n (rows are observations) X p (columns are features). That’s why by default, the &lt;code&gt;gene x cell&lt;/code&gt; original matrix is transposed first to &lt;code&gt;cell x gene&lt;/code&gt;: &lt;code&gt;irlba(A = t(x = data.use), nv = pcs.compute, ...)&lt;/code&gt;.
After &lt;code&gt;irlba&lt;/code&gt;, the &lt;code&gt;v&lt;/code&gt; matrix is the gene loadings, the &lt;code&gt;u&lt;/code&gt; matrix is the cell embeddings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;number-of-significant-pcs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;number of significant PCs&lt;/h3&gt;
&lt;p&gt;For downstream analysis, e.g. &lt;code&gt;{Seurat::FindClusters}&lt;/code&gt; only the PCs that significantly contribute to the variation of the data are used. &lt;code&gt;Seruat&lt;/code&gt; uses &lt;code&gt;JackStraw&lt;/code&gt; and &lt;code&gt;JackStrawplot&lt;/code&gt; function to achieve it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;JackStraw&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Randomly permutes a subset of data, and calculates projected PCA scores for these ‘random’ genes. Then compares the PCA scores for the ‘random’ genes with the observed PCA scores to determine statistical significance. End result is a &lt;strong&gt;p-value for each gene’s association with each principal component&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;JackStrawplot&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Plots the results of the JackStraw analysis for PCA significance. For each PC, plots a QQ-plot comparing the distribution of p-values for all genes across each PC, compared with a uniform distribution. Also determines a p-value for the overall significance of each PC.The p-value for each PC is based on a proportion test comparing the number of genes with a p-value below a particular threshold (score.thresh), compared with the proportion of genes expected under a uniform distribution of p-values.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The other day, I saw a tweet on permute the original matrix to calculate the significance of the PCs.
I forget the original tweet, but this is from a retweet: &lt;a href=&#34;https://twitter.com/MattOldach/status/1075037756563382274&#34; class=&#34;uri&#34;&gt;https://twitter.com/MattOldach/status/1075037756563382274&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;references:
This is called Horn’s Parallel Analysis (original paper &lt;a href=&#34;https://link.springer.com/article/10.1007%2FBF02289447&#34; class=&#34;uri&#34;&gt;https://link.springer.com/article/10.1007%2FBF02289447&lt;/a&gt; and a modification &lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/0013164495055003002?journalCode=epma&#34; class=&#34;uri&#34;&gt;https://journals.sagepub.com/doi/abs/10.1177/0013164495055003002?journalCode=epma&lt;/a&gt;. It’s a great method for removing noisy components.&lt;/p&gt;
&lt;p&gt;This is not exactly the same as what &lt;code&gt;Seurat&lt;/code&gt; is doing, but the idea is similar.
I want to put it down here for my future reference.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;permutation-test-for-pca&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;permutation “test” for PCA&lt;/h3&gt;
&lt;p&gt;The code below is copied from that tweet, credit goes to the author.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_eigenperm&amp;lt;- function(data, nperm = 1000){
        pca_out&amp;lt;- prcomp(data, scale. = T)
        eigenperm&amp;lt;- data.frame(matrix(NA, nperm, ncol(data)))
        n&amp;lt;- ncol(data)
        data_i&amp;lt;- data.frame(matrix(NA, nrow(data), ncol(data)))
        for (j in 1: nperm){
        for (i in 1:n){
                data_i[,i]&amp;lt;- sample(data[,i], replace = F)
        }
        pca.perm&amp;lt;- prcomp(data_i, scale. = T)
        eigenperm[j,]&amp;lt;- pca.perm$sdev^2
        }
        colnames(eigenperm)&amp;lt;- colnames(pca_out$rotation)
        eigenperm
        
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use the same &lt;code&gt;NCI60&lt;/code&gt; data set for demonstration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ──────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.1.0     ✔ purrr   0.2.5
## ✔ tibble  1.4.2     ✔ dplyr   0.7.8
## ✔ tidyr   0.8.2     ✔ stringr 1.3.1
## ✔ readr   1.3.1     ✔ forcats 0.3.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ─────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

library(ISLR)

ncidat&amp;lt;- NCI60$data
rownames(ncidat)&amp;lt;- NCI60$labs

dim(ncidat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]   64 6830&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fa_pca_perm&amp;lt;- pca_eigenperm(t(ncidat))
fa_pca&amp;lt;- prcomp(t(ncidat))
fa_pca_rand95&amp;lt;- 
        data.frame(Random_Eigenvalues = sapply(fa_pca_perm, quantile, 0.95)) %&amp;gt;%
        #95% percentile of randome eigenvalues
        mutate(PC = colnames(fa_pca$rotation)) %&amp;gt;%
        #add PC IDs as discrete var
        cbind(Eigenvalues = fa_pca$sdev^2)
#combine rand95 with real eigenvals

## only the first 9 PCs
fa_pca_rand95_long&amp;lt;-
        gather(fa_pca_rand95[1:9, ], key = Variable, value = Value, -PC)

ggplot(fa_pca_rand95_long, aes(PC, Value, fill = Variable)) +
        geom_bar(stat = &amp;quot;identity&amp;quot;, position = position_dodge())+
        labs(y=&amp;quot;Eigenvalue&amp;quot;, x=&amp;quot;&amp;quot;, fill= &amp;quot;&amp;quot;) +
        theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-04-permutate-test-for-pca-components_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see after PC6, the Eigenvalues are almost the same with the permuted data.
For single cell data, permutation can take a long time, that’s why in &lt;code&gt;JackStraw&lt;/code&gt; there is an
option &lt;code&gt;prop.freq&lt;/code&gt; (Proportion of the data to randomly permute for each replicate) to
permute only a subset of the data matrix.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The end of 2018</title>
      <link>/post/the-end-of-2018/</link>
      <pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-end-of-2018/</guid>
      <description>

&lt;p&gt;It is almost the end of 2018. It is a good time to review what I have achieved during the year
and look forward to a brand new 2019. I wrote a similar post for 2017 &lt;a href=&#34;http://crazyhottommy.blogspot.com/2017/12/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;some-highlights-of-the-year-2018&#34;&gt;Some highlights of the year 2018:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;My son Noah Tang was born in April. He is so lovely and we love him so much. Can&amp;rsquo;t believe he is
almost 9 months old.
&lt;img src=&#34;/img/noah.jpg&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Our epigenomic project was selected by the &lt;a href=&#34;https://bigdatau.ini.usc.edu/roadtrip&#34; target=&#34;_blank&#34;&gt;Data Science Road-Trip program&lt;/a&gt; by USC. I spent 2 weeks in PNNL and worked closely with &lt;a href=&#34;https://www.pnnl.gov/science/staff/staff_info.asp?staff_num=8785&#34; target=&#34;_blank&#34;&gt;Lisa Bramer&lt;/a&gt; and developed a pipeline to do feature selection using machine learning from a lot of chromHMM data sets. You can find the github repo &lt;a href=&#34;https://github.com/crazyhottommy/pyflow-chromForest/tree/vsurf_merge&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. I kept a note for everyday what I did as well at &lt;a href=&#34;https://github.com/crazyhottommy/Epigenome_RoadTrip&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. I will think about writing it up.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I finally migrated my &lt;a href=&#34;http://crazyhottommy.blogspot.com/&#34; target=&#34;_blank&#34;&gt;previous blog&lt;/a&gt; to blogdown which you are reading now :) Oh my, I love it. It makes blogging so much fun.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I taught the ChIP-seq lesson for &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/talk/2018-dibsi-course/&#34; target=&#34;_blank&#34;&gt;2018 ANGUS Next-Gen Sequence Analysis Workshop&lt;/a&gt; held in UC Davis from 7/1/2018 to 7/14/2018, and TAed for the rest of the sessions. It was a great teaching experience for me. I got to know many people and built connections. Most importantly, I enjoyed the teaching very much! Thanks &lt;a href=&#34;https://biology.ucdavis.edu/people/c-titus-brown&#34; target=&#34;_blank&#34;&gt;Titus Brown&lt;/a&gt; for the invitation. I highly recommend you to attend this workshop if you want to start learning sequencing data analysis. The environment is so welcoming and Titus is so hilarious:) I was in the workshop to learn in 2014 and now I am back to teach!&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I went back to University of Florida where I did my PhD to give a &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/talk/2018-uf-talk/&#34; target=&#34;_blank&#34;&gt;talk&lt;/a&gt;. It was very nice to be back home and catch up with my supervisor, other professors and some church friends!&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Several co-author &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/&#34; target=&#34;_blank&#34;&gt;papers/see the publications section&lt;/a&gt; are out in 2018. My first video shot for JOVE can be found at &lt;a href=&#34;https://www.jove.com/video/56972/an-integrated-platform-for-genome-wide-mapping-chromatin-states-using&#34; target=&#34;_blank&#34;&gt;https://www.jove.com/video/56972/an-integrated-platform-for-genome-wide-mapping-chromatin-states-using&lt;/a&gt;. I was nervous but It was fun.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In addition, two papers are out in Biorxiv at the end of 2018. I am co-first author in one of them. Both papers describe how epigenetic regulator KMT2D mediate tumor progression in melanoma and lung cancer, respectively. We are in the process submitting those papers to journals, but you can read more details at &lt;a href=&#34;https://www.biorxiv.org/content/early/2018/12/28/507202&#34; target=&#34;_blank&#34;&gt;https://www.biorxiv.org/content/early/2018/12/28/507202&lt;/a&gt; and &lt;a href=&#34;https://www.biorxiv.org/content/early/2018/12/27/507327&#34; target=&#34;_blank&#34;&gt;https://www.biorxiv.org/content/early/2018/12/27/507327&lt;/a&gt;.&lt;br /&gt;
The work was done in &lt;a href=&#34;http://railab.org/people.html&#34; target=&#34;_blank&#34;&gt;Kunal Rai&amp;rsquo;s lab&lt;/a&gt; where I had a chance to play with large amount of ChIP-seq data sets. My &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/project/snakemake-pipelines/&#34; target=&#34;_blank&#34;&gt;Snakemake pipeline&lt;/a&gt; is being used in the lab by others and has processed thousands of ChIP-seq data sets.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;My three &lt;a href=&#34;https://github.com/crazyhottommy&#34; target=&#34;_blank&#34;&gt;most stared github repos&lt;/a&gt; were cited in a paper &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fbioe.2018.00198/full&#34; target=&#34;_blank&#34;&gt;GitHub Statistics as a Measure of the Impact of Open-Source Bioinformatics Software&lt;/a&gt; by &lt;a href=&#34;https://medschool.vcu.edu/expertise/detail.html?id=mdozmorov&#34; target=&#34;_blank&#34;&gt;Mikhail G. Dozmorov&lt;/a&gt;. The table summarizing the popular github repos can be found at &lt;a href=&#34;https://github.com/mdozmorov/bioinformatics-impact/blob/master/tables/table_1.md&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. It&amp;rsquo;s over three years&amp;rsquo; cumulative work for those repos.  I am so glad that my notes are helpful for other researchers. I am always supportive for open science and believe sharing knowledge is the way to promote science progress.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I joined Harvard FAS Informatics as a bioinformatics scientist in October and started working on single-cell RNAseq with &lt;a href=&#34;https://www.dulaclab.com/&#34; target=&#34;_blank&#34;&gt;Dulac lab&lt;/a&gt; and will have a chance to play with other single molecule transcriptome data generated from &lt;a href=&#34;http://zhuang.harvard.edu/&#34; target=&#34;_blank&#34;&gt;Xiaowei Zhuang&amp;rsquo;s lab&lt;/a&gt;. I am really excited about learning more!&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I have my green card approved. This is important so I can work in the US without worrying about my visa status. Thanks to my previous postdoc adviser &lt;a href=&#34;https://www.jax.org/research-and-faculty/faculty/roel-verhaak&#34; target=&#34;_blank&#34;&gt;Roel Verhaak&lt;/a&gt;, &lt;a href=&#34;http://pinellolab.org/&#34; target=&#34;_blank&#34;&gt;Luca Pinello&lt;/a&gt;, &lt;a href=&#34;https://www.ialbert.me/&#34; target=&#34;_blank&#34;&gt;Istvan Albert&lt;/a&gt;,  &lt;a href=&#34;https://blogs.cornell.edu/sethupathylab/&#34; target=&#34;_blank&#34;&gt;Praveen Sethupathy&lt;/a&gt; and &lt;a href=&#34;https://medicine.iu.edu/faculty/14584/cheng-liang/&#34; target=&#34;_blank&#34;&gt;Liang Cheng&lt;/a&gt; for writing recommendation letters. I will need to visit Luca&amp;rsquo;s lab to say thanks personally, I have not had a chance to meet him after I moved to Harvard.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I finally got a chance to write my frist R packagefor single cell cluster stability testing. It is in github: &lt;a href=&#34;https://github.com/crazyhottommy/scclusteval&#34; target=&#34;_blank&#34;&gt;scclusteval&lt;/a&gt;. I implemented some functions for visualizing single cell data and evaluating cluster stability. I will make it public once I clean up a bit. I was so satisfied to have it installed by &lt;code&gt;devtools::install_github()&lt;/code&gt; and all functions and help pages are readily available as a package. I mean I am gradually transiting myself from an R user to R programmer. I know I still have a lot to learn, but this is exciting! By the way, I highly recommend the &lt;a href=&#34;https://github.com/r-lib/usethis&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;usethis&lt;/code&gt;&lt;/a&gt; package for writing R packages and read the &lt;a href=&#34;http://r-pkgs.had.co.nz/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;R pacakges book&lt;/code&gt;&lt;/a&gt; by Hadley Wickham. read &lt;a href=&#34;https://blog.methodsconsultants.com/posts/developing-r-packages-using-gitlab-ci-part-i/&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; and &lt;a href=&#34;https://www.hvitfeldt.me/blog/usethis-workflow-for-package-development/&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; blog post to get started on how to write a minimal functional R package.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-to-expect-in-2019&#34;&gt;What to expect in 2019&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;I will have a lot of opportunities to teach workshops on R, Unix and single-cell RNAseq at my current position.&lt;/li&gt;
&lt;li&gt;A lot to learn on neuroscience. I will audit classes taught by Catherine Dulac.&lt;/li&gt;
&lt;li&gt;I will guest lecture a few lessons for &lt;a href=&#34;https://canvas.harvard.edu/courses/39391&#34; target=&#34;_blank&#34;&gt;STAT 115: Introduction to Computational Biology and Bioinformatics&lt;/a&gt; taught by &lt;a href=&#34;http://liulab.dfci.harvard.edu/&#34; target=&#34;_blank&#34;&gt;Sheirly Liu&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I would love to learn more on deep learning and apply it to single cell data analysis. Currently, I am taking classes from Coursera.&lt;/li&gt;
&lt;li&gt;Attend several conferences. Would love to catch up with the twitter-verse in person.&lt;/li&gt;
&lt;li&gt;A few more papers to write. I have at least 2 first author papers to finish. It&amp;rsquo;s hanging there forever.&lt;/li&gt;
&lt;li&gt;Of course, spend time with the family.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A tale of two heatmap functions</title>
      <link>/post/a-tale-of-two-heatmap-functions/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-tale-of-two-heatmap-functions/</guid>
      <description>&lt;p&gt;You probably do not understand heatmap! Please read &lt;a href=&#34;http://www.opiniomics.org/you-probably-dont-understand-heatmaps/&#34;&gt;You probably don’t understand heatmaps by Mick Watson&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the blog post, Mick used &lt;code&gt;heatmap&lt;/code&gt; function in the &lt;code&gt;stats&lt;/code&gt; package, I will try to walk you through comparing &lt;code&gt;heatmap&lt;/code&gt;, and &lt;code&gt;heatmap.2&lt;/code&gt; from &lt;code&gt;gplots&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;Before I start, I want to quote this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The defaults of almost every heat map function in R does the hierarchical clustering first, then scales the rows then displays the image”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;see these two posts in biostar: &lt;a href=&#34;https://www.biostars.org/p/85527/&#34;&gt;post1&lt;/a&gt;&lt;br /&gt;
and &lt;a href=&#34;https://www.biostars.org/p/15285/&#34;&gt;post2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In other words, the &lt;code&gt;scale&lt;/code&gt; parameter inside the &lt;code&gt;heatmap&lt;/code&gt; functions only plays a role in displaying the colors, but does not involve clustering. This is critical to know! We will test to see if this hold true.&lt;/p&gt;
&lt;div id=&#34;heatmap-function-in-stats-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;heatmap function in stats package&lt;/h3&gt;
&lt;p&gt;Simulate the data. The example is exactly the same as in the Mick Watson’s blog post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stats)
library(gplots)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gplots&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:stats&amp;#39;:
## 
##     lowess&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h1 &amp;lt;- c(10,20,10,20,10,20,10,20)
h2 &amp;lt;- c(20,10,20,10,20,10,20,10)

l1 &amp;lt;- c(1,3,1,3,1,3,1,3)
l2 &amp;lt;- c(3,1,3,1,3,1,3,1)

mat &amp;lt;- rbind(h1,h2,l1,l2)

par(mfrow =c(1,1), mar=c(4,4,1,1))
plot(1:8,rep(0,8), ylim=c(0,35), pch=&amp;quot;&amp;quot;, xlab=&amp;quot;Time&amp;quot;, ylab=&amp;quot;Gene Expression&amp;quot;)

for (i in 1:nrow(mat)) {
lines(1:8,mat[i,], lwd=3, col=i)
}

legend(1,35,rownames(mat), 1:4, cex=0.7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this dummy example, we have four genes (l1, l2, h1, h2) that are measured in 8 time points.&lt;/p&gt;
&lt;p&gt;when we do clustering, we want to cluster l1 h1 together, and l2 h2 together as they have the same trend across the time points. However, you will notice that the scale of the expression levels of these four genes are different: with h1 and h2 are high, and l1 l2 are low.&lt;/p&gt;
&lt;p&gt;If we calculate the distance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#?dist to see other distance measures
dist(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           h1        h2        l1
## h2 28.284271                    
## l1 38.470768 40.496913          
## l2 40.496913 38.470768  5.656854&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## I will use the default for linkage method: complete
plot(hclust(dist(mat)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The default distance measure is &lt;a href=&#34;https://en.wikipedia.org/wiki/Euclidean_distance&#34;&gt;Eucledian distance&lt;/a&gt;, and you will see h1 and h2 are closer (28.284271), l1 and l2 are closer. This simply because how euclidean distance is defined.&lt;/p&gt;
&lt;p&gt;Let’s check the help for &lt;code&gt;?heatmap&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;scale character indicating if the values should be centered and scaled in either the row direction or the column direction, or none. The default is “row” if symm false, and “none” otherwise.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;symm
logical indicating if x should be treated symmetrically; can only be true when x is a square matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;So the default is scale row inside the &lt;code&gt;heatmap&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## default, I will give parameters explicitly
heatmap(mat, Colv=NA, col=greenred(10), scale = &amp;quot;row&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We do see h1,h2 cluster together; l1 l2 cluster together. Inside heatmap function, the default distance measure is the same as default of &lt;code&gt;dist&lt;/code&gt;, the linkage method is the same as &lt;code&gt;hclust&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you read the heatmap carefully, you will find that h1,h2 are with large values, but they have the same red color as l1,l2. &lt;strong&gt;This confirms that heatmap does clustering first, and then scale the row for representing the color!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;How about if we turn off scale inside &lt;code&gt;heatmap&lt;/code&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap(mat, Colv = NA, col=greenred(10), scale = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You will see the &lt;strong&gt;clustering does not change, but the color changed!&lt;/strong&gt; l1 and l2 are all green now (small values)&lt;/p&gt;
&lt;p&gt;How about if we scale the genes before we feed into heatmap? scale works on columns, transpose for rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat.scaled&amp;lt;- t(scale(t(mat), center=TRUE, scale = TRUE))
mat.scaled&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]       [,2]       [,3]       [,4]       [,5]       [,6]
## h1 -0.9354143  0.9354143 -0.9354143  0.9354143 -0.9354143  0.9354143
## h2  0.9354143 -0.9354143  0.9354143 -0.9354143  0.9354143 -0.9354143
## l1 -0.9354143  0.9354143 -0.9354143  0.9354143 -0.9354143  0.9354143
## l2  0.9354143 -0.9354143  0.9354143 -0.9354143  0.9354143 -0.9354143
##          [,7]       [,8]
## h1 -0.9354143  0.9354143
## h2  0.9354143 -0.9354143
## l1 -0.9354143  0.9354143
## l2  0.9354143 -0.9354143
## attr(,&amp;quot;scaled:center&amp;quot;)
## h1 h2 l1 l2 
## 15 15  2  2 
## attr(,&amp;quot;scaled:scale&amp;quot;)
##       h1       h2       l1       l2 
## 5.345225 5.345225 1.069045 1.069045&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see how the distance change among genes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dist(mat.scaled)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          h1       h2       l1
## h2 5.291503                  
## l1 0.000000 5.291503         
## l2 5.291503 0.000000 5.291503&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust(dist(mat.scaled)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Wow! h1 and l1 are clustered together; l2 and h2 are clustered together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap(mat.scaled, Colv = NA, col=greenred(10), scale = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I hope you understand now how scale the data &lt;strong&gt;before or after&lt;/strong&gt; can affect the looking of your heatmaps.&lt;/p&gt;
&lt;p&gt;If we do not scale the data beforehand, but we still want l1 and h1 cluster together; l2 and h2 cluster together, we can use a different distance measure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## correlation among the genes
cor(t(mat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    h1 h2 l1 l2
## h1  1 -1  1 -1
## h2 -1  1 -1  1
## l1  1 -1  1 -1
## l2 -1  1 -1  1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 1- correation to define the distance
1- cor(t(mat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    h1 h2 l1 l2
## h1  0  2  0  2
## h2  2  0  2  0
## l1  0  2  0  2
## l2  2  0  2  0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc &amp;lt;- hclust(as.dist(1-cor(t(mat))))
plot(hc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Indeed, h1 and l1 are together; h2 and l2 are together.&lt;/p&gt;
&lt;p&gt;Now, we plot the heatmap, but set scale = “none” inside heatmap&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap(mat, Colv = NA, Rowv=as.dendrogram(hc), col=greenred(10), scale = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Is this what you expect?! Yes, l1 and h2 are clustered together; l1 and h1 clustered together. but because the value range are different, you see l1 and l2 are green (small values); h1 and h2 are red (big values).&lt;/p&gt;
&lt;p&gt;The magic will happen if we set scale =“row” which is the default:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap(mat, Colv = NA, Rowv=as.dendrogram(hc), col=greenred(10), scale = &amp;quot;row&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I hope I have clarified a bit for the complications of heatmaps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;heatmap.2-function-in-gplots-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;heatmap.2 function in gplots package&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;scale character indicating if the values should be centered and scaled in either the row direction or the column direction, or none. The default is “none”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;The default is none!!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Please also pay attention to the Color Key of the heatmap.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## defaults of heatmap.2, scale is none
heatmap.2(mat, trace = &amp;quot;none&amp;quot;, Colv= NA, dendrogram = &amp;quot;row&amp;quot;, scale = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## FACT of heatmap functions in R: it does clustering first and then use the scale argument (if set) to represent the data.
heatmap.2(mat, trace = &amp;quot;none&amp;quot;, Colv= NA, dendrogram = &amp;quot;row&amp;quot;, scale = &amp;quot;row&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;how about we scale the data explicitly first and use euclidean distance. works fine!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap.2(t(scale(t(mat), center=TRUE, scale=TRUE)), trace = &amp;quot;none&amp;quot;, Colv= NA, dendrogram = &amp;quot;row&amp;quot;, scale = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-1--corx-as-distance-and-do-not-scale-before-hand&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;use 1- cor(x) as distance and do not scale before hand&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap.2(mat, trace = &amp;quot;none&amp;quot;, 
          Colv= NA, dendrogram = &amp;quot;row&amp;quot;,
          scale = &amp;quot;none&amp;quot;,
          hclust=function(x) hclust(x, method=&amp;#39;complete&amp;#39;), distfun=function(x) as.dist(1-cor(t(x))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-1--corx-as-distance-and-do-not-scale-before-hand-but-use-scale-in-the-heatmap.2-function-to-represent-the-colors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;use 1- cor(x) as distance and do not scale before hand, but use scale in the heatmap.2 function to represent the colors&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap.2(mat, trace = &amp;quot;none&amp;quot;, 
          Colv= NA, dendrogram = &amp;quot;row&amp;quot;,
          scale = &amp;quot;row&amp;quot;,
          hclust=function(x) hclust(x, method=&amp;#39;complete&amp;#39;), distfun=function(x) as.dist(1-cor(t(x))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scale-before-hand-and-use-1--corx-as-distance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;scale before hand and use 1- cor(x) as distance&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap.2(t(scale(t(mat), center=TRUE, scale=TRUE)), trace = &amp;quot;none&amp;quot;, 
          Colv= NA, dendrogram = &amp;quot;row&amp;quot;,
          hclust=function(x) hclust(x, method=&amp;#39;complete&amp;#39;), distfun=function(x) as.dist(1-cor(t(x))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that the dendrogram can be rotated and without changing the clustering. Check &lt;a href=&#34;https://cran.r-project.org/web/packages/dendsort/index.html&#34;&gt;Dendersort&lt;/a&gt; if you want to specify the order of the dendrogram.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://jokergoo.github.io/ComplexHeatmap-reference/book/&#34;&gt;ComplexHeatmap&lt;/a&gt; package which now I am using mainly &lt;strong&gt;does not&lt;/strong&gt; do any scaling inside the Heatmap function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-messages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Take home messages&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Generate heatmap is easy, but make sure to understand the parameters in each heatmap function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Understand your data and what you are looking for. Do you need to scale your data before clustering?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance measure and linkage method can drastically affect your clustering. Choose the right one for your data!. Please also read my &lt;a href=&#34;https://rpubs.com/crazyhottommy/heatmap_demystified&#34;&gt;last post&lt;/a&gt; on this theme.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
