<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DNA confesses Data speak on DNA confesses Data speak</title>
    <link>/</link>
    <description>Recent content in DNA confesses Data speak on DNA confesses Data speak</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Ming Tang</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>My opinionated selection of books/urls for bioinformatics/data science curriculum</title>
      <link>/post/my-opinionated-selection-of-books-for-bioinformatics-data-science-curriculum/</link>
      <pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/my-opinionated-selection-of-books-for-bioinformatics-data-science-curriculum/</guid>
      <description>

&lt;p&gt;There was a paper on this topic: &lt;a href=&#34;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003662&#34; target=&#34;_blank&#34;&gt;A New Online Computational Biology Curriculum&lt;/a&gt;.&lt;br /&gt;
I am going to provide a biased list below (I have read most of the books if not all). I say it is biased because you will see many books of R are from Hadely Wickham. I now use &lt;a href=&#34;https://www.tidyverse.org/&#34; target=&#34;_blank&#34;&gt;tidyverse&lt;/a&gt; most of the time.&lt;/p&gt;

&lt;h2 id=&#34;unix&#34;&gt;Unix&lt;/h2&gt;

&lt;p&gt;I suggest people who want to learn bioinformatics starting to learn unix commands first. It is so powerful and also omnipresent in high-performance computing settings (clouding computing etc). You can not survive without knowing it.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://linuxcommand.org/tlcl.php&#34; target=&#34;_blank&#34;&gt;The linux command line&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nostarch.com/howlinuxworks2&#34; target=&#34;_blank&#34;&gt;How Linux works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datascienceatthecommandline.com/&#34; target=&#34;_blank&#34;&gt;Data science at the command line&lt;/a&gt;
It was a fun reading for me and learned many tricks from this book.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rik.smith-unna.com/command_line_bootcamp&#34; target=&#34;_blank&#34;&gt;command line bootcamp&lt;/a&gt; interactive online session to learn unix. it is not working anymore unfortunately.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;computational-biology&#34;&gt;Computational biology&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://library.open.oregonstate.edu/computationalbiology/&#34; target=&#34;_blank&#34;&gt;A Primer for Computational Biology&lt;/a&gt; by Shawn T. O&amp;rsquo;Neil&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://practicalcomputing.org/&#34; target=&#34;_blank&#34;&gt;Practical computing for biologist&lt;/a&gt; by  Steven H.D Haddock and Casey W. Dunn This was the first book that I used to learn unix, regex and python.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://shop.oreilly.com/product/0636920030157.do&#34; target=&#34;_blank&#34;&gt;Bioinformatics data skills&lt;/a&gt; by Vince Buffalo. This is a must have! once you have some experience on bioinformatics.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;r-programming&#34;&gt;R programming&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://r4ds.had.co.nz/&#34; target=&#34;_blank&#34;&gt;R for data science&lt;/a&gt; by Garrett Grolemund and Hadley Wickham.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://adv-r.had.co.nz/&#34; target=&#34;_blank&#34;&gt;Advanced R&lt;/a&gt; by Hadley Wickham.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r-pkgs.had.co.nz/&#34; target=&#34;_blank&#34;&gt;R packages&lt;/a&gt; by Hadley Wickham. If you want to transit from an R user to developer, writing an R package will get you started.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;stats-r-focused&#34;&gt;Stats  (R focused)&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://rafalab.github.io/pages/harvardx.html&#34; target=&#34;_blank&#34;&gt;Data analysis for the life science with R&lt;/a&gt; by Micheal Love and Rafael A. Irizarry. I took the course on edx for 3 times! learned a ton! You can buy a paper book at &lt;a href=&#34;https://www.crcpress.com/Data-Analysis-for-the-Life-Sciences-with-R/Irizarry-Love/p/book/9781498775670&#34; target=&#34;_blank&#34;&gt;https://www.crcpress.com/Data-Analysis-for-the-Life-Sciences-with-R/Irizarry-Love/p/book/9781498775670&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://compgenomr.github.io/book/&#34; target=&#34;_blank&#34;&gt;Computational Genomics with R&lt;/a&gt; by Altuna Akalin.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/bios221/book/&#34; target=&#34;_blank&#34;&gt;Mordern statistics for mordern biology&lt;/a&gt; by Susan Holmes and Wolfgang Huber.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;python-programming&#34;&gt;Python programming&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://pythonforbiologists.com/advanced-python-for-biologists&#34; target=&#34;_blank&#34;&gt;(Advanced) python for biologist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wesmckinney.com/pages/book.html&#34; target=&#34;_blank&#34;&gt;Python for data analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/data-science-from/9781492041122/&#34; target=&#34;_blank&#34;&gt;Data science from scratch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;machine-learning&#34;&gt;Machine learning&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/&#34; target=&#34;_blank&#34;&gt;An intro to statistical learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/books/practical-data-science-with-r&#34; target=&#34;_blank&#34;&gt;Practical Data science with R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.manning.com/books/deep-learning-with-r&#34; target=&#34;_blank&#34;&gt;Deeping learning with R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;visualization&#34;&gt;Visualization&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://serialmentor.com/dataviz/&#34; target=&#34;_blank&#34;&gt;Fundamentals of Data Visualization&lt;/a&gt; by Claus O.Wilke&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/Visual-Display-Quantitative-Information/dp/1930824130&#34; target=&#34;_blank&#34;&gt;The Visual Display of Quantitative Information&lt;/a&gt; by Edward R. Tufte as well.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Those two books are not teaching you how to make figures programmatically (although the book by Claus was generated by Rmarkdown and the codes for all the figures can be found at &lt;a href=&#34;https://github.com/clauswilke/dataviz&#34; target=&#34;_blank&#34;&gt;https://github.com/clauswilke/dataviz&lt;/a&gt;). They teach you what kind of figures are informative and pleasant to eyes. &lt;a href=&#34;https://www.data-to-viz.com/&#34; target=&#34;_blank&#34;&gt;From data to viz&lt;/a&gt; is a website guiding you to choose the right graph for your data.&lt;/p&gt;

&lt;p&gt;I am still using R/ggplot2 for visualization.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://socviz.co/&#34; target=&#34;_blank&#34;&gt;Data Visualization&lt;/a&gt; by Kieran Healy.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cookbook-r.com/Graphs/&#34; target=&#34;_blank&#34;&gt;R Graphics Cookbook&lt;/a&gt; by Winston Chang.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/dp/0387981403&#34; target=&#34;_blank&#34;&gt;ggplot2: Elegant Graphics for Data Analysis&lt;/a&gt; by Hadely Wickham.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, I have compiled many useful links at &lt;a href=&#34;https://github.com/crazyhottommy/getting-started-with-genomics-tools-and-resources&#34; target=&#34;_blank&#34;&gt;https://github.com/crazyhottommy/getting-started-with-genomics-tools-and-resources&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s your favorite book that I have missed? Comment below!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/books.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Harvard FAS informatics nanocourse</title>
      <link>/talk/2019-harvard-fas-workshop/</link>
      <pubDate>Mon, 19 Aug 2019 00:00:00 -0400</pubDate>
      
      <guid>/talk/2019-harvard-fas-workshop/</guid>
      <description>&lt;p&gt;In this 2-week long Harvard FAS informatics nanocourse, I co-taught snakemake
for one afternoon and lead-instructed scRNAseq analysis for a full day.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The snakemake workshop material was adapted from Titus Brown and can be found at
&lt;a href=&#34;https://github.com/crazyhottommy/2019-snakemake-Harvard-Informatics-nanocourse&#34; target=&#34;_blank&#34;&gt;https://github.com/crazyhottommy/2019-snakemake-Harvard-Informatics-nanocourse&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The scRNAseq workshop material was developed using &lt;a href=&#34;https://github.com/jdblischak/workflowr&#34; target=&#34;_blank&#34;&gt;workflowr&lt;/a&gt; and can be found at
&lt;a href=&#34;https://crazyhottommy.github.io/scRNA-seq-workshop-Fall-2019/&#34; target=&#34;_blank&#34;&gt;https://crazyhottommy.github.io/scRNA-seq-workshop-Fall-2019/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The rest of the nanocourse material can be found at &lt;a href=&#34;https://github.com/harvardinformatics/micro-course&#34; target=&#34;_blank&#34;&gt;https://github.com/harvardinformatics/micro-course&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I used the &lt;a href=&#34;https://carpentries.org/&#34; target=&#34;_blank&#34;&gt;carpentries&lt;/a&gt; teaching style. A blue sticky note means OK and a red sticky note means having problems.&lt;/p&gt;

&lt;p&gt;Snakemake in action!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/snakemake_wp.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;scRNAseq workshop in a lamont library.
&lt;img src=&#34;/img/scRNAseq_wp.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Develop Bioconductor packages with docker container</title>
      <link>/post/develop-bioconductor-packages-with-docker-container/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/develop-bioconductor-packages-with-docker-container/</guid>
      <description>

&lt;h3 id=&#34;readings&#34;&gt;Readings&lt;/h3&gt;

&lt;p&gt;links to read:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.bioconductor.org/developers/package-guidelines/#rcode&#34; target=&#34;_blank&#34;&gt;https://www.bioconductor.org/developers/package-guidelines/#rcode&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Bioconductor/Contributions&#34; target=&#34;_blank&#34;&gt;https://github.com/Bioconductor/Contributions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;use container  &lt;a href=&#34;https://github.com/Bioconductor/bioconductor_full&#34; target=&#34;_blank&#34;&gt;https://github.com/Bioconductor/bioconductor_full&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am following the last link.&lt;/p&gt;

&lt;h3 id=&#34;pull-the-container&#34;&gt;pull the container&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull bioconductor/bioconductor_full:devel
docker images 

REPOSITORY                       TAG                  IMAGE ID            CREATED             SIZE
bioconductor/bioconductor_full   devel                ae3ec2be7376        3 hours ago         5.7GB
seuratv3                         latest               9b358ab1fd63        2 days ago          2.76GB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is 5.7G in size.&lt;/p&gt;

&lt;p&gt;start the Rstuido from the image. I have another Rstudio instance using port 8787, let me use a different one (e.g. 8080).  docker Rstudio default port is &lt;code&gt;8787&lt;/code&gt;, change the host port to &lt;code&gt;8080&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p ~/R/host-site-library

docker run                                      \
  -e PASSWORD=&amp;quot;xyz&amp;quot;                   \
  -p 8080:8787                                \
  -v ~/R/host-site-library:/usr/local/lib/R/host-site-library  \
  -v ~/github_repos:/home/rstudio \
  bioconductor/bioconductor_full:devel
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;NOTE: The path &lt;code&gt;~/R/host-site-library&lt;/code&gt; is mapped to .libPaths() in R. So, when R is started, all the libraries in the host directory &lt;code&gt;host-site-library&lt;/code&gt; are available to R. It is stored on your machine mounted from the volume you fill in place of host-site-library.&lt;/p&gt;

&lt;p&gt;The mounted path must be an &lt;strong&gt;absolute path&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I also mounted the &lt;code&gt;github_repo&lt;/code&gt; folder in my host machine to the docker home directory. Because every time you exit a container and start it again, the modification you did to the container will be gone (unless you make an modified image and use that for the next time). I will save the R package in my &lt;code&gt;~/github_repo&lt;/code&gt; folder in the host machine.&lt;/p&gt;

&lt;p&gt;type &lt;code&gt;localhost:8080&lt;/code&gt;, you should see the Rstudio login page. username is &lt;code&gt;rstudio&lt;/code&gt;, password is &lt;code&gt;xyz&lt;/code&gt; in this dummy example.&lt;/p&gt;

&lt;p&gt;In Rstudio:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; .libPaths()
[1] &amp;quot;/usr/local/lib/R/host-site-library&amp;quot; &amp;quot;/usr/local/lib/R/site-library&amp;quot;     
[3] &amp;quot;/usr/local/lib/R/library&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;you will see &lt;code&gt;/usr/local/lib/R/host-site-library&lt;/code&gt; is in the &lt;code&gt;libpath&lt;/code&gt;, that corresponds to the &lt;code&gt;~/R/host-site-library&lt;/code&gt; in the host machine, if you do package installation, it will be installed in the&lt;code&gt;~/R/host-site-library&lt;/code&gt; .&lt;/p&gt;

&lt;h3 id=&#34;start-an-r-package-use-usethis&#34;&gt;start an R package use usethis&lt;/h3&gt;

&lt;p&gt;follow these two blog posts and &lt;a href=&#34;http://r-pkgs.had.co.nz/&#34; target=&#34;_blank&#34;&gt;R packages book&lt;/a&gt; by Hadley Wickham:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://blog.methodsconsultants.com/posts/developing-r-packages-using-gitlab-ci-part-i/&#34; target=&#34;_blank&#34;&gt;https://blog.methodsconsultants.com/posts/developing-r-packages-using-gitlab-ci-part-i/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.hvitfeldt.me/blog/usethis-workflow-for-package-development/&#34; target=&#34;_blank&#34;&gt;https://www.hvitfeldt.me/blog/usethis-workflow-for-package-development/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;install.packages(c(&amp;quot;devtools&amp;quot;, &amp;quot;roxygen2&amp;quot;, &amp;quot;usethis&amp;quot;, &amp;quot;available&amp;quot;, &amp;quot;rmarkdown&amp;quot;))

## check if the package name is avaiable (not used by others)
library(available)
available(&amp;quot;myawesomepackage&amp;quot;)

library(usethis)
usethis::create_package(&amp;quot;~/myawesomepackage&amp;quot;)

use_git()
use_github()
use_mit_license(&amp;quot;Ming Tang&amp;quot;)
usethis::use_pipe()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add a function&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;usethis::use_r(&amp;quot;myawesomefunc&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On mac:&lt;br /&gt;
&lt;code&gt;command + option + shift + R&lt;/code&gt; for inserting roxygen comment.&lt;br /&gt;
&lt;code&gt;command + shift + D&lt;/code&gt; for documentation.&lt;br /&gt;
&lt;code&gt;command + shfit + B&lt;/code&gt; for building package.&lt;/p&gt;

&lt;p&gt;add more functions, repeat.&lt;/p&gt;

&lt;h3 id=&#34;next&#34;&gt;Next&lt;/h3&gt;

&lt;p&gt;Next is to add test and setup some continuous integration. Read this &lt;a href=&#34;https://jef.works/blog/2019/02/17/automate-testing-of-your-R-package/&#34; target=&#34;_blank&#34;&gt;Automate testing of your R package using Travis CI, Codecov, and testthat&lt;/a&gt; by Jean Fan.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Snakemake pipeline post-processing scATAC-seq</title>
      <link>/project/single-cell-atacseq/</link>
      <pubDate>Wed, 17 Jul 2019 00:00:00 -0400</pubDate>
      
      <guid>/project/single-cell-atacseq/</guid>
      <description>

&lt;h3 id=&#34;what-does-it-do&#34;&gt;What does it do?&lt;/h3&gt;

&lt;p&gt;For single cell ATACseq experiment, one gets a merged bam file for all cells. After going through clustering, one groups similar cells into cell types (cell states). This workflow will split the bam by clusters to create a pseudo bulk bam for each cluster, create bigwig tracks for visulization, call peaks for each cluster and merge the peaks across the clusters. Finally it will count reads per peak per cell from the original bam file on the merged peaks.&lt;/p&gt;

&lt;p&gt;In the future, the peak calling software should be barcode aware, so one does not need to split the bam file by cluster. But for now, I have this work for me.&lt;/p&gt;

&lt;p&gt;You can find the workflow at my &lt;a href=&#34;https://github.com/crazyhottommy/pyflow-scATACseq&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/rulegraph_scATAC.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Run Rstudio server with singularity on HPC</title>
      <link>/post/run-rstudio-server-with-singularity-on-hpc/</link>
      <pubDate>Sun, 09 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/run-rstudio-server-with-singularity-on-hpc/</guid>
      <description>

&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;

&lt;p&gt;Please read the following before go ahead:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;what is &lt;a href=&#34;https://www.docker.com/&#34; target=&#34;_blank&#34;&gt;docker&lt;/a&gt;?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;what is &lt;a href=&#34;https://www.rocker-project.org/&#34; target=&#34;_blank&#34;&gt;Rocker&lt;/a&gt;?&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;what is &lt;a href=&#34;https://www.sylabs.io/docs/&#34; target=&#34;_blank&#34;&gt;singularity&lt;/a&gt;?&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;from Harvard Research computing website: &lt;a href=&#34;https://www.rc.fas.harvard.edu/resources/documentation/software/singularity-on-odyssey/&#34; target=&#34;_blank&#34;&gt;Odyssey has singularity installed&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why Singularity?
There are some important differences between Docker and Singularity:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;Docker and Singularity have their own container formats.&lt;/li&gt;
&lt;li&gt;Docker containers may be imported to run via Singularity.&lt;/li&gt;
&lt;li&gt;Docker containers need root privileges for full functionality which is not suitable for a shared HPC environment.&lt;/li&gt;
&lt;li&gt;Singularity allows working with containers as a regular user. No sudo is required,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our &lt;a href=&#34;https://informatics.fas.harvard.edu/&#34; target=&#34;_blank&#34;&gt;informatics group&lt;/a&gt; has several big memory (1TB) computing nodes that allow us to run interactive jobs. I want to have big memory to run Rstudio for my scRNAseq data.&lt;/p&gt;

&lt;h3 id=&#34;run-rstudio-server-with-singularity&#34;&gt;Run Rstudio server with singularity&lt;/h3&gt;

&lt;p&gt;I basically followed this tutorial &lt;a href=&#34;https://www.rocker-project.org/use/singularity/&#34; target=&#34;_blank&#34;&gt;https://www.rocker-project.org/use/singularity/&lt;/a&gt; written by my colleague Nathan Weeks sitting in the same office with me. Thanks!&lt;/p&gt;

&lt;p&gt;First, go to &lt;a href=&#34;https://www.rocker-project.org/images/&#34; target=&#34;_blank&#34;&gt;https://www.rocker-project.org/images/&lt;/a&gt; choose the image you want. I use &lt;code&gt;tidyverse&lt;/code&gt; heavily, so I downloaded the &lt;code&gt;tidyverse&lt;/code&gt; image buit upon &lt;code&gt;Rstudio&lt;/code&gt; image&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## ssh to remote HPC and pull the docker image by singularity
ssh bio1
mkdir singularity-images; cd !$
singularity pull --name rstudio.simg docker://rocker/tidyverse:latest


# This example bind mounts the /project directory on the host into the Singularity container.
# By default the only host file systems mounted within the container are $HOME, /tmp, /proc, /sys, and /dev.
# type in the password you want to set, make it more complicated than this dummy one
PASSWORD=&#39;xyz&#39; singularity exec --bind=/project  rstudio.simg rserver --auth-none=0  --auth-pam-helper-path=pam-helper --www-address=127.0.0.1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;back to my mac and connect to it via &lt;a href=&#34;https://www.ssh.com/ssh/tunneling/&#34; target=&#34;_blank&#34;&gt;SSH tunnel&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;Nathan explained by drawing the following.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/ssh_tunnel.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh -Nf -L 8787:localhost:8787 bio1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;on my local mac, open &lt;code&gt;localhost:8787&lt;/code&gt; in web browser, type in the Odyssey (HPC) user name and password (xyz in this dummy example). Rstudio server now is ready for me! Magic!!!&lt;/p&gt;

&lt;p&gt;Note: if mulitple people using the same node for Rstudio sever, you will need to pick a different
port than &lt;code&gt;8787&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;one-more-note-on-r-packages&#34;&gt;One more note on R packages&lt;/h3&gt;

&lt;p&gt;create an &lt;code&gt;.Renviron&lt;/code&gt; file in your home diretory&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# User-installed R packages go into their home directory
echo &#39;R_LIBS_USER=~/R/%p-library/%v&#39; &amp;gt;&amp;gt; ${HOME}/.Renviron
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The platform and version will be replaced by the corresponding R versions&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls ~/R/x86_64-pc-linux-gnu-library/
3.6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;install packages inside Rstudio and the packages will be installed to &lt;code&gt;~/R/x86_64-pc-linux-gnu-library/3.6&lt;/code&gt;. R version in this singularity image is R3.6. Note that if you use R on command line at the remote machine and use the same version of R. the library may not be compatible. e.g. singularity container is based on debian （Ubuntu) and HPC is based on RPM (CentOS). One may need to have mulitiple &lt;code&gt;.Renviron&lt;/code&gt; file and switch back and forth depending on which R one is using. If you have better options, please let me know!&lt;/p&gt;

&lt;h3 id=&#34;jump-to-other-folders&#34;&gt;Jump to other folders&lt;/h3&gt;

&lt;p&gt;by default, Rstudio opens the home directory. if you want to go to other folders, you can click &lt;code&gt;...&lt;/code&gt; in the file pane.
You can then type in the path you want to jump to.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/change_path.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;submit-a-slurm-job&#34;&gt;Submit a slurm job&lt;/h3&gt;

&lt;p&gt;If you do not have a big computing node that you can run interactive job, you can follow Nathan&amp;rsquo;s &lt;a href=&#34;https://www.rocker-project.org/use/singularity/&#34; target=&#34;_blank&#34;&gt;tutorial&lt;/a&gt; on how to submit slurm job to run Rstudio server with singularity.&lt;/p&gt;

&lt;h3 id=&#34;fix-home-directory-filled-up-issue&#34;&gt;Fix home directory filled up issue&lt;/h3&gt;

&lt;p&gt;I am enjoying Rstudio with my HPC large computing nodes and suddenly I got emails from the HPC staff saying I am using up my home directory space. It turns out Rstudio writes the suspended session files to &lt;code&gt;~/.rstudio/&lt;/code&gt; folder. I &lt;code&gt;ncdu&lt;/code&gt; the folder and it is 34G! I googled around and found exactly this &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/218417097-Filling-up-the-home-directory-with-RStudio-Server&#34; target=&#34;_blank&#34;&gt;Filling up the home directory with RStudio Server&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One of the solution is to turn off session time out.&lt;/p&gt;

&lt;p&gt;put  &lt;code&gt;session-timeout-minutes=0&lt;/code&gt; in the &lt;code&gt;/etc/rstudio/rsession.conf&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Let me take a look at the file inside the container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;singularity shell rstudio.simg

cat /etc/rstudio/rsession.conf
# R Session Configuration File

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is an empty file. I will make a
rsession.conf file in the home directory of the host machine
adding that one line.&lt;/p&gt;

&lt;p&gt;Now, bind the modified rsession.conf file in host to the ression.conf file
inside the container:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat ~/resession.conf
# R Session Configuration File
session-timeout-minutes=0

# now open rstudio server
PASSWORD=&#39;xyz&#39; singularity exec --bind=~/rsession.conf:/etc/rstudio/rsession.conf  rstudio.simg rserver --auth-none=0  --auth-pam-helper-path=pam-helper --www-address=127.0.0.1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should fix the problem :)&lt;/p&gt;

&lt;p&gt;Nathan dived into the source code of Rsutido server &lt;a href=&#34;https://github.com/rstudio/rstudio/blob/master/src/cpp/server/ServerSessionManager.cpp#L111&#34; target=&#34;_blank&#34;&gt;https://github.com/rstudio/rstudio/blob/master/src/cpp/server/ServerSessionManager.cpp#L111&lt;/a&gt;
and documentation &lt;a href=&#34;https://docs.rstudio.com/ide/server-pro/r-sessions.html#user-and-group-profiles&#34; target=&#34;_blank&#34;&gt;https://docs.rstudio.com/ide/server-pro/r-sessions.html#user-and-group-profiles&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The enironment name of time out is &lt;code&gt;RSTUDIO_SESSION_TIMEOUT&lt;/code&gt;, so one can do&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;PASSWORD=&#39;xyz&#39; RSTUDIO_SESSION_TIMEOUT=&#39;0&#39; singularity exec rstudio.simg rserver --auth-none=0  --auth-pam-helper-path=pam-helper --www-address=127.0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to have the same effect of setting up the &lt;code&gt;rsession.conf&lt;/code&gt; file.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calculate scATACseq TSS enrichment score</title>
      <link>/post/calculate-scatacseq-tss-enrichment-score/</link>
      <pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/calculate-scatacseq-tss-enrichment-score/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://www.encodeproject.org/data-standards/terms/#enrichment&#34; target=&#34;_blank&#34;&gt;TSS enrichment score&lt;/a&gt; serves as an important quality control metric for ATACseq data. I want to write a script for single cell ATACseq data.&lt;/p&gt;

&lt;p&gt;From the Encode page:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Transcription Start Site (TSS) Enrichment Score - The TSS enrichment calculation is a signal to noise calculation. The reads around a reference set of TSSs are collected to form an aggregate distribution of reads centered on the TSSs and extending to 1000 bp in either direction (for a total of 2000bp). This distribution is then normalized by taking the average read depth in the 100 bps at each of the end flanks of the distribution (for a total of 200bp of averaged data) and calculating a fold change at each position over that average read depth. This means that the flanks should start at 1, and if there is high read signal at transcription start sites (highly open regions of the genome) there should be an increase in signal up to a peak in the middle. We take the signal value at the center of the distribution after this normalization as our TSS enrichment metric. Used to evaluate ATAC-seq.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It was not so clear to me from the definition on how &lt;strong&gt;EXACTLY&lt;/strong&gt; this score is calculated.&lt;/p&gt;

&lt;p&gt;I inspected the &lt;a href=&#34;https://github.com/jianhong/ATACseqQC/blob/master/R/TSSEscore.R#L80&#34; target=&#34;_blank&#34;&gt;source code&lt;/a&gt; of  &lt;code&gt;ATACseqQC&lt;/code&gt; which calculates the TSS enrichment score for bulk ATACseq data, but I think it is not calculating it the right way as described by the ENCODE page.&lt;/p&gt;

&lt;p&gt;I reached out to &lt;a href=&#34;https://twitter.com/Satpathology&#34; target=&#34;_blank&#34;&gt;Ansu Satpathy&lt;/a&gt; (thanks!), and got a script written by Jeffrey Granja, who are the authors of this recent scATACseq paper:
&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/610550v1&#34; target=&#34;_blank&#34;&gt;Massively parallel single-cell chromatin landscapes of human immune cell development and intratumoral T cell exhaustion (2019)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I studied the script and also got the confirmation from ENCODE how they calculate the TSS enrichment score
&lt;a href=&#34;https://github.com/ENCODE-DCC/atac-seq-pipeline/issues/50&#34; target=&#34;_blank&#34;&gt;https://github.com/ENCODE-DCC/atac-seq-pipeline/issues/50&lt;/a&gt; by a python script.&lt;/p&gt;

&lt;p&gt;To work with this coverage type of data in R, I want to take advantage of the data structure &lt;code&gt;View&lt;/code&gt; in bioconductor, so I borrowed some codes from &lt;a href=&#34;https://bioconductor.org/packages/release/bioc/vignettes/genomation/inst/doc/GenomationManual.html&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;genomation::ScoreMatrix&lt;/code&gt;&lt;/a&gt; instead of using the script sent by Ansu. It is a very nice package by &lt;a href=&#34;https://twitter.com/AltunaAkalin&#34; target=&#34;_blank&#34;&gt;Altuna Akalin&lt;/a&gt;. A side note, he has a very nice book you might be interested in: &lt;a href=&#34;http://compgenomr.github.io/book/how-to-contribute.html&#34; target=&#34;_blank&#34;&gt;Computational Genomics with R&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Below, I ended up with a hybrid script from multiple sources.
Now it works with the 10x cellranger-atac output &lt;code&gt;fragment.tsv.gz&lt;/code&gt;. One can tweak it to work with the bam file. However, the bam file is 25G, R takes a long time to parse it.&lt;/p&gt;

&lt;p&gt;I explain what the script does:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;for each TSS, get per base coverage for the 1000 bp flanking region(flank = 1000).&lt;/li&gt;
&lt;li&gt;do this for all TSSs, We get a matrix of #TSS x 2000 bp dimension.&lt;/li&gt;
&lt;li&gt;do a column sum of the matrix.&lt;/li&gt;
&lt;li&gt;sum of the coverage of the endFlank (100bp) at both ends and divide by 200 bp to get a
normalization factor.&lt;/li&gt;
&lt;li&gt;divide the the normalization factor for -1900 to + 1900 bp to get per base normalized coverage.&lt;/li&gt;
&lt;li&gt;do a smoothing with a defined window (50bp by default) using &lt;code&gt;zoo::rollmean&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;select the highest value within a window (highest_tss_flank, 50 bp by default) around the TSS because the highest peak is not necessary at exactly the TSS site (position 0)&lt;/li&gt;
&lt;li&gt;repeat 1-7 for all cells.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;extra-technical-notes&#34;&gt;Extra technical notes:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;One thing to note is that one needs to filter out the TSSs which are not within the coverage. e.g. A TSS with 1000 bp flanking regions fall out of the coverage.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;use only the common chromosomes between coverage and the txs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;convert GRanges to IntergerRangesList does not maintain the order of the GRanges.
so a unique id was given for each Ranges, and the matrix can be reordered according to this unique id. That&amp;rsquo;s what &lt;code&gt;constrainRanges()&lt;/code&gt; does. read this thread for more &lt;a href=&#34;https://stat.ethz.ch/pipermail/bioc-devel/2016-June/009433.html&#34; target=&#34;_blank&#34;&gt;https://stat.ethz.ch/pipermail/bioc-devel/2016-June/009433.html&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-long-it-takes&#34;&gt;How long it takes.&lt;/h3&gt;

&lt;p&gt;It took me around ~15 seconds to calculate the TSS enrichment score for a single cell.
1.213291 hours for 5000 PBMC cells using 15 workers (not too bad :).&lt;/p&gt;

&lt;h3 id=&#34;r-code&#34;&gt;R code&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(GenomicRanges)
library(dplyr)

#&#39; checkClass function
#&#39; 
#&#39; check whether the x object corresponds to the given class
#&#39;
#&#39; @param x object
#&#39; @param class.name class name
#&#39; @param var.name uses x object
#&#39; @keywords internal
checkClass = function(x, class.name, var.name = deparse(substitute(x))){
  
  fun.name = match.call(call=sys.call(sys.parent(n=1)))[[1]]
  if(!class(x) %in% class.name)
    stop(paste(fun.name,&#39;: &#39;, 
               var.name, 
               &#39; is not of class: &#39;, 
               paste(class.name, collapse=&#39; &#39;), 
               &#39;\n&#39;, sep=&#39;&#39;))
}

### remove the tss that do not have coverage
### I took some code from the ScoreMatrix.R function in the genomation package.
### give the credit due :)
### see https://github.com/BIMSBbioinfo/genomation/blob/master/R/scoreMatrix.R#L113
constrainRanges = function(target, windows){
  
  checkClass(target, c(&#39;SimpleRleList&#39;,&#39;RleList&#39;,&#39;CompressedRleList&#39;))
  checkClass(windows, &#39;GRanges&#39;)
  
  mcols(windows)$X_rank = 1:length(windows)
  r.chr.len = elementNROWS(target)
  constraint = GRanges(seqnames=names(r.chr.len),
                       IRanges(start=rep(1,length(r.chr.len)),
                               end=as.numeric(r.chr.len)))
  # suppressWarnings is done becuause GenomicRanges function give warnings 
  #if you don&#39;t have the same seqnames in both objects
  win.list.chr = suppressWarnings(subsetByOverlaps(windows, 
                                                   constraint,
                                                   type = &amp;quot;within&amp;quot;,
                                                   ignore.strand = TRUE))
  
  if(length(win.list.chr) == 0)
    stop(&#39;All windows fell have coordinates outside windows boundaries&#39;)
  return(win.list.chr)
}



#&#39; Calculate tss enrichment score from 10xscATAC fragment file
#&#39;
#&#39; @param frag_gz_file  fragment.tsv.gz file from 10x cellranger-atac output or 
#&#39; anyother tool but in the same format.
#&#39; @param txs  a txdb object
#&#39; @param flank flanking bp of tss (upstream and downstream)
#&#39; @param endFlank  bp end flanks of flank for local noise control
#&#39;     flank               flank
#&#39;  ---------------|-----------------
#&#39;                tss
#&#39;  ---                           ---
#&#39;  endFlank                     endFlank
#&#39;  
#&#39; @param highest_tss_flank bp flanking tss windown for choosing the highest tss score.
#&#39; The highest tss enrichment score is not always exactly at tss.
#&#39; @param barcodeList valid barcode list, a file with one column 
#&#39; @param smooth window size to smooth 
#&#39; @param strand.aware consider tss strandness when calculating 
#&#39;
#&#39; @return
#&#39; @export
#&#39;
#&#39; @examples
#&#39; library(TxDb.Hsapiens.UCSC.hg19.knownGene)
#&#39; library(dplyr); library(readr); library(BiocParallel)
#&#39; txs &amp;lt;- transcripts(TxDb.Hsapiens.UCSC.hg19.knownGene)
#&#39; scores&amp;lt;- TssEnrichmentFromFrags(&amp;quot;fragment.tsv.gz&amp;quot;, txs = txs)

TssEnrichmentFromFrags &amp;lt;- function(frag_gz_file,
                               txs,
                               flank = 1000,
                               endFlank = 100,
                               highest_tss_flank= 50,
                               smooth = 50,
                               strand.aware = TRUE,
                               workers = 1,
                               barcodeList = NULL){
        
        # Make GRanges of fragments that are solid for the cells that we care about
        frags_valid &amp;lt;- data.table::fread(paste0(&amp;quot;zcat &amp;lt; &amp;quot;, frag_gz_file)) %&amp;gt;% 
                data.frame() %&amp;gt;% 
                mutate(V2 = V2 + 1) %&amp;gt;% # make it 1 based for R
                GenomicRanges::makeGRangesFromDataFrame(seqnames.field = &amp;quot;V1&amp;quot;, start.field = &amp;quot;V2&amp;quot;, end.field = &amp;quot;V3&amp;quot;, keep.extra.columns = TRUE)
        if (!is.null(barcodeList)){
                validBarcodes&amp;lt;- read_tsv(barcodeList, col_names = F)
                frags_valid&amp;lt;- frags_valid[frags_valid$V4 %in% validBarcodes$X1]
        }
        
        # common chromosome names, do it per cell instead, see TssEnrichmentSingleCell
        seqlev&amp;lt;- intersect(seqlevels(frags_valid), seqlevels(txs))
        frags_valid&amp;lt;- keepSeqlevels(frags_valid, seqlev, pruning.mode=&amp;quot;coarse&amp;quot;)
        
        # calculate coverage per cell
        frags_valid_per_cell&amp;lt;- split(frags_valid, frags_valid$V4)
        
       
        # this step can take minutes 
        multicoreParam &amp;lt;- BiocParallel::MulticoreParam(workers = workers)
        # can add the chromosome length as additional argument for `coverage`
        # to get 0 coverages if there are no reads there. 
        cvgs&amp;lt;- bplapply(frags_valid_per_cell, function(x) coverage(x), BPPARAM = multicoreParam)
        
        txs &amp;lt;- unique(txs)
        
        txs.flanks&amp;lt;- promoters(txs, upstream = flank, 
                            downstream = flank)
        txs.length&amp;lt;- length(txs.flanks)
        
        TssEnrichmentScores&amp;lt;- BiocParallel::bplapply(cvgs, TssEnrichmentSingleCell, txs.flanks, strand.aware = strand.aware, endFlank = endFlank, flank = flank, highest_tss_flank, smooth = smooth, BPPARAM = multicoreParam)

        enrichment&amp;lt;- do.call(&amp;quot;rbind&amp;quot;, TssEnrichmentScores)
        return(enrichment)
}    

TssEnrichmentSingleCell&amp;lt;- function(cvg, txs.flanks, strand.aware = TRUE, flank = 1000,
                               endFlank = 100,
                               highest_tss_flank= 50,
                               smooth = 50 ){
        ## remove tss not in the coverage and assign a unique id for each tss: X_rank
        txs.flanks&amp;lt;- constrainRanges(cvg, txs.flanks)
        txs.length&amp;lt;- length(txs.flanks)
        if(length(txs.flanks)!=txs.length){
              warning(paste0(txs.length-length(txs.flanks),
                             &amp;quot; Tss removed because they fall out of the coverage&amp;quot;))
            }
        # common chromosomes
        chrs&amp;lt;- sort(intersect(names(cvg), as.character(unique(seqnames(txs.flanks)))))
        
        # convert GRanges to IntergerRangesList does not maintain the order
        # a unique id was given for each Ranges
        myViews&amp;lt;- Views(cvg[chrs],as(txs.flanks,&amp;quot;IntegerRangesList&amp;quot;)[chrs]) # get subsets of RleList
        mat = lapply(myViews,function(x) t(viewApply(x,as.vector)) )
        mat = do.call(&amp;quot;rbind&amp;quot;,mat)
        
        r.list=split(mcols(txs.flanks)[,&amp;quot;X_rank&amp;quot;], as.vector(seqnames(txs.flanks))  )
        r.list=r.list[order(names(r.list))]
        ranks=do.call(&amp;quot;c&amp;quot;,r.list)
        rownames(mat) = ranks
        
        if(strand.aware == TRUE){
              orig.rows=txs.flanks[strand(txs.flanks) == &#39;-&#39;,]$X_rank
              mat[rownames(mat) %in% orig.rows,] = mat[rownames(mat) %in% 
                                                         orig.rows, ncol(mat):1]
        }
        
        # reorder according to the original Granges (txs)
        mat = mat[order(ranks),]
        
  
        ### normlization by the endFlank local noise
        profile &amp;lt;- colSums(mat)
        profile_norm &amp;lt;- profile/mean(profile[c(1:endFlank,(flank*2-endFlank+1):(flank*2))])

        #smooth
        profile_norm_smooth &amp;lt;- zoo::rollmean(profile_norm, smooth, fill = 1)
        

        #enrichment
        max_finite &amp;lt;- function(x){
        suppressWarnings(max(x[is.finite(x)], na.rm=TRUE))
        }
        
        e &amp;lt;- max_finite(profile_norm_smooth[(flank-highest_tss_flank):(flank+highest_tss_flank)])
        return(e)
}


&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>clustering scATACseq data: the TF-IDF way</title>
      <link>/post/clustering-scatacseq-data-the-tf-idf-way/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/clustering-scatacseq-data-the-tf-idf-way/</guid>
      <description>&lt;p&gt;scATACseq data are very sparse. It is sparser than scRNAseq. To do clustering of
scATACseq data, there are some preprocessing steps need to be done.&lt;/p&gt;
&lt;p&gt;I want to reproduce what has been done after reading the method section of these two recent scATACseq paper:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/30078704&#34;&gt;A Single-Cell Atlas of In Vivo Mammalian Chromatin Accessibility&lt;/a&gt;
Darren et.al Cell 2018&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Latent Semantic Indexing Cluster Analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to get an initial sense of the relationship between individual cells, &lt;strong&gt;we first broke the genome into 5kb windows and then scored each cell for any insertions in these windows, generating a large, sparse, binary matrix of 5kb windows by cells for each tissue.&lt;/strong&gt; Based on this matrix, we retained the top 20,000 most commonly used sites in each tissue (this number could extend a little above 20,000 because we included tied sites at the threshold) and then filtered out the bottom 5% of cells in terms of the number of 5kb windows with any insertions. We then reduced the dimensionality of these large binary matrices using a term &lt;strong&gt;frequency-inverse document frequency (‘‘TF-IDF’’) transformation.&lt;/strong&gt; To do this, we first weighted all the sites for individual cells by the total number of sites accessible in that cell (‘‘term frequency’’). We then multiplied these weighted values by log(1 + the inverse frequency of each site across all cells), the ‘‘inverse document frequency.’’ We then used singular value decomposition on the TF-IDF matrix to generate a lower dimensional representation of the data by only retaining the 2nd through 10th dimensions (because the first dimension tends to be highly correlated with read depth). These LSI-transformed scores of accessibility were then standardized by row (i.e., mean subtracted and divided by standard deviation), capped at ± 1.5, and used to bi-cluster cells and windows based on cosine distances using the ward algorithm in R. Visual examination of the resulting heatmaps identified between 2 and 7 distinct clusters of cells, de- pending on the tissue. These relatively crude groups of cells were used for peak calling (described below) to maintain enough cells in each group for identifying peaks while also retaining sufficient sensitivity to identify peaks that were restricted to subset of cells.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;t-distributed Stochastic Neighbor Embedding and Iterative Cluster Analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To take a more holistic approach to understanding the relationships of different cell types across the entire dataset, we combined all cells from all tissues and used the t-distributed stochastic neighbor embedding dimensionality reduction technique to visualize the full dataset and identify clusters of cells representing individual cell types. &lt;strong&gt;As with the LSI analysis above, we started by generating a large binary matrix of sites by cells, but instead of scoring cells for reads overlapping 5kb windows in the genome we scored cells for reads overlapping the master list of potential regulatory elements we had previously identified based on LSI clusters.&lt;/strong&gt; Starting with all cells that passed our nucleosome signal and read depth thresholds, we again wanted to remove the most sparsely sampled sites and cells to more clearly define differences between cell types. To do so, we first filtered out any sites that were not observed as accessible in at least 5% of cells in at least one LSI cluster and then filtered out cells that were more than 1 standard deviation below the mean number of sites observed. We then transformed this matrix with the TF-IDF algorithm described above. Finally, we generated a lower dimen- sional representation of the data by including the first 50 dimensions of the singular value decomposition of this TF-IDF-transformed matrix. This representation was then used as input for the Rtsne package in R (Krijthe, 2015). To identify clusters of cells in this two dimensional representation of the data, we used the Louvain clustering algorithm implemented in Seurat (Satija et al., 2015). Resolu- tion and K parameters for Louvain clustering were chosen for each major cluster to produce reasonable groupings of cells that are well- separated in each t-SNE embedding. This analysis identified 30 distinct clusters of cells, but to get at even finer structure, we subset TF-IDF normalized data on each of these 30 clusters of cells and repeated SVD and t-SNE to identify subclusters, again using Louvain clustering. Through this round of ‘‘iterative’’ t-SNE, we identified a total of 85 distinct clusters. Note that for one major cluster, major cluster 12, we found that Monocle 20s implementation of density peak clustering (Qiu et al., 2017; Trapnell et al., 2014) seemed to produce more reasonable clusters. Rho and delta parameters were set in the same manner as for Louvain clustering.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/610550v1&#34;&gt;Massively parallel single-cell chromatin landscapes of human immune cell development and intratumoral T cell exhaustion&lt;/a&gt;
Ansuman et.al 2019 biorxiv&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;ATAC-seq-centric Latent Semantic Indexing clustering and visualization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We clustered scATAC-seq data using an approach that does not require bulk data or prior
knowledge. To achieve this, we adopted the strategy by Cusanovich et. al9, to compute
the term frequency-inverse document frequency (“TF-IDF”) transformation. Briefly we
divided each index by the colSums of the matrix to compute the cell “term frequency.”
Next, we multiplied these values by log(1 + ncol(matrix) / rowSums(matrix)), which
represents the “inverse document frequency.” This resulted in a TF-IDF matrix that was
used as input to irlba’s singular value decomposition (SVD) implementation in R. We then
used the first 50 reduced dimensions as input into a Seurat object and then crude clusters
were identified by using Seurat’s (v2.3) SNN graph clustering “FindClusters” with a default
resolution of 0.8. We found that there was detectable batch effect that confounded further
analyses. To attenuate this batch effect, we calculated the cluster sums from the binarized
accessibility matrix and then log-normalized by using edgeR’s “cpm(matrix , log = TRUE,
prior.count = 3)” in R. Next, we identified the top 25,000 varying peaks across all clusters
using “rowVars” in R. This was done on the cluster log-normalized matrix vs the sparse
binary matrix because: (1) it reduced biases due to cluster cell sizes, and (2) it attenuated
the mean-variability relationship by converting to log space with a scaled prior count.
These 25,000 variable peaks were then used to subset the sparse binarized accessibility
matrix and recomputed the “TF-IDF” transform. We used singular value decomposition
on the TF-IDF matrix to generate a lower dimensional representation of the data by
retaining the first 50 dimensions. We then used these reduced dimensions as input into
a Seurat object and then crude clusters were identified by using Seurat’s (v2.3) SNN
graph clustering “FindClusters” with a default resolution of 0.8. These same reduced
dimensions were used as input to Seurat’s “RunUMAP” with default parameters and
plotted in ggplot2 using R&lt;/p&gt;
&lt;p&gt;Both papers used the so called &lt;code&gt;Latent Semantic Indexing&lt;/code&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Latent_semantic_analysis&#34;&gt;LSI method&lt;/a&gt; and used a transformation of the
binarized scATAC count matrix called ’TF-IDF` (term frequency–inverse document frequency) which is
used in text mining. TF-IDF can be used for scRNAseq data as well. see &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6101073/&#34;&gt;Single cell RNA-seq data clustering using TF-IDF based methods&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The transformation is not complicated as described above:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Briefly we divided each index by the colSums of the matrix to compute the cell “term frequency.”
Next, we multiplied these values by log(1 + ncol(matrix) / rowSums(matrix)), which
represents the “inverse document frequency.” This resulted in a TF-IDF matrix&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;Seurat&lt;/code&gt; version 3 has a function called &lt;a href=&#34;https://github.com/satijalab/seurat/blob/master/R/preprocessing.R#L1253&#34;&gt;&lt;code&gt;TF.IDF&lt;/code&gt;&lt;/a&gt; for that purpose.&lt;/p&gt;
&lt;p&gt;But note that, it does not do the log transformation in this function, but do it at &lt;a href=&#34;https://github.com/satijalab/seurat/blob/master/R/dimensional_reduction.R#L669&#34; class=&#34;uri&#34;&gt;https://github.com/satijalab/seurat/blob/master/R/dimensional_reduction.R#L669&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I will first show you the long way to do the clustering through which I want to gain some more
deep understanding of the whole process and I will show you how to use &lt;code&gt;Seurat&lt;/code&gt; V3 for that.&lt;/p&gt;
&lt;p&gt;I am going to use the 10k pbmc scATAC data from 10x for demonstration. You can download the data from
&lt;a href=&#34;https://support.10xgenomics.com/single-cell-atac/datasets/1.1.0/atac_v1_pbmc_10k&#34; class=&#34;uri&#34;&gt;https://support.10xgenomics.com/single-cell-atac/datasets/1.1.0/atac_v1_pbmc_10k&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;the-long-way&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The long way&lt;/h3&gt;
&lt;p&gt;read in the sparse matrix&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Matrix)
library(readr)
library(dplyr)
mat&amp;lt;- readMM(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/matrix.mtx&amp;quot;)
peaks&amp;lt;- read_tsv(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/peaks.bed&amp;quot;, col_names = F)
peaks&amp;lt;- peaks %&amp;gt;%
        mutate(id1 = paste(X2, X3, sep = &amp;quot;-&amp;quot;)) %&amp;gt;%
        mutate(id = paste(X1, id1, sep = &amp;quot;:&amp;quot;))
        
barcodes&amp;lt;- read_tsv(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/barcodes.tsv&amp;quot;, col_names =F)

rownames(mat)&amp;lt;- peaks$id1
colnames(mat)&amp;lt;- barcodes$X1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;binarize the data and do TF-IDF transformation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# binarize the matrix 
mat@x[mat@x &amp;gt;0]&amp;lt;- 1 

TF.IDF.custom &amp;lt;- function(data, verbose = TRUE) {
  if (class(x = data) == &amp;quot;data.frame&amp;quot;) {
    data &amp;lt;- as.matrix(x = data)
  }
  if (class(x = data) != &amp;quot;dgCMatrix&amp;quot;) {
    data &amp;lt;- as(object = data, Class = &amp;quot;dgCMatrix&amp;quot;)
  }
  if (verbose) {
    message(&amp;quot;Performing TF-IDF normalization&amp;quot;)
  }
  npeaks &amp;lt;- Matrix::colSums(x = data)
  tf &amp;lt;- t(x = t(x = data) / npeaks)
  # log transformation
  idf &amp;lt;- log(1+ ncol(x = data) / Matrix::rowSums(x = data))
  norm.data &amp;lt;- Diagonal(n = length(x = idf), x = idf) %*% tf
  norm.data[which(x = is.na(x = norm.data))] &amp;lt;- 0
  return(norm.data)
}


mat&amp;lt;- TF.IDF.custom(mat)

# what&amp;#39;s the range after transformation?
range(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.00000000 0.01111942&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 89796  8728&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dimension reduction with SVD, use &lt;code&gt;irlba::irlba&lt;/code&gt; for approximated calculation.&lt;/p&gt;
&lt;p&gt;Note: &lt;code&gt;svd&lt;/code&gt; singular value decomposition gives the same results as &lt;code&gt;prcomp&lt;/code&gt;for exact PC calculation.
see my previous &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/post/pca-in-action/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(irlba)
set.seed(123)
mat.lsi&amp;lt;- irlba(mat, 50)

d_diagtsne &amp;lt;- matrix(0, 50, 50)
diag(d_diagtsne) &amp;lt;- mat.lsi$d
mat_pcs &amp;lt;- t(d_diagtsne %*% t(mat.lsi$v))
dim(mat_pcs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8728   50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## PCA plot PC1 vs PC2
plot(mat_pcs[,1], mat_pcs[,2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rownames(mat_pcs)&amp;lt;- colnames(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;clustering in the PCA space using KNN.&lt;/p&gt;
&lt;p&gt;I took some code from &lt;a href=&#34;https://jef.works/blog/2017/09/13/graph-based-community-detection-for-clustering-analysis/&#34;&gt;Jean Fan’s blog post&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RANN)
knn.info&amp;lt;- RANN::nn2(mat_pcs, k = 30)

## convert to adjacancy matrix
knn &amp;lt;- knn.info$nn.idx

adj &amp;lt;- matrix(0, nrow(mat_pcs), nrow(mat_pcs))
rownames(adj) &amp;lt;- colnames(adj) &amp;lt;- rownames(mat_pcs)

for(i in seq_len(nrow(mat_pcs))) {
    adj[i,rownames(mat_pcs)[knn[i,]]] &amp;lt;- 1
}

## convert to graph
library(igraph)
g &amp;lt;- igraph::graph.adjacency(adj, mode=&amp;quot;undirected&amp;quot;)
g &amp;lt;- simplify(g) ## remove self loops

## identify communities, many algorithums. Use the Louvain clustering

km &amp;lt;- igraph::cluster_louvain(g)

com &amp;lt;- km$membership
names(com) &amp;lt;- km$names

# cluster id for each barcode
head(com)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AAACGAAAGAGCGAAA-1 AAACGAAAGAGTTTGA-1 AAACGAAAGCGAGCTA-1 
##                  7                 14                  2 
## AAACGAAAGGCTTCGC-1 AAACGAAAGTGCTGAG-1 AAACGAACAAGGGTAC-1 
##                 11                  1                 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## total 13 clusters
table(com)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## com
##    1    2    3    4    5    6    7    8    9   10   11   12   13   14 
## 1776  389  482   34 1100  520  491  640  781  487  204  888  173  763&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;t-SNE for visualization&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rtsne)
library(ggplot2)
library(tibble)
set.seed(345)

mat_tsne&amp;lt;- Rtsne(mat_pcs,  dims = 2, perplexity = 30, verbose = TRUE, 
               max_iter = 1000, check_duplicates = FALSE, is_distance = FALSE, 
               theta = 0.5, pca = FALSE, exaggeration_factor = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Read the 8728 x 50 data matrix successfully!
## OpenMP is working. 1 threads.
## Using no_dims = 2, perplexity = 30.000000, and theta = 0.500000
## Computing input similarities...
## Building tree...
## Done in 1.81 seconds (sparsity = 0.015574)!
## Learning embedding...
## Iteration 50: error is 94.027001 (50 iterations in 1.63 seconds)
## Iteration 100: error is 80.430931 (50 iterations in 1.34 seconds)
## Iteration 150: error is 77.384844 (50 iterations in 1.10 seconds)
## Iteration 200: error is 76.435871 (50 iterations in 1.12 seconds)
## Iteration 250: error is 75.985857 (50 iterations in 1.16 seconds)
## Iteration 300: error is 2.655848 (50 iterations in 1.02 seconds)
## Iteration 350: error is 2.321504 (50 iterations in 1.02 seconds)
## Iteration 400: error is 2.140627 (50 iterations in 1.05 seconds)
## Iteration 450: error is 2.024543 (50 iterations in 1.06 seconds)
## Iteration 500: error is 1.944114 (50 iterations in 1.06 seconds)
## Iteration 550: error is 1.884803 (50 iterations in 1.10 seconds)
## Iteration 600: error is 1.840703 (50 iterations in 1.14 seconds)
## Iteration 650: error is 1.806387 (50 iterations in 1.06 seconds)
## Iteration 700: error is 1.780991 (50 iterations in 1.07 seconds)
## Iteration 750: error is 1.761708 (50 iterations in 1.07 seconds)
## Iteration 800: error is 1.747014 (50 iterations in 1.10 seconds)
## Iteration 850: error is 1.735953 (50 iterations in 1.07 seconds)
## Iteration 900: error is 1.728716 (50 iterations in 1.11 seconds)
## Iteration 950: error is 1.725798 (50 iterations in 1.13 seconds)
## Iteration 1000: error is 1.724810 (50 iterations in 1.21 seconds)
## Fitting performed in 22.62 seconds.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_tsne&amp;lt;- as.data.frame(mat_tsne$Y)
colnames(df_tsne)&amp;lt;- c(&amp;quot;tSNE1&amp;quot;, &amp;quot;tSNE2&amp;quot;)
df_tsne$barcode&amp;lt;- rownames(mat_pcs)

df_tsne&amp;lt;- left_join(df_tsne, enframe(com), by = c(&amp;quot;barcode&amp;quot; = &amp;quot;name&amp;quot;)) %&amp;gt;%
        dplyr::rename(cluster = value) %&amp;gt;%
        mutate(cluster = as.factor(cluster))


ggplot(df_tsne, aes(x = tSNE1, y = tSNE2)) + 
        geom_point(aes(col = cluster), size = 0.5) +
        theme_bw(base_size = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks pretty good :)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-easier-way-use-seurat&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The easier way: use Seurat&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Seurat)
peaks &amp;lt;- Read10X_h5(filename = &amp;quot;/Users/mingtang/github_repos/blogdown_data/atac_v1_pbmc_10k_filtered_peak_bc_matrix.h5&amp;quot;)

# binarize the matrix
peaks@x[peaks@x &amp;gt;0]&amp;lt;- 1 

## create a seurat object
atac.lsi &amp;lt;- CreateSeuratObject(counts = peaks, assay = &amp;#39;ATAC&amp;#39;, project = &amp;#39;10k_pbmc&amp;#39;)

atac.lsi &amp;lt;- RunLSI(object = atac.lsi, n = 50, scale.max = NULL)

# atac.lsi@reductions

atac.lsi&amp;lt;- FindNeighbors(atac.lsi, reduction = &amp;quot;lsi&amp;quot;, dims = 1:50)
atac.lsi&amp;lt;- FindClusters(atac.lsi, resolution = 0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Modularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck
## 
## Number of nodes: 8728
## Number of edges: 246454
## 
## Running Louvain algorithm...
## Maximum modularity in 10 random starts: 0.9129
## Number of communities: 20
## Elapsed time: 0 seconds&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;atac.lsi &amp;lt;- RunTSNE(object = atac.lsi, reduction = &amp;quot;lsi&amp;quot;, dims = 1:50)
DimPlot(object = atac.lsi, reduction = &amp;#39;tsne&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You may argue those two t-SNE graphs look very different in terms of number of clusters
and the shape of the clusters. And I agree. There are many reasons for that.
I hope &lt;code&gt;Seurat&lt;/code&gt; team can give some insights.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The &lt;code&gt;TF-IDF&lt;/code&gt; function in &lt;code&gt;Seurat&lt;/code&gt; does not do log transformation
as in the papers: &lt;code&gt;idf &amp;lt;- log(1+ ncol(x = data) / Matrix::rowSums(x = data))&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;but rather do a log transformation &lt;a href=&#34;https://github.com/satijalab/seurat/blob/master/R/dimensional_reduction.R#L669&#34;&gt;later&lt;/a&gt;: &lt;code&gt;tf.idf &amp;lt;- LogNorm(data = tf.idf, display_progress = verbose, scale_factor = 1e4)&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;I am not an expert in the graph clustering, but the clustering algorithm in
&lt;code&gt;Seurat&lt;/code&gt; is probably not exactly the same with &lt;code&gt;igraph::cluster_louvain&lt;/code&gt;.
Moreover, one can always tweak the k.param and resolution parameters, and the cluster number changes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can compare the cell identities for each cluster&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# https://github.com/crazyhottommy/scclusteval
library(scclusteval)

# takes two named vector, and calculate the pairwise Jaccard similarity score
# for all clusters
PairWiseJaccardSetsHeatmap(com, Idents(atac.lsi))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-other-notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some other notes&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;It is known that first dimension is correlated with sequencing depth (although Ansuman et.al did not find such). Nevertheless, if you see such correlation, when cluster
cells in the PC space, you can exclude the first PC. e.g. &lt;code&gt;atac.lsi&amp;lt;- FindNeighbors(atac.lsi, reduction = &amp;quot;lsi&amp;quot;, dims = 2:50)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I did not do &lt;code&gt;LSI&lt;/code&gt; first for crude clustering using the titled 5kb genome bin matrix and call peaks for each crude cluster and then get the count matrix per peak per cell. I am not sure how much this extra work can benefit the clustering.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It turns out the &lt;code&gt;TF-IDF&lt;/code&gt; transformation is critical for this sparse matrix. If you do not do it, you will find your t-SNE plot looks really funky! do not trust me, try it yourself:)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;for clustering scATAC data, one can use the peak x cell matrix or derive a gene activity score by tools such as &lt;a href=&#34;https://cole-trapnell-lab.github.io/cicero-release/&#34;&gt;&lt;code&gt;Cicero&lt;/code&gt;&lt;/a&gt; to generate a gene x cell matrix. This is useful when you want to transfer the RNAseq cell type labels to the scATACseq data. see more details in the &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/460147v1&#34;&gt;Seurat V3 paper&lt;/a&gt;. The question is then, which matrix should we use for clustering? The clustering of these two different matrix can be different but there should be no surprise. We can use the gene activity score matrix as a label transferring mediator and get the cell labels and then super-impose the cluster id to the t-SNE plot clustered by the peak x cell matrix.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Acknowledgements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I want to thank 10x genomics for making the data publicly available.&lt;/li&gt;
&lt;li&gt;I want to thank &lt;a href=&#34;https://jef.works/blog/2017/09/13/graph-based-community-detection-for-clustering-analysis/&#34;&gt;Jean Fan&lt;/a&gt; for putting up some nice posts.&lt;/li&gt;
&lt;li&gt;I want to thank Tim Stuart for answering questions with &lt;code&gt;Seurat&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;I got some ideas from &lt;a href=&#34;https://github.com/jaychung10010/Mammary_snATAC-seq&#34; class=&#34;uri&#34;&gt;https://github.com/jaychung10010/Mammary_snATAC-seq&lt;/a&gt; as well. Thanks for posting the codes.&lt;/li&gt;
&lt;li&gt;I want to thank everyone else who give help and suggestions along my adventure of analyzing scATACseq data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;update&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;UPDATE&lt;/h3&gt;
&lt;p&gt;Do the IF-IDF Seurat way&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Matrix)
library(readr)
library(dplyr)
mat&amp;lt;- readMM(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/matrix.mtx&amp;quot;)
peaks&amp;lt;- read_tsv(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/peaks.bed&amp;quot;, col_names = F)
peaks&amp;lt;- peaks %&amp;gt;%
        mutate(id1 = paste(X2, X3, sep = &amp;quot;-&amp;quot;)) %&amp;gt;%
        mutate(id = paste(X1, id1, sep = &amp;quot;:&amp;quot;))
        
barcodes&amp;lt;- read_tsv(&amp;quot;/Users/mingtang/github_repos/blogdown_data/filtered_peak_bc_matrix/barcodes.tsv&amp;quot;, col_names =F)

rownames(mat)&amp;lt;- peaks$id1
colnames(mat)&amp;lt;- barcodes$X1
# binarize the matrix 
mat@x[mat@x &amp;gt;0]&amp;lt;- 1 
# Seurat version TF-IDF
mat&amp;lt;- TF.IDF(mat)
mat&amp;lt;- LogNormalize(mat,scale_factor = 1e4)

### SVD
library(irlba)
set.seed(123)
mat.lsi&amp;lt;- irlba(mat, 50)

d_diagtsne &amp;lt;- matrix(0, 50, 50)
diag(d_diagtsne) &amp;lt;- mat.lsi$d
mat_pcs &amp;lt;- t(d_diagtsne %*% t(mat.lsi$v))
dim(mat_pcs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8728   50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## PCA plot PC1 vs PC2
plot(mat_pcs[,1], mat_pcs[,2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rownames(mat_pcs)&amp;lt;- colnames(mat)

library(RANN)
knn.info&amp;lt;- RANN::nn2(mat_pcs, k = 30)

## convert to adjacancy matrix
knn &amp;lt;- knn.info$nn.idx

adj &amp;lt;- matrix(0, nrow(mat_pcs), nrow(mat_pcs))
rownames(adj) &amp;lt;- colnames(adj) &amp;lt;- rownames(mat_pcs)

for(i in seq_len(nrow(mat_pcs))) {
    adj[i,rownames(mat_pcs)[knn[i,]]] &amp;lt;- 1
}

## convert to graph
library(igraph)
g &amp;lt;- igraph::graph.adjacency(adj, mode=&amp;quot;undirected&amp;quot;)
g &amp;lt;- simplify(g) ## remove self loops

## identify communities, many algorithums. Use the Louvain clustering

km &amp;lt;- igraph::cluster_louvain(g)

com &amp;lt;- km$membership
names(com) &amp;lt;- km$names

# cluster id for each barcode
head(com)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## AAACGAAAGAGCGAAA-1 AAACGAAAGAGTTTGA-1 AAACGAAAGCGAGCTA-1 
##                 13                  7                 12 
## AAACGAAAGGCTTCGC-1 AAACGAAAGTGCTGAG-1 AAACGAACAAGGGTAC-1 
##                 14                 13                 16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## total 13 clusters
table(com)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## com
##    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 
##  801  376  617  629  572   56  607  435  280  390  131  417 2554  490  241 
##   16 
##  132&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### T-sne visualization
library(Rtsne)
library(ggplot2)
library(tibble)
set.seed(345)

mat_tsne&amp;lt;- Rtsne(mat_pcs,  dims = 2, perplexity = 30, verbose = TRUE, 
               max_iter = 1000, check_duplicates = FALSE, is_distance = FALSE, 
               theta = 0.5, pca = FALSE, exaggeration_factor = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Read the 8728 x 50 data matrix successfully!
## OpenMP is working. 1 threads.
## Using no_dims = 2, perplexity = 30.000000, and theta = 0.500000
## Computing input similarities...
## Building tree...
## Done in 2.57 seconds (sparsity = 0.014984)!
## Learning embedding...
## Iteration 50: error is 94.871477 (50 iterations in 1.48 seconds)
## Iteration 100: error is 84.409610 (50 iterations in 1.50 seconds)
## Iteration 150: error is 82.319098 (50 iterations in 1.18 seconds)
## Iteration 200: error is 81.831573 (50 iterations in 1.35 seconds)
## Iteration 250: error is 81.608255 (50 iterations in 1.40 seconds)
## Iteration 300: error is 3.039995 (50 iterations in 1.20 seconds)
## Iteration 350: error is 2.691975 (50 iterations in 1.14 seconds)
## Iteration 400: error is 2.508723 (50 iterations in 1.24 seconds)
## Iteration 450: error is 2.390684 (50 iterations in 1.14 seconds)
## Iteration 500: error is 2.308249 (50 iterations in 1.16 seconds)
## Iteration 550: error is 2.248218 (50 iterations in 1.12 seconds)
## Iteration 600: error is 2.201765 (50 iterations in 1.27 seconds)
## Iteration 650: error is 2.166028 (50 iterations in 1.21 seconds)
## Iteration 700: error is 2.137659 (50 iterations in 1.13 seconds)
## Iteration 750: error is 2.115987 (50 iterations in 1.11 seconds)
## Iteration 800: error is 2.098913 (50 iterations in 1.16 seconds)
## Iteration 850: error is 2.086752 (50 iterations in 1.08 seconds)
## Iteration 900: error is 2.079435 (50 iterations in 1.07 seconds)
## Iteration 950: error is 2.078012 (50 iterations in 1.14 seconds)
## Iteration 1000: error is 2.076638 (50 iterations in 1.12 seconds)
## Fitting performed in 24.21 seconds.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_tsne&amp;lt;- as.data.frame(mat_tsne$Y)
colnames(df_tsne)&amp;lt;- c(&amp;quot;tSNE1&amp;quot;, &amp;quot;tSNE2&amp;quot;)
df_tsne$barcode&amp;lt;- rownames(mat_pcs)

df_tsne&amp;lt;- left_join(df_tsne, enframe(com), by = c(&amp;quot;barcode&amp;quot; = &amp;quot;name&amp;quot;)) %&amp;gt;%
        dplyr::rename(cluster = value) %&amp;gt;%
        mutate(cluster = as.factor(cluster))

ggplot(df_tsne, aes(x = tSNE1, y = tSNE2)) + 
        geom_point(aes(col = cluster), size = 0.5) +
        theme_bw(base_size = 14)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-05-03-clustering-scatacseq-data-the-tf-idf-way_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, it still looks different from the &lt;code&gt;Seurat&lt;/code&gt; output. Any comments?&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>plot 10x scATAC coverage by cluster/group</title>
      <link>/post/plot-10x-scatac-coverage-by-cluster-group/</link>
      <pubDate>Mon, 29 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/plot-10x-scatac-coverage-by-cluster-group/</guid>
      <description>&lt;p&gt;This post was inspired by &lt;a href=&#34;https://twitter.com/ahill_tweets&#34;&gt;Andrew Hill&lt;/a&gt;’s &lt;a href=&#34;http://andrewjohnhill.com/blog/2019/04/12/streamlining-scatac-seq-visualization-and-analysis/&#34;&gt;recent blog post&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Inspired by some nice posts by &lt;a href=&#34;https://twitter.com/timoast?ref_src=twsrc%5Etfw&#34;&gt;@timoast&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/tangming2005?ref_src=twsrc%5Etfw&#34;&gt;@tangming2005&lt;/a&gt; and work from &lt;a href=&#34;https://twitter.com/10xGenomics?ref_src=twsrc%5Etfw&#34;&gt;@10xGenomics&lt;/a&gt;. Would still definitely have to split BAM files for other tasks, so easy to use tools for that are super useful too!&lt;/p&gt;&amp;mdash; Andrew J Hill (@ahill_tweets) &lt;a href=&#34;https://twitter.com/ahill_tweets/status/1116875339303493634?ref_src=twsrc%5Etfw&#34;&gt;April 13, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Andrew wrote that blog post in light of my other &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/post/split-a-10xscatac-bam-file-by-cluster/&#34;&gt;recent blog post&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/timoast&#34;&gt;Tim&lt;/a&gt;’s (developer of the almighty &lt;a href=&#34;https://satijalab.org/seurat/&#34;&gt;Seurat&lt;/a&gt; package) &lt;a href=&#34;https://timoast.github.io/blog/sinto/&#34;&gt;blog post&lt;/a&gt;. Writing blog post is fun and I am happy to see so many new ideas can be generated through online communications.&lt;/p&gt;
&lt;p&gt;I took Andrew’s idea of reading in the reads in a certain window by taking advantages of tabix indexed file fragment.tsv.gz which is an output from 10x &lt;code&gt;cellranger-atac&lt;/code&gt;. I then split the reads by a grouping file which specifies which group each cell belongs to and a total number of reads in each cell. For visualization, instead of using ggplot2, I used the awesome &lt;a href=&#34;https://bernatgel.github.io/karyoploter_tutorial/&#34;&gt;karyoploteR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I experimented a bit and came up with a function below. Note the script is fast as only the reads fall in the specified region are read into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(tidyr)
library(dplyr)
library(tibble)
library(Rsamtools)
library(karyoploteR)
library(org.Hs.eg.db)
library(org.Mm.eg.db)
library(TxDb.Hsapiens.UCSC.hg19.knownGene)
library(TxDb.Mmusculus.UCSC.mm10.knownGene)


extend &amp;lt;- function(x, upstream=0, downstream=0)     
{
        if (any(strand(x) == &amp;quot;*&amp;quot;))
                warning(&amp;quot;&amp;#39;*&amp;#39; ranges were treated as &amp;#39;+&amp;#39;&amp;quot;)
        on_plus &amp;lt;- strand(x) == &amp;quot;+&amp;quot; | strand(x) == &amp;quot;*&amp;quot;
        new_start &amp;lt;- start(x) - ifelse(on_plus, upstream, downstream)
        new_end &amp;lt;- end(x) + ifelse(on_plus, downstream, upstream)
        ranges(x) &amp;lt;- IRanges(new_start, new_end)
        trim(x)
}


addGeneNameToTxdb&amp;lt;- function(txdb = TxDb.Hsapiens.UCSC.hg19.knownGene, 
                             eg.db = org.Hs.eg.db){
        gene&amp;lt;- genes(txdb)
        ## 1: 1 mapping
        ss&amp;lt;- AnnotationDbi::select(eg.db, keys = gene$gene_id,  
                              keytype=&amp;quot;ENTREZID&amp;quot;, columns = &amp;quot;SYMBOL&amp;quot; )
        gene$symbol&amp;lt;- ss[, 2]
        return(gene)
}


plotCoverageByGroup&amp;lt;- function(chrom = NULL, start = NULL, end = NULL, gene_name = NULL, upstream = 2000,
                               downstream = 2000, fragment, grouping,
                               genome =&amp;#39;hg19&amp;#39;, txdb = TxDb.Hsapiens.UCSC.hg19.knownGene,
                               eg.db = org.Hs.eg.db,
                               ymax = NULL, label_cex = 1, 
                               yaxis_cex = 1, track_col = &amp;quot;cadetblue2&amp;quot;,
                               tick.dist = 10000, minor.tick.dist = 2000,
                               tick_label_cex = 1){
        grouping&amp;lt;- readr::read_tsv(grouping)
        if(! all(c(&amp;quot;cell&amp;quot;, &amp;quot;cluster&amp;quot;, &amp;quot;depth&amp;quot;) %in% colnames(grouping))) {
                stop(&amp;#39;Grouping dataframe must have cell, cluster, and depth columns.&amp;#39;)
        }
        ## get number of reads per group for normalization. 
        ## not furthur normalize by the cell number in each group.
        grouping&amp;lt;-  grouping %&amp;gt;%
                group_by(cluster) %&amp;gt;%
                dplyr::mutate(cells_in_group = n(), total_depth_in_group = sum(depth)) %&amp;gt;%
                # reads per million (RPM)
                dplyr::mutate(scaling_factor = 1e6/(total_depth_in_group)) %&amp;gt;%
                ungroup() %&amp;gt;%
                dplyr::select(cell, cluster, scaling_factor)
        
        
        if (is.null(chrom) &amp;amp; is.null(start) &amp;amp; is.null(end) &amp;amp; !is.null(gene_name)){
                gene &amp;lt;- genes(txdb)
                gene &amp;lt;- addGeneNameToTxdb(txdb = txdb, eg.db = eg.db)
                gr&amp;lt;- gene[which(gene$symbol == gene_name)]
                if (length(gr) == 0){
                        stop(&amp;quot;gene name is not found in the database&amp;quot;)
                } else if (length(gr) &amp;gt; 1) {
                        gr&amp;lt;- gr[1]
                        warning(&amp;quot;multiple GRanges found for the gene, using the first one&amp;quot;)
                } else {
                        gr&amp;lt;- extend(gr, upstream = upstream, downstream = downstream)
                } 
                
        } else if (!is.null(chrom) &amp;amp; !is.null(start) &amp;amp; !is.null(end)){
                gr&amp;lt;- GRanges(seq = chrom, IRanges(start = start, end = end ))
        }
        
        
        ## read in the fragment.tsv.gz file
        ## with &amp;quot;chr&amp;quot;, &amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;, &amp;quot;cell&amp;quot;, &amp;quot;duplicate&amp;quot; columns. output from cellranger-atac
        # this returns a list
        reads&amp;lt;- scanTabix(fragment, param = gr)
        
        reads&amp;lt;- reads[[1]] %&amp;gt;% 
                tibble::enframe() %&amp;gt;% 
                dplyr::select(-name) %&amp;gt;%
                tidyr::separate(value, into = c(&amp;quot;chr&amp;quot;, &amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;, &amp;quot;cell&amp;quot;, &amp;quot;duplicate&amp;quot;), sep = &amp;quot;\t&amp;quot;) %&amp;gt;%
                dplyr::mutate_at(.vars = c(&amp;quot;start&amp;quot;, &amp;quot;end&amp;quot;), as.numeric) %&amp;gt;% 
                # make it 1 based for R, the fragment.tsv is 0 based
                dplyr::mutate(start = start + 1) %&amp;gt;% 
                inner_join(grouping) %&amp;gt;%
                makeGRangesFromDataFrame(keep.extra.columns = TRUE)
        # GRangesList object by group/cluster
        reads_by_group&amp;lt;- split(reads, reads$cluster)
        
        ## plotting
        pp &amp;lt;- getDefaultPlotParams(plot.type=1)
        pp$leftmargin &amp;lt;- 0.15
        pp$topmargin &amp;lt;- 15
        pp$bottommargin &amp;lt;- 15
        pp$ideogramheight &amp;lt;- 5
        pp$data1inmargin &amp;lt;- 10
        kp &amp;lt;- plotKaryotype(genome = genome, zoom = gr, plot.params = pp)
        kp&amp;lt;- kpAddBaseNumbers(kp, tick.dist = tick.dist, minor.tick.dist = minor.tick.dist,
                              add.units = TRUE, cex= tick_label_cex, digits = 6)
        ## calculate the normalized coverage
        normalized_coverage&amp;lt;- function(x){
                if (!is(x, &amp;quot;GRangesList&amp;quot;))
                        stop(&amp;quot;&amp;#39;x&amp;#39; must be a GRangesList object&amp;quot;)
                # specify the width to the whole chromosome to incldue the 0s
                cvgs&amp;lt;- lapply(x, function(x) coverage(x, width = kp$chromosome.lengths) * x$scaling_factor[1])
                return(cvgs)
        }
        
        coverage_norm&amp;lt;- normalized_coverage(reads_by_group)
        
        ## calculate the max coverage if not specified 
        if (is.null(ymax)) {
                yaxis_common&amp;lt;- ceiling(max(lapply(coverage_norm, max) %&amp;gt;% unlist()))
        } else {
                yaxis_common&amp;lt;- ymax
        }
        ## add gene information
        genes.data &amp;lt;- makeGenesDataFromTxDb(txdb,
                                            karyoplot=kp,
                                            plot.transcripts = TRUE, 
                                            plot.transcripts.structure = TRUE)
        genes.data &amp;lt;- addGeneNames(genes.data)
        genes.data &amp;lt;- mergeTranscripts(genes.data)
        
        kp&amp;lt;- kpPlotGenes(kp, data=genes.data, r0=0, r1=0.05, gene.name.cex = 1)
        
        for(i in seq_len(length(coverage_norm))) {
                read &amp;lt;- coverage_norm[[i]]
                at &amp;lt;- autotrack(i, length(coverage_norm), r0=0.1, r1=1, margin = 0.1)
                kp &amp;lt;- kpPlotCoverage(kp, data=read,
                                     r0=at$r0, r1=at$r1, col = track_col, ymax = yaxis_common)
                kpAxis(kp, ymin=0, ymax=yaxis_common, numticks = 2, r0=at$r0, r1=at$r1, cex = yaxis_cex, labels = c(&amp;quot;&amp;quot;, yaxis_common))
                kpAddLabels(kp, labels = names(coverage_norm)[i], r0=at$r0, r1=at$r1, 
                            cex=label_cex, label.margin = 0.005)
        }
        
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;usage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Usage&lt;/h3&gt;
&lt;p&gt;The example files can be downloaded from &lt;a href=&#34;https://osf.io/q5dwj/&#34; class=&#34;uri&#34;&gt;https://osf.io/q5dwj/&lt;/a&gt;.
&lt;code&gt;atac_v1_pbmc_10k_fragments.tsv.gz&lt;/code&gt; is the 10k pbmc atac data downloaded from 10x website. Thanks for making the
data public available. Make sure put the tabix index &lt;code&gt;atac_v1_pbmc_10k_fragments.tsv.gz.tbi&lt;/code&gt; in the same folder.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;grouping.txt&lt;/code&gt; is a 3 column tsv file containing header: cell, cluster, and depth. The cluster label was transferred from the 10x pbmc scRNAseq data set using &lt;code&gt;Seurat&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;LYZ gene is a marker for CD16+ cells.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## LYZ gene 
chrom&amp;lt;-  &amp;quot;chr12&amp;quot;
start&amp;lt;-  69730394
end&amp;lt;- 69760971

plotCoverageByGroup(chrom = chrom, start = start, end = end, fragment = &amp;quot;atac_v1_pbmc_10k_fragments.tsv.gz&amp;quot;,
                    grouping = &amp;quot;grouping.txt&amp;quot;, track_col = &amp;quot;red&amp;quot;, label_cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-29-plot-10x-scatac-coverage-by-cluster-group_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;NKG7 is a marker for NK cells.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plotCoverageByGroup(gene_name = &amp;quot;NKG7&amp;quot;, fragment = &amp;quot;atac_v1_pbmc_10k_fragments.tsv.gz&amp;quot;,
                    grouping = &amp;quot;grouping.txt&amp;quot;, tick_label_cex = 1, tick.dist = 5000,
                    minor.tick.dist = 1000, label_cex = 0.75)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-29-plot-10x-scatac-coverage-by-cluster-group_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;what I did for some extra work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I normalized each track by total number of reads in that group in reads per million. I did not do
any further normalization on the cell number of each group as Andrew did. I am open to discussion on how to
best normalize the tracks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I calculated the max value of all the tracks and set a common y-axis for all the tracks. Users can set a customized &lt;code&gt;ymax&lt;/code&gt; as well.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I added some functionality of specifying only a gene name, and one can extend that gene ranges by padding upstream (from transcription start site) and downstream (from transcription end site) bps.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One can either plot Human or Mouse data. Other organisms can be easily supported by modifying the script.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;what-to-do-next&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;what to do next&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;customer specified cluster (a subset of clusters in a certain order) to plot.
When one has a lot of clusters (e.g. over 50), one probably does not want to plot all of them.&lt;/li&gt;
&lt;li&gt;specify color for each cluster track.&lt;/li&gt;
&lt;li&gt;bam support by using&lt;code&gt;Rsamtools::ScanBam&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;smooth window? The plots showed above were not smoothed and they look good to me. Not sure needed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Any suggestions/discussions are welcomed!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reproducible research in bioinformatics</title>
      <link>/talk/2019-bunkerhill-talk/</link>
      <pubDate>Thu, 28 Mar 2019 14:30:00 -0400</pubDate>
      
      <guid>/talk/2019-bunkerhill-talk/</guid>
      <description>&lt;p&gt;I was invited to give a talk on reproducible bioinformatics research to the students in the &lt;a href=&#34;https://www.bhcc.edu/&#34; target=&#34;_blank&#34;&gt;Bunker Hill Community College&lt;/a&gt; in Boston, MA. I was so glad to introduce bioinformatics to the students and share my own perspectives on reproducible research.&lt;/p&gt;

&lt;p&gt;The movie &lt;a href=&#34;https://en.wikipedia.org/wiki/Good_Will_Hunting&#34; target=&#34;_blank&#34;&gt;Good Will Hunting&lt;/a&gt; was shot there :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/bunkerhill-talk.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KRAS-IRF2 Axis Drives Immune Suppression and Immune Therapy Resistance in Colorectal Cancer</title>
      <link>/publication/2019-03-20-irf2/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 -0400</pubDate>
      
      <guid>/publication/2019-03-20-irf2/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Use docopt to write command line R utilities </title>
      <link>/post/use-docopt-to-write-command-line-r-utilities/</link>
      <pubDate>Fri, 22 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/use-docopt-to-write-command-line-r-utilities/</guid>
      <description>&lt;p&gt;I was writing an R script to plot the ATACseq fragment length distribution and wanted
to turn the R script to a command line utility.&lt;/p&gt;

&lt;p&gt;I then (re)discovered this awesome &lt;a href=&#34;https://github.com/docopt/docopt.R&#34; target=&#34;_blank&#34;&gt;docopt.R&lt;/a&gt;.
One just needs to write the help message the you want to display and &lt;code&gt;docopt()&lt;/code&gt; will
parse the options, arguments and return a named list which can be accessed inside the
R script. check &lt;a href=&#34;http://docopt.org/&#34; target=&#34;_blank&#34;&gt;http://docopt.org/&lt;/a&gt; for more information as well.&lt;/p&gt;

&lt;p&gt;See below for an example. You can download it at &lt;a href=&#34;https://github.com/crazyhottommy/scATACutils/blob/master/R/plot_atac_frag_distribution.R&#34; target=&#34;_blank&#34;&gt;https://github.com/crazyhottommy/scATACutils/blob/master/R/plot_atac_frag_distribution.R&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
#! /usr/bin/env Rscript
&#39;Plot fragment length distribution from ATACseq data
Usage:
    plot_atac_fragment_len_distribution.R (--poly | --hist) (--pdf | --png) [--width=&amp;lt;width&amp;gt; --height=&amp;lt;height&amp;gt; --bin=&amp;lt;bp&amp;gt;] &amp;lt;input&amp;gt; &amp;lt;output&amp;gt;
    
Options:
    -h --help  Show this screen.
    -v --version  Show version.
    --bin=&amp;lt;bp&amp;gt;  Bin size [default: 5]
    --poly  Plot frequency polygon.
    --hist  Plot histogram.
    --pdf  Save to pdf.
    --png  Save to png.
    --width=&amp;lt;width&amp;gt;  Width of the output [default: 4]
    --height=&amp;lt;height&amp;gt; Height of the output [default: 4]

Arguments:
    input  fragment length in a one column dataframe without header or stdin
    output  output filename
&#39; -&amp;gt; doc

suppressMessages(library(ggplot2))
# check this awesome docoptR https://github.com/docopt/docopt.R
## make sure use the development version, the CRAN version not working for me
# library(devtools) 
# devtools::install_github(&amp;quot;docopt/docopt.R&amp;quot;)
suppressMessages(library(docopt))
suppressMessages(library(dplyr))

# this will give error if try interactively, because no input and output argument are given
# https://github.com/docopt/docopt.R/issues/27
arguments &amp;lt;- docopt(doc, version = &#39;plot_atac_frag_distribution v1.0\n\n&#39;)

# for testing interactively
#arguments &amp;lt;- docopt(doc, version = &#39;FragmentSizeDistribution v1.0&#39;, args = c(&amp;quot;scripts/fragment3.txt&amp;quot;,&amp;quot;my.pdf&amp;quot;))
#print(arguments)

## File Read ##
# taken from https://stackoverflow.com/questions/26152998/how-to-make-r-script-takes-input-from-pipe-and-user-given-parameter
# if the input is stdin one can do 
# cat fragment.txt | ./plot_atac_frag_distribution.R --poly --pdf stdin  out.pdf
# cat fragment.txt | ./plot_atac_frag_distribution.R --poly --pdf - out.pdf
# ./plot_atac_frag_distribution.R --poly --pdf &amp;lt;(cat fragment.txt)  out.pdf


OpenRead &amp;lt;- function(arg) {
    if (arg %in% c(&amp;quot;-&amp;quot;, &amp;quot;/dev/stdin&amp;quot;)) {
        file(&amp;quot;stdin&amp;quot;, open = &amp;quot;r&amp;quot;)
    } else if (grepl(&amp;quot;^/dev/fd/&amp;quot;, arg)) {
        fifo(arg, open = &amp;quot;r&amp;quot;)
    } else {
        file(arg, open = &amp;quot;r&amp;quot;)
    }
}

dat.con &amp;lt;- OpenRead(arguments$input)
fragment &amp;lt;- read.table(dat.con, header = FALSE)

#fragment&amp;lt;- read.table(arguments$input, header = F)

names(fragment)&amp;lt;- c(&amp;quot;length&amp;quot;)

plot_hist&amp;lt;- function(fragment, bin) {
        g&amp;lt;- ggplot(fragment %&amp;gt;% filter(length &amp;lt;=2000), aes(x = length)) + 
                geom_histogram(binwidth = bin, aes(y=..density..), fill = &amp;quot;red&amp;quot;) +
                geom_density(alpha=.2, fill=&amp;quot;#FF6666&amp;quot;, col = &amp;quot;black&amp;quot;) +
                coord_cartesian(xlim = c(0,1000)) +
                scale_x_continuous(breaks = c(0, 100, 200, 300, 400, 800)) +
                theme_minimal(base_size = 14)
        return(g)
        
}

plot_polygon&amp;lt;- function(fragment, bin){
        g&amp;lt;- ggplot(fragment %&amp;gt;% filter(length &amp;lt;=2000), aes(x = length, stat(density))) + 
                geom_freqpoly(binwidth = bin, col = &amp;quot;blue&amp;quot;) +
                coord_cartesian(xlim = c(0,1000)) +
                scale_x_continuous(breaks = c(0, 100, 200, 300, 400, 800)) +
                theme_minimal(base_size = 14)
        return(g)
}


main&amp;lt;- function(fragment, arguments){
    if (arguments$poly){
        g&amp;lt;- plot_polygon(fragment, as.numeric(arguments$bin))
    } else if (arguments$hist){
        g&amp;lt;- plot_hist(fragment, as.numeric(arguments$bin))
    }
    device&amp;lt;- ifelse(arguments$pdf, &amp;quot;pdf&amp;quot;, &amp;quot;png&amp;quot;)
    
    ggsave(arguments$output, plot = g,  device = device, width =as.numeric(arguments$width), 
           height = as.numeric(arguments$height) )
    
}

main(fragment, arguments)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;save it to &lt;code&gt;plot_atac_frag_distribution.R&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;on command line, one can do:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;
./plot_atac_frag_distribution.R -h
Plot fragment length distribution from ATACseq data
Usage:
    plot_atac_fragment_len_distribution.R (--poly | --hist) (--pdf | --png) [--width=&amp;lt;width&amp;gt; --height=&amp;lt;height&amp;gt; --bin=&amp;lt;bp&amp;gt;] &amp;lt;input&amp;gt; &amp;lt;output&amp;gt;

Options:
    -h --help  Show this screen.
    -v --version  Show version.
    --bin=&amp;lt;bp&amp;gt;  Bin size [default: 5]
    --poly  Plot frequency polygon.
    --hist  Plot histogram.
    --pdf  Save to pdf.
    --png  Save to png.
    --width=&amp;lt;width&amp;gt;  Width of the output [default: 4]
    --height=&amp;lt;height&amp;gt; Height of the output [default: 4]

Arguments:
    input  fragment length in a one column dataframe without header or stdin
    output  output filename

./plot_atac_frag_distribution.R --poly --png  --bin 10 fragment.txt out.png
cat fragment.txt | ./plot_atac_frag_distribution.R --poly --pdf stdin  out.pdf
cat fragment.txt | ./plot_atac_frag_distribution.R --hist --pdf - out.pdf
./plot_atac_frag_distribution.R --hist --pdf &amp;lt;(cat fragment.txt)  out.pdf

samtools view my.bam | awk &#39;$9&amp;gt;0&#39; | cut -f 9 |./plot_atac_frag_distribution.R --poly --pdf - out.pdf

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;histogram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/out2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;polygon:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/out3.png&#34; alt=&#34;&#34; /&gt;
Pretty cool!!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important notes:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[default: 4]  The space after &lt;code&gt;:&lt;/code&gt; is needed.&lt;/li&gt;
&lt;li&gt;use two spaces to separate the option and the explanation&lt;/li&gt;
&lt;li&gt;use four spaces to indent&lt;/li&gt;
&lt;li&gt;use the development version of optdoc.R&lt;/li&gt;
&lt;li&gt;when testing interactively. docopt() may give error when the mandatory arguments
are not specified, but running on command line is fine.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;see &lt;a href=&#34;https://github.com/docopt/docopt.R/issues/24&#34; target=&#34;_blank&#34;&gt;https://github.com/docopt/docopt.R/issues/24&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also check &lt;a href=&#34;http://dirk.eddelbuettel.com/code/littler.examples.html&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;littler&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;littler provides the r program, a simplified command-line interface for GNU R. This allows direct execution of commands, use in piping where the output of one program supplies the input of the next, as well as adding the ability for writing hash-bang scripts, i.e. creating executable files starting with, say, #!/usr/bin/r.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Split a 10xscATAC bam file by cluster</title>
      <link>/post/split-a-10xscatac-bam-file-by-cluster/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/split-a-10xscatac-bam-file-by-cluster/</guid>
      <description>

&lt;p&gt;I want to split the PBMC scATAC bam from 10x by cluster id. So, I can then make a bigwig for each cluster to visualize in &lt;code&gt;IGV&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The first thing I did was googling to see if anyone has written such a tool (Do not reinvent the wheels!). People have done that because I saw figures from the scATAC papers. I just could not find it. Maybe I need to refine my googling skills.&lt;/p&gt;

&lt;p&gt;I decided to write one myself. The following is my journey for this small task.&lt;/p&gt;

&lt;p&gt;download the 5k pbmc scATAC data from &lt;a href=&#34;https://support.10xgenomics.com/single-cell-atac/datasets/1.0.1/atac_v1_pbmc_5k&#34; target=&#34;_blank&#34;&gt;https://support.10xgenomics.com/single-cell-atac/datasets/1.0.1/atac_v1_pbmc_5k&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;split-the-cell-barcodes-by-cluster-id&#34;&gt;split the cell barcodes by cluster id&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd analysis/clustering/graphclust
head clusters.csv
Barcode,Cluster
AAACGAAAGCGCAATG-1,1
AAACGAAAGGGTATCG-1,4
AAACGAAAGTAACATG-1,8
AAACGAAAGTTACACC-1,1
AAACGAACAGAGATGC-1,4
AAACGAACATGCTATG-1,5
AAACGAAGTGCATCAT-1,3
AAACGAAGTGGACGAT-1,3
AAACGAAGTGGCCTCA-1,7

# there are ^M characters at the end of the line if you do cat -A you will see it.
# change it to unix
dos2unix clusters.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;awk -F&amp;quot;,&amp;quot; &#39;NR&amp;gt;1{print $1 &amp;gt;&amp;gt; &amp;quot;cluster_&amp;quot;$2&amp;quot;.csv&amp;quot;}&#39; clusters.csv
wc -l *csv
   330 cluster_10.csv
   322 cluster_11.csv
   258 cluster_12.csv
   191 cluster_13.csv
   608 cluster_1.csv
   563 cluster_2.csv
   559 cluster_3.csv
   532 cluster_4.csv
   483 cluster_5.csv
   425 cluster_6.csv
   366 cluster_7.csv
   360 cluster_8.csv
   338 cluster_9.csv
  5336 clusters.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;use-bamtools&#34;&gt;use bamtools&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;time bamtools filter -tag CB:Z:AAACTGCAGAGCAGCT-1 -in atac_v1_pbmc_5k_possorted_bam.bam -out AAACTGCAGAGCAGCT-1.bam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This takes a little over 1 hour for one barcode! And there is no easy way
to specify a group of barcodes.&lt;/p&gt;

&lt;h3 id=&#34;use-the-linux-tricks&#34;&gt;use the linux tricks&lt;/h3&gt;

&lt;p&gt;inspired partly by this post &lt;a href=&#34;https://www.biostars.org/p/263346/&#34; target=&#34;_blank&#34;&gt;https://www.biostars.org/p/263346/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wc -l clusters.csv
5336 clusters.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and let&amp;rsquo;s see how fast each regular expression takes for &lt;code&gt;awk&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;time samtools view atac_v1_pbmc_5k_possorted_bam.bam | awk -v tag=&amp;quot;CB:Z:AAACTGCAGAGCAGCT-1&amp;quot; &#39;index($0,tag)&amp;gt;0&#39; &amp;gt;&amp;gt; AAACTGCAGAGCAGCT-1.sam

real    27m14.332s
user    48m36.883s
sys     4m37.908s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is not too bad, but if we loops over the &lt;code&gt;clusters.csv&lt;/code&gt; files for 5335 times,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;samtools view -H atac_v1_pbmc_5k_possorted_bam.bam &amp;gt; header.txt

cat clusters.csv \
| sed &#39;1d&#39; \
| while IFS=&#39;,&#39; read -r barcode cluster
    do samtools view atac_v1_pbmc_5k_possorted_bam.bam |  awk -v tag=&amp;quot;CB:Z:$barcode&amp;quot; &#39;index($0,tag)&amp;gt;0&#39; &amp;gt;&amp;gt; &amp;quot;$cluster.sam&amp;quot;
    done

## then cat the header with the sam.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;it will take ~30min * 5335 = ~100 days to finish.&lt;/p&gt;

&lt;p&gt;we can do better to parallize by GNU parallel&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## not tested...
cat clusters.csv \
| sed &#39;1d&#39; \
| parallel --colsep &#39;,&#39; -j 40 &#39;samtools view atac_v1_pbmc_5k_possorted_bam.bam |awk -v tag=&amp;quot;CB:Z:{1}&amp;quot; &#39;index(\$0,tag)&amp;gt;0&#39; &amp;gt;&amp;gt; {2}.sam&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, using 40 cores may reduce our time to &lt;sup&gt;100&lt;/sup&gt;&amp;frasl;&lt;sub&gt;40&lt;/sub&gt; = 5 days.&lt;/p&gt;

&lt;h3 id=&#34;use-pysam&#34;&gt;use pysam&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s only loop over the sam file once&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pysam
import csv

cluster_dict = {}
with open(&#39;clusters.csv&#39;) as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=&#39;,&#39;)
    #skip header
    header = next(csv_reader)
    for row in csv_reader:
        cluster_dict[row[0]] = row[1]

clusters = set(x for x in cluster_dict.values())


fin = pysam.AlignmentFile(&amp;quot;atac_v1_pbmc_5k_possorted_bam.bam&amp;quot;, &amp;quot;rb&amp;quot;)

# open the number of bam files as the same number of clusters, and map the out file handler to the cluster id, write to a bam with wb
fouts_dict = {}
for cluster in clusters:
    fout = pysam.AlignmentFile(&amp;quot;cluster&amp;quot; + cluster + &amp;quot;.bam&amp;quot;, &amp;quot;wb&amp;quot;, template = fin)
    fouts_dict[cluster] = fout

for read in fin:
    tags = read.tags
    CB_list = [ x for x in tags if x[0] == &amp;quot;CB&amp;quot;]
    if CB_list:
        cell_barcode = CB_list[0][1]
    # the bam files may contain reads not in the final clustered barcodes
    # will be None if the barcode is not in the clusters.csv file
    else: 
        continue
    cluster_id = cluster_dict.get(cell_barcode)
    if cluster_id:
        fouts_dict[cluster_id].write(read)

## do not forget to close the files
fin.close()
for fout in fouts_dict.values():
    fout.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;real    172m58.758s
user    172m10.678s
sys     0m46.071s&lt;/p&gt;

&lt;p&gt;Note, some read record in the bam file do not have &lt;code&gt;CB&lt;/code&gt; but only &lt;code&gt;CR&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;from &lt;a href=&#34;https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/output/bam&#34; target=&#34;_blank&#34;&gt;https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/output/bam&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CB&lt;/td&gt;
&lt;td&gt;Chromium cellular barcode sequence that is error-corrected and confirmed against a list of known-good barcode sequences.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;CR&lt;/td&gt;
&lt;td&gt;Chromium cellular barcode sequence as reported by the sequencer.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;How many of those reads with &lt;code&gt;CR&lt;/code&gt;?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# every read has a CR tag
samtools view atac_v1_pbmc_5k_possorted_bam.bam| grep -v &amp;quot;CR&amp;quot; | wc -l
0 

# not every read has a CB tag.
samtools view atac_v1_pbmc_5k_possorted_bam.bam| grep -v &amp;quot;CB&amp;quot; | wc -l
10647804

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;use-pybam&#34;&gt;use pybam&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.biostars.org/p/186732/&#34; target=&#34;_blank&#34;&gt;https://www.biostars.org/p/186732/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/JohnLonginotto/pybam&#34; target=&#34;_blank&#34;&gt;https://github.com/JohnLonginotto/pybam&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note that pybam is python2.x&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source activate py27
cd ~/apps
git clone https://github.com/JohnLonginotto/pybam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;inside python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pybam
import csv

cluster_dict = {}
with open(&#39;clusters.csv&#39;) as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=&#39;,&#39;)
    #skip header
    header = next(csv_reader)
    for row in csv_reader:
        cluster_dict[row[0]] = row[1]

clusters = set(x for x in cluster_dict.values())


# open the number of bam files as the same number of clusters, and map the out file handler to the cluster id

header = pybam.read(&#39;atac_v1_pbmc_5k_possorted_bam.bam&#39;).file_header
fouts_dict = {}
for cluster in clusters:
    fout = open(&amp;quot;cluster&amp;quot; + cluster + &amp;quot;.sam&amp;quot;, &amp;quot;w&amp;quot;)
    fout.write(header)
    fouts_dict[cluster] = fout

for read in pybam.read(&#39;possorted_bam.bam&#39;):
        ## not always the same position in the list for the CB tag
        ## there could be no CB tag for a certian read as well
        ## it will return empty list
        CB_list = [ x for x in read.sam_tags_list if x[0] == &amp;quot;CB&amp;quot;]
        if CB_list:
            cell_barcode = CB_list[0][2]
            cluster_id = cluster_dict.get(cell_barcode)
            if cluster_id:
                fouts_dict[cluster_id].write(read.sam + &#39;\n&#39;)
        
## do not forget to close the files
for fout in fouts_dict.values():
    fout.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;real    1262m30.849s
user    1240m11.906s
sys     22m9.325s&lt;/p&gt;

&lt;p&gt;Did not find how to write to a bam file, so I have to write to a sam file. I asked on github issues but no responses. The author is not actively maintaining the library anymore.&lt;/p&gt;

&lt;h3 id=&#34;use-hts-nim&#34;&gt;use hts-nim&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/brentp/hts-nim-tools/issues/5&#34; target=&#34;_blank&#34;&gt;https://github.com/brentp/hts-nim-tools/issues/5&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks Brent for providing the code.&lt;/p&gt;

&lt;h4 id=&#34;htslib-need-to-be-in-ld-library-path&#34;&gt;htslib need to be in &lt;code&gt;$LD_LIBRARY_PATH&lt;/code&gt;:&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/samtools/htslib/releases/download/1.6/htslib-1.6.tar.bz2
tar xjf htslib-1.6.tar.bz2
cd htslib-1.6
./configure ~/bin/

make

# add this to .bashrc and source ~/.bashrc
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/n/home02/mtang/apps/htslib-1.6
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;install-nim-and-hts-nim&#34;&gt;install &lt;code&gt;nim&lt;/code&gt; and &lt;code&gt;hts-nim&lt;/code&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://raw.githubusercontent.com/brentp/hts-nim/master/scripts/simple-install.sh

chmod u+x simple-install.sh
./simple-install.sh

# add nim to PATH

git clone https://github.com/brentp/hts-nim
cd hts-nim
nimble install -y
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import hts
import os
import strutils
import tables
var ibam:Bam

# lookup from cb -&amp;gt; cluster
var clusterTbl = initTable[string,string]()
# lookup from cluster -&amp;gt; bam
var tbl = initTable[string, Bam]()

for x in paramStr(1).lines:
  var toks = x.strip().split(&amp;quot;,&amp;quot;)
  clusterTbl[toks[0]] = toks[1]

if not open(ibam, paramStr(2)):
   quit &amp;quot;couldn&#39;t open bam&amp;quot;

for aln in ibam:
  var cb = tag[string](aln, &amp;quot;CB&amp;quot;).get
  if cb.isNullOrEmpty: continue
  if cb notin clusterTbl: continue
  var cluster = clusterTbl[cb]
  if cluster notin tbl:
    var obam: Bam
    if not open(obam, &amp;quot;out-cluster-&amp;quot; &amp;amp; cluster &amp;amp; &amp;quot;.bam&amp;quot;, mode=&amp;quot;w&amp;quot;):
      quit &amp;quot;couldn&#39;t open bam for writing&amp;quot;
    obam.write_header(ibam.hdr)
    tbl[cluster] = obam
  tbl[cluster].write(aln)

for k, bam in tbl:
  bam.close()
ibam.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;compile&#34;&gt;compile&lt;/h4&gt;

&lt;p&gt;save it to &lt;code&gt;split_scATAC_bam.nim&lt;/code&gt; and compile:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nim compile -d:release scATAC_split_scATAC_bam.nim
split_scATAC_bam clusters.csv atac_v1_pbmc_5k_possorted_bam.bam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;real    105m17.140s
user    102m17.214s
sys     2m58.312s&lt;/p&gt;

&lt;p&gt;it is &lt;sup&gt;172&lt;/sup&gt;&amp;frasl;&lt;sub&gt;105&lt;/sub&gt; &lt;strong&gt;~1.6 times faster&lt;/strong&gt; in &lt;code&gt;hts-nim&lt;/code&gt; than in &lt;code&gt;pysam&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;speed-up&#34;&gt;speed up&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;parallize by chromosome&lt;/li&gt;
&lt;li&gt;pysam parallization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hts-nim&lt;/code&gt; from Brent:&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;you can add &lt;code&gt;threads=2&lt;/code&gt; (or 3) to the &lt;code&gt;open&lt;/code&gt; calls to get a bit more speed on de/compressing the bam which will be the most CPU time&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I tested using &lt;code&gt;threads = 3&lt;/code&gt; for the same bam file, it took&lt;/p&gt;

&lt;p&gt;real    92m11.205s
user    100m11.622s
sys     6m3.067s&lt;/p&gt;

&lt;p&gt;one saved another 105-92 = 13 mins using multi-thread &lt;code&gt;hts-nim&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;C htslib, I expect the speed will be similar to &lt;code&gt;hts-nim&lt;/code&gt; since &lt;code&gt;hts-nim&lt;/code&gt; is a wrapper around it.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;lessons-learned&#34;&gt;Lessons learned&lt;/h3&gt;

&lt;p&gt;I had &lt;a href=&#34;https://github.com/brentp/hts-nim-tools/issues/5#issuecomment-464114496&#34; target=&#34;_blank&#34;&gt;a bug&lt;/a&gt; in my &lt;code&gt;pysam&lt;/code&gt; code and it pulls out some reads without the &lt;code&gt;CB&lt;/code&gt; tag. Thanks Brent for catching it. I spent some time to debug and could not find it.&lt;/p&gt;

&lt;p&gt;Lessons that I have learned:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How to make sure the output of the software is correct is very difficult. unit testing is important.&lt;/li&gt;
&lt;li&gt;It is good to have someone else with more programming experience to look at the code for you. You are so used to the code that you write and can not find the &amp;ldquo;obvious&amp;rdquo; problem.&lt;/li&gt;
&lt;li&gt;Do not use libraries that are not well maintained. The &lt;code&gt;pybam&lt;/code&gt; author is not maintaining the library now and it is written in python2.x. I am writing all my python code in python3.x&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have put the python and nim code at &lt;a href=&#34;https://github.com/crazyhottommy/scATACutils&#34; target=&#34;_blank&#34;&gt;scATACutils&lt;/a&gt;. The &lt;code&gt;pysam&lt;/code&gt; code and &lt;code&gt;hts-nim&lt;/code&gt; code generate exactly the same results.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>understand 10x scRNAseq and scATAC fastqs</title>
      <link>/post/understand-10x-scrnaseq-and-scatac-fastqs/</link>
      <pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/understand-10x-scrnaseq-and-scatac-fastqs/</guid>
      <description>

&lt;h3 id=&#34;single-cell-rnaseq&#34;&gt;single cell RNAseq&lt;/h3&gt;

&lt;p&gt;Please read the following posts by Dave Tang. When I google, I always find his posts on top of the pages. Thanks for sharing your knowledge.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://davetang.org/muse/2018/06/06/10x-single-cell-bam-files/&#34; target=&#34;_blank&#34;&gt;https://davetang.org/muse/2018/06/06/10x-single-cell-bam-files/&lt;/a&gt;
&lt;a href=&#34;https://davetang.org/muse/2018/08/09/getting-started-with-cell-ranger/&#34; target=&#34;_blank&#34;&gt;https://davetang.org/muse/2018/08/09/getting-started-with-cell-ranger/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From the 10x manual:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The final Single Cell 3’ Libraries contain the P5 and P7 primers used in Illumina bridge amplification PCR. The 10x Barcode and Read 1 (primer site for sequencing read 1) is added to the molecules during the GEMRT incubation. The P5 primer, Read 2 (primer site for sequencing read 2), Sample Index and P7 primer will be added during library construction&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/rnaseq_library.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A Single Cell 3’ Library comprises standard Illumina paired-end constructs which begin and end with P5 and P7. The Single Cell 3’ v2 16 bp 10x Barcodes are encoded at the start of Read 1, while sample index sequences are incorporated as the i7 index read. Read 1 and Read 2 are standard Illumina sequencing primer sites used in paired-end sequencing. Read 1 is used to sequence the 16 bp 10x Barcode and 10 bp UMI, while Read 2 is used to sequence the cDNA fragment.&lt;/p&gt;

&lt;p&gt;Each sample index provided in the Chromium i7 Sample Index Kit combines 4 different sequences in order to balance across all 4 nucleotides.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After &lt;code&gt;cellranger mkfastq&lt;/code&gt;, three &lt;code&gt;fastq.gz&lt;/code&gt; files will be produced: &lt;code&gt;I1&lt;/code&gt;, &lt;code&gt;R1&lt;/code&gt; and &lt;code&gt;R2&lt;/code&gt;. &lt;code&gt;I1&lt;/code&gt; is the 8 bp sample barcode, &lt;code&gt;R1&lt;/code&gt; is the 16bp &lt;code&gt;feature barcode&lt;/code&gt; + 10 bp &lt;code&gt;UMI&lt;/code&gt;, &lt;code&gt;R2&lt;/code&gt; is the reads mapped to the transcriptome.&lt;/p&gt;

&lt;p&gt;Feature barcode whitelist can be found at the cellranger installation path:  &lt;code&gt;cellranger-2.1.0/cellranger-cs/2.1.0/lib/python/cellranger/barcodes/737K-august-2016.txt&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;single-cell-atac&#34;&gt;single cell ATAC&lt;/h3&gt;

&lt;p&gt;after &lt;code&gt;cellranger-atac mkfastq&lt;/code&gt;, there four &lt;code&gt;fastq.gz&lt;/code&gt; files will be generated. &lt;code&gt;I1&lt;/code&gt;, &lt;code&gt;R1&lt;/code&gt;, &lt;code&gt;R2&lt;/code&gt; and &lt;code&gt;R3&lt;/code&gt;.
&lt;code&gt;I1&lt;/code&gt; is the 8 bp sample barcode, &lt;code&gt;R1&lt;/code&gt; is the forward read, &lt;code&gt;R2&lt;/code&gt; is the 16 bp &lt;code&gt;10x feature barcode&lt;/code&gt; and &lt;code&gt;R3&lt;/code&gt; is the reverse read. Thanks &lt;a href=&#34;https://twitter.com/Itti_Q&#34; target=&#34;_blank&#34;&gt;Aditi Qamra&lt;/a&gt; for pointing it out.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/atac_library.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The 16bp feature barcode whitelist can be found at cellranger-atac installation path:&lt;code&gt;cellranger-atac-1.0.1/cellranger-atac-cs/1.0.1/lib/python/barcodes/737K-cratac-v1.txt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that I have put the sample barcodes and feature barcodes files at &lt;a href=&#34;https://osf.io/2z9gj/files/&#34; target=&#34;_blank&#34;&gt;https://osf.io/2z9gj/files/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;are-the-feature-barcode-whitelist-the-same-for-scrnaseq-and-scatac&#34;&gt;Are the feature barcode whitelist the same for scRNAseq and scATAC?&lt;/h3&gt;

&lt;p&gt;In theory, 16bp barcode can have &lt;code&gt;4^16&lt;/code&gt; (4,294,967,296) combinations, but you will want some diversities of the sequences to better distinguish 2 barcodes.Keep in mind that there are PCR or sequencing errors for the barcodes.&lt;/p&gt;

&lt;p&gt;Both are 737K, but are the sequences the same?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat 737K-august-2016.txt | sort | uniq &amp;gt; scRNAseq_barcode.txt
cat 737K-cratac-v1.txt | sort | uniq &amp;gt; scATAC_barcode.txt

wc -l scRNAseq_barcode.txt
737280 scRNAseq_barcode.txt

wc -l scATAC_barcode.txt
737280 scATAC_barcode.txt

## only 10812 cell barcodes are common  
comm -12 scRNAseq_barcode.txt scATAC_barcode.txt  | wc -l
10812
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, 10x uses quite different cell barcodes for scRNAseq and scATACseq applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluating single cell RNAseq cluster stability</title>
      <link>/project/evaluating-scrnaseq-cluster/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 -0500</pubDate>
      
      <guid>/project/evaluating-scrnaseq-cluster/</guid>
      <description>&lt;p&gt;The goal of scclusteval(Single Cell Cluster Evaluation) is to evaluate the single cell clustering stability by boostrapping/subsampling the cells and provide many visualization methods for comparing clusters.&lt;/p&gt;

&lt;p&gt;for Theory behind the method, see Christian Henning, “Cluster-wise assessment of cluster stability,” Research Report 271, Dept. of Statistical Science, University College London, December 2006)&lt;/p&gt;

&lt;p&gt;You can find the package at my &lt;a href=&#34;https://github.com/crazyhottommy/scclusteval&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/raincloud_cluster.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to make a transcript to gene mapping file</title>
      <link>/post/how-to-make-a-transcript-to-gene-mapping-file/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-make-a-transcript-to-gene-mapping-file/</guid>
      <description>&lt;p&gt;I need a transcript to gene mapping file for &lt;code&gt;Salmon&lt;/code&gt;. I am aware of annotation &lt;code&gt;bioconductor&lt;/code&gt; packages that can do this job. However, I was working on a species which does not have the annotation in a package format (I am going to use Drosphila as an example for this blog post). I had to go and got the gtf file and made such a file from scratch.&lt;/p&gt;
&lt;p&gt;Please read the &lt;a href=&#34;https://useast.ensembl.org/info/website/upload/gff.html&#34;&gt;specifications&lt;/a&gt; of those two file formats.&lt;/p&gt;
&lt;div id=&#34;download-drosophila-gtf-file-from-ensemble-and-gff-file-from-ncbi&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Download drosophila gtf file from ENSEMBLE and gff file from NCBI&lt;/h3&gt;
&lt;p&gt;Find the &lt;code&gt;gff&lt;/code&gt; file at &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/genome/?term=drosophila+melanogaster&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/genome/?term=drosophila+melanogaster&lt;/a&gt;&lt;br /&gt;
Find the &lt;code&gt;gtf&lt;/code&gt; file at &lt;a href=&#34;ftp://ftp.ensembl.org/pub/release-95/gtf/drosophila_melanogaster/&#34; class=&#34;uri&#34;&gt;ftp://ftp.ensembl.org/pub/release-95/gtf/drosophila_melanogaster/&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;#gtf file
zless -S ~/Downloads/Drosophila_melanogaster.BDGP6.95.gtf.gz | grep -v &amp;quot;#&amp;quot; | cut -f3 | sort | uniq -c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 160859 CDS
##    4 Selenocysteine
## 187373 exon
## 46299 five_prime_utr
## 17737 gene
## 30492 start_codon
## 33892 three_prime_utr
## 34767 transcript&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;#gff file
zless -S ~/Downloads/GCF_000001215.4_Release_6_plus_ISO1_MT_genomic.gff.gz| grep -v &amp;quot;#&amp;quot; | cut -f3 | sort | uniq -c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 160949 CDS
##    1 RNase_MRP_RNA
##    2 RNase_P_RNA
##    2 SRP_RNA
##  584 antisense_RNA
## 187809 exon
## 17421 gene
## 2275 lnc_RNA
## 30480 mRNA
##  479 miRNA
## 5416 mobile_genetic_element
##   77 ncRNA
##  263 primary_transcript
##  308 pseudogene
##  134 rRNA
## 1870 region
##    1 sequence_feature
##   32 snRNA
##  289 snoRNA
##  319 tRNA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;use-unix-command-to-make-a-transcripts-to-gene-mapping-file-from-gtf-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use unix command to make a transcripts to gene mapping file from gtf file&lt;/h3&gt;
&lt;p&gt;We see the feature types are quite different although they are both annotation files for the same species.
The &lt;code&gt;gtf&lt;/code&gt; file is relatively well formatted, and we can make a transcripts to gene mapping file easily using
unix command line.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;zless -S ~/Downloads/Drosophila_melanogaster.BDGP6.95.gtf.gz | grep -v &amp;quot;#&amp;quot; | awk &amp;#39;$3==&amp;quot;transcript&amp;quot;&amp;#39; | cut -f9 | tr -s &amp;quot;;&amp;quot; &amp;quot; &amp;quot; | awk &amp;#39;{print$4&amp;quot;\t&amp;quot;$2}&amp;#39; | sort | uniq |  sed &amp;#39;s/\&amp;quot;//g&amp;#39; | tee tx2gene_ensemble.tsv| head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## FBgn0013687  FBgn0013687
## FBtr0005088  FBgn0260439
## FBtr0006151  FBgn0000056
## FBtr0070000  FBgn0031081
## FBtr0070001  FBgn0052826
## FBtr0070002  FBgn0031085
## FBtr0070003  FBgn0062565
## FBtr0070006  FBgn0031089
## FBtr0070007  FBgn0031092
## FBtr0070008  FBgn0031094&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hmm…why the first line has both genes in the two columns?…
sanity check:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;zless -S ~/Downloads/Drosophila_melanogaster.BDGP6.95.gtf.gz | grep &amp;quot;FBgn0013687&amp;quot; | less -S&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## mitochondrion_genome FlyBase gene    14917   19524   .   +   .   gene_id &amp;quot;FBgn0013687&amp;quot;; gene_name &amp;quot;mt:ori&amp;quot;; gene_source &amp;quot;FlyBase&amp;quot;; gene_biotype &amp;quot;pseudogene&amp;quot;;
## mitochondrion_genome FlyBase transcript  14917   19524   .   +   .   gene_id &amp;quot;FBgn0013687&amp;quot;; transcript_id &amp;quot;FBgn0013687&amp;quot;; gene_name &amp;quot;mt:ori&amp;quot;; gene_source &amp;quot;FlyBase&amp;quot;; gene_biotype &amp;quot;pseudogene&amp;quot;; transcript_source &amp;quot;FlyBase&amp;quot;; transcript_biotype &amp;quot;pseudogene&amp;quot;;
## mitochondrion_genome FlyBase exon    14917   19524   .   +   .   gene_id &amp;quot;FBgn0013687&amp;quot;; transcript_id &amp;quot;FBgn0013687&amp;quot;; exon_number &amp;quot;1&amp;quot;; gene_name &amp;quot;mt:ori&amp;quot;; gene_source &amp;quot;FlyBase&amp;quot;; gene_biotype &amp;quot;pseudogene&amp;quot;; transcript_source &amp;quot;FlyBase&amp;quot;; transcript_biotype &amp;quot;pseudogene&amp;quot;; exon_id &amp;quot;FBgn0013687-E1&amp;quot;;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed it is in the original gtf file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-gffutilsto-make-a-transcripts-to-gene-mapping-file-from-gff-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use &lt;code&gt;gffutils&lt;/code&gt;to make a transcripts to gene mapping file from gff file&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;gff&lt;/code&gt; file is not that well defined. One may still be able to use some unix tricks to get the tx2gene.tsv file from a gff file, but it can be rather awkward especially for gff files from other not well annotated species. Instead, let’s use &lt;code&gt;gffutils&lt;/code&gt;, a python package to do the same.&lt;/p&gt;
&lt;p&gt;install &lt;code&gt;gffutils&lt;/code&gt; in terminal:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;source activate snakemake&lt;/code&gt;&lt;br /&gt;
&lt;code&gt;conda install gffutils&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note, I am running python through Rsutdio/ First read how to set python path for &lt;code&gt;reticulate&lt;/code&gt; at &lt;a href=&#34;https://rstudio.github.io/reticulate/articles/versions.html&#34; class=&#34;uri&#34;&gt;https://rstudio.github.io/reticulate/articles/versions.html&lt;/a&gt;
read more on &lt;a href=&#34;https://cran.r-project.org/web/packages/reticulate/vignettes/versions.html&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/reticulate/vignettes/versions.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Somehow, I have to create a &lt;code&gt;.Rprofile&lt;/code&gt; in the same folder of &lt;code&gt;.Rproj&lt;/code&gt; file with the following line to use my snakemake conda environment which is python3:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Sys.setenv(PATH = paste(&amp;quot;/anaconda3/envs/snakemake/bin/&amp;quot;, Sys.getenv(&amp;quot;PATH&amp;quot;), sep=&amp;quot;:&amp;quot;))&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reticulate)

# check which python I am using
py_discover_config()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## python:         /anaconda3/envs/snakemake/bin//python
## libpython:      /anaconda3/envs/snakemake/lib/libpython3.6m.dylib
## pythonhome:     /anaconda3/envs/snakemake:/anaconda3/envs/snakemake
## version:        3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 14:01:38)  [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
## numpy:          /anaconda3/envs/snakemake/lib/python3.6/site-packages/numpy
## numpy_version:  1.15.3
## 
## python versions found: 
##  /anaconda3/envs/snakemake/bin//python
##  /usr/bin/python
##  /anaconda3/envs/py27/bin/python
##  /anaconda3/envs/snakemake/bin/python&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these did not work for me...
# use_condaenv(&amp;quot;snakemake&amp;quot;, required = TRUE)
# use_python(&amp;quot;/anaconda3/envs/snakemake/bin/python&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import sys
print(sys.version)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 14:05:31) 
## [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import gffutils
import itertools
import os
os.listdir()
db = gffutils.create_db(&amp;quot;GCF_000001215.4_Release_6_plus_ISO1_MT_genomic.gff.gz&amp;quot;, &amp;quot;:memory:&amp;quot;, force = True,merge_strategy=&amp;quot;merge&amp;quot;, id_spec={&amp;#39;gene&amp;#39;: &amp;#39;Dbxref&amp;#39;})
list(db.featuretypes())
# one can do it for one type of features, say mRNA
for mRNA in itertools.islice(db.features_of_type(&amp;#39;mRNA&amp;#39;), 10):
        print(mRNA[&amp;#39;transcript_id&amp;#39;][0], mRNA[&amp;#39;gene&amp;#39;][0])
        #print(mRNA.attributes.items())
        
## but I then have to do the same for lnc_RNA and others.        
## instead, loop over all features in the database&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NM_001103384.3 CG17636
## NM_001258513.2 CG17636
## NM_001258512.2 CG17636
## NM_001297796.1 RhoGAP1A
## NM_001297795.1 RhoGAP1A
## NM_001103385.2 RhoGAP1A
## NM_001103386.2 RhoGAP1A
## NM_001169155.1 RhoGAP1A
## NM_001297797.1 RhoGAP1A
## NM_001297801.1 tyn&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tx_and_gene=[]
with open(&amp;quot;tx2gene_NCBI.tsv&amp;quot;, &amp;quot;w&amp;quot;) as f:
        for feature in db.all_features():
                transcript = feature.attributes.get(&amp;#39;transcript_id&amp;#39;, [None])[0]
                gene = feature.attributes.get(&amp;#39;gene&amp;#39;, [None])[0]
                if gene and transcript and ([transcript, gene] not in tx_and_gene):
                        tx_and_gene.append([transcript, gene])
                        f.write(transcript + &amp;quot;\t&amp;quot; + gene + &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These lines of codes are not hard to write. It takes more time to read the package documentation and understand how to use the package. One problem with bioinFORMATics is that there are so many different file formats. To make things worse, even for gff file format, many files do not follow the exact specification. You can have a taste of that at &lt;a href=&#34;http://daler.github.io/gffutils/examples.html&#34; class=&#34;uri&#34;&gt;http://daler.github.io/gffutils/examples.html&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
