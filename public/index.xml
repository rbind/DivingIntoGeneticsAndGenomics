<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DNA confesses Data speak on DNA confesses Data speak</title>
    <link>/</link>
    <description>Recent content in DNA confesses Data speak on DNA confesses Data speak</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Ming Tang</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Reproducible research in bioinformatics</title>
      <link>/talk/2019-bunkerhill-talk/</link>
      <pubDate>Thu, 28 Mar 2019 14:30:00 -0400</pubDate>
      
      <guid>/talk/2019-bunkerhill-talk/</guid>
      <description>&lt;p&gt;I was invited to give a talk on reproducible bioinformatics research to the students in the &lt;a href=&#34;https://www.bhcc.edu/&#34; target=&#34;_blank&#34;&gt;Bunker Hill Community College&lt;/a&gt; in Boston, MA. I was so glad to introduce bioinformatics to the students and share my own perspectives on reproducible research.&lt;/p&gt;

&lt;p&gt;The movie &lt;a href=&#34;https://en.wikipedia.org/wiki/Good_Will_Hunting&#34; target=&#34;_blank&#34;&gt;Good Will Hunting&lt;/a&gt; was shot there :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/bunkerhill-talk.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KRAS-IRF2 Axis Drives Immune Suppression and Immune Therapy Resistance in Colorectal Cancer</title>
      <link>/publication/2019-03-20-irf2/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 -0400</pubDate>
      
      <guid>/publication/2019-03-20-irf2/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Use docopt to write command line R utilities </title>
      <link>/post/use-docopt-to-write-command-line-r-utilities/</link>
      <pubDate>Fri, 22 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/use-docopt-to-write-command-line-r-utilities/</guid>
      <description>&lt;p&gt;I was writing an R script to plot the ATACseq fragment length distribution and wanted
to turn the R script to a command line utility.&lt;/p&gt;

&lt;p&gt;I then (re)discovered this awesome &lt;a href=&#34;https://github.com/docopt/docopt.R&#34; target=&#34;_blank&#34;&gt;docopt.R&lt;/a&gt;.
One just needs to write the help message the you want to display and &lt;code&gt;docopt()&lt;/code&gt; will
parse the options, arguments and return a named list which can be accessed inside the
R script. check &lt;a href=&#34;http://docopt.org/&#34; target=&#34;_blank&#34;&gt;http://docopt.org/&lt;/a&gt; for more information as well.&lt;/p&gt;

&lt;p&gt;See below for an example. You can download it at &lt;a href=&#34;https://github.com/crazyhottommy/scATACutils/blob/master/R/plot_atac_frag_distribution.R&#34; target=&#34;_blank&#34;&gt;https://github.com/crazyhottommy/scATACutils/blob/master/R/plot_atac_frag_distribution.R&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;
#! /usr/bin/env Rscript
&#39;Plot fragment length distribution from ATACseq data
Usage:
    plot_atac_fragment_len_distribution.R (--poly | --hist) (--pdf | --png) [--width=&amp;lt;width&amp;gt; --height=&amp;lt;height&amp;gt; --bin=&amp;lt;bp&amp;gt;] &amp;lt;input&amp;gt; &amp;lt;output&amp;gt;
    
Options:
    -h --help  Show this screen.
    -v --version  Show version.
    --bin=&amp;lt;bp&amp;gt;  Bin size [default: 5]
    --poly  Plot frequency polygon.
    --hist  Plot histogram.
    --pdf  Save to pdf.
    --png  Save to png.
    --width=&amp;lt;width&amp;gt;  Width of the output [default: 4]
    --height=&amp;lt;height&amp;gt; Height of the output [default: 4]

Arguments:
    input  fragment length in a one column dataframe without header or stdin
    output  output filename
&#39; -&amp;gt; doc

suppressMessages(library(ggplot2))
# check this awesome docoptR https://github.com/docopt/docopt.R
## make sure use the development version, the CRAN version not working for me
# library(devtools) 
# devtools::install_github(&amp;quot;docopt/docopt.R&amp;quot;)
suppressMessages(library(docopt))
suppressMessages(library(dplyr))

# this will give error if try interactively, because no input and output argument are given
# https://github.com/docopt/docopt.R/issues/27
arguments &amp;lt;- docopt(doc, version = &#39;plot_atac_frag_distribution v1.0\n\n&#39;)

# for testing interactively
#arguments &amp;lt;- docopt(doc, version = &#39;FragmentSizeDistribution v1.0&#39;, args = c(&amp;quot;scripts/fragment3.txt&amp;quot;,&amp;quot;my.pdf&amp;quot;))
#print(arguments)

## File Read ##
# taken from https://stackoverflow.com/questions/26152998/how-to-make-r-script-takes-input-from-pipe-and-user-given-parameter
# if the input is stdin one can do 
# cat fragment.txt | ./plot_atac_frag_distribution.R --poly --pdf stdin  out.pdf
# cat fragment.txt | ./plot_atac_frag_distribution.R --poly --pdf - out.pdf
# ./plot_atac_frag_distribution.R --poly --pdf &amp;lt;(cat fragment.txt)  out.pdf


OpenRead &amp;lt;- function(arg) {
    if (arg %in% c(&amp;quot;-&amp;quot;, &amp;quot;/dev/stdin&amp;quot;)) {
        file(&amp;quot;stdin&amp;quot;, open = &amp;quot;r&amp;quot;)
    } else if (grepl(&amp;quot;^/dev/fd/&amp;quot;, arg)) {
        fifo(arg, open = &amp;quot;r&amp;quot;)
    } else {
        file(arg, open = &amp;quot;r&amp;quot;)
    }
}

dat.con &amp;lt;- OpenRead(arguments$input)
fragment &amp;lt;- read.table(dat.con, header = FALSE)

#fragment&amp;lt;- read.table(arguments$input, header = F)

names(fragment)&amp;lt;- c(&amp;quot;length&amp;quot;)

plot_hist&amp;lt;- function(fragment, bin) {
        g&amp;lt;- ggplot(fragment %&amp;gt;% filter(length &amp;lt;=2000), aes(x = length)) + 
                geom_histogram(binwidth = bin, aes(y=..density..), fill = &amp;quot;red&amp;quot;) +
                geom_density(alpha=.2, fill=&amp;quot;#FF6666&amp;quot;, col = &amp;quot;black&amp;quot;) +
                coord_cartesian(xlim = c(0,1000)) +
                scale_x_continuous(breaks = c(0, 100, 200, 300, 400, 800)) +
                theme_minimal(base_size = 14)
        return(g)
        
}

plot_polygon&amp;lt;- function(fragment, bin){
        g&amp;lt;- ggplot(fragment %&amp;gt;% filter(length &amp;lt;=2000), aes(x = length, stat(density))) + 
                geom_freqpoly(binwidth = bin, col = &amp;quot;blue&amp;quot;) +
                coord_cartesian(xlim = c(0,1000)) +
                scale_x_continuous(breaks = c(0, 100, 200, 300, 400, 800)) +
                theme_minimal(base_size = 14)
        return(g)
}


main&amp;lt;- function(fragment, arguments){
    if (arguments$poly){
        g&amp;lt;- plot_polygon(fragment, as.numeric(arguments$bin))
    } else if (arguments$hist){
        g&amp;lt;- plot_hist(fragment, as.numeric(arguments$bin))
    }
    device&amp;lt;- ifelse(arguments$pdf, &amp;quot;pdf&amp;quot;, &amp;quot;png&amp;quot;)
    
    ggsave(arguments$output, plot = g,  device = device, width =as.numeric(arguments$width), 
           height = as.numeric(arguments$height) )
    
}

main(fragment, arguments)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;save it to &lt;code&gt;plot_atac_frag_distribution.R&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;on command line, one can do:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;
./plot_atac_frag_distribution.R -h
Plot fragment length distribution from ATACseq data
Usage:
    plot_atac_fragment_len_distribution.R (--poly | --hist) (--pdf | --png) [--width=&amp;lt;width&amp;gt; --height=&amp;lt;height&amp;gt; --bin=&amp;lt;bp&amp;gt;] &amp;lt;input&amp;gt; &amp;lt;output&amp;gt;

Options:
    -h --help  Show this screen.
    -v --version  Show version.
    --bin=&amp;lt;bp&amp;gt;  Bin size [default: 5]
    --poly  Plot frequency polygon.
    --hist  Plot histogram.
    --pdf  Save to pdf.
    --png  Save to png.
    --width=&amp;lt;width&amp;gt;  Width of the output [default: 4]
    --height=&amp;lt;height&amp;gt; Height of the output [default: 4]

Arguments:
    input  fragment length in a one column dataframe without header or stdin
    output  output filename

./plot_atac_frag_distribution.R --poly --png  --bin 10 fragment.txt out.png
cat fragment.txt | ./plot_atac_frag_distribution.R --poly --pdf stdin  out.pdf
cat fragment.txt | ./plot_atac_frag_distribution.R --hist --pdf - out.pdf
./plot_atac_frag_distribution.R --hist --pdf &amp;lt;(cat fragment.txt)  out.pdf

samtools view my.bam | awk &#39;$9&amp;gt;0&#39; | cut -f 9 |./plot_atac_frag_distribution.R --poly --pdf - out.pdf

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;histogram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/out2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;polygon:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/out3.png&#34; alt=&#34;&#34; /&gt;
Pretty cool!!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Important notes:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[default: 4]  The space after &lt;code&gt;:&lt;/code&gt; is needed.&lt;/li&gt;
&lt;li&gt;use two spaces to separate the option and the explanation&lt;/li&gt;
&lt;li&gt;use four spaces to indent&lt;/li&gt;
&lt;li&gt;use the development version of optdoc.R&lt;/li&gt;
&lt;li&gt;when testing interactively. docopt() may give error when the mandatory arguments
are not specified, but running on command line is fine.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;see &lt;a href=&#34;https://github.com/docopt/docopt.R/issues/24&#34; target=&#34;_blank&#34;&gt;https://github.com/docopt/docopt.R/issues/24&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also check &lt;a href=&#34;http://dirk.eddelbuettel.com/code/littler.examples.html&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;littler&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;littler provides the r program, a simplified command-line interface for GNU R. This allows direct execution of commands, use in piping where the output of one program supplies the input of the next, as well as adding the ability for writing hash-bang scripts, i.e. creating executable files starting with, say, #!/usr/bin/r.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Split a 10xscATAC bam file by cluster</title>
      <link>/post/split-a-10xscatac-bam-file-by-cluster/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/split-a-10xscatac-bam-file-by-cluster/</guid>
      <description>

&lt;p&gt;I want to split the PBMC scATAC bam from 10x by cluster id. So, I can then make a bigwig for each cluster to visualize in &lt;code&gt;IGV&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The first thing I did was googling to see if anyone has written such a tool (Do not reinvent the wheels!). People have done that because I saw figures from the scATAC papers. I just could not find it. Maybe I need to refine my googling skills.&lt;/p&gt;

&lt;p&gt;I decided to write one myself. The following is my journey for this small task.&lt;/p&gt;

&lt;p&gt;download the 5k pbmc scATAC data from &lt;a href=&#34;https://support.10xgenomics.com/single-cell-atac/datasets/1.0.1/atac_v1_pbmc_5k&#34; target=&#34;_blank&#34;&gt;https://support.10xgenomics.com/single-cell-atac/datasets/1.0.1/atac_v1_pbmc_5k&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;split-the-cell-barcodes-by-cluster-id&#34;&gt;split the cell barcodes by cluster id&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd analysis/clustering/graphclust
head clusters.csv
Barcode,Cluster
AAACGAAAGCGCAATG-1,1
AAACGAAAGGGTATCG-1,4
AAACGAAAGTAACATG-1,8
AAACGAAAGTTACACC-1,1
AAACGAACAGAGATGC-1,4
AAACGAACATGCTATG-1,5
AAACGAAGTGCATCAT-1,3
AAACGAAGTGGACGAT-1,3
AAACGAAGTGGCCTCA-1,7

# there are ^M characters at the end of the line if you do cat -A you will see it.
# change it to unix
dos2unix clusters.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;awk -F&amp;quot;,&amp;quot; &#39;NR&amp;gt;1{print $1 &amp;gt;&amp;gt; &amp;quot;cluster_&amp;quot;$2&amp;quot;.csv&amp;quot;}&#39; clusters.csv
wc -l *csv
   330 cluster_10.csv
   322 cluster_11.csv
   258 cluster_12.csv
   191 cluster_13.csv
   608 cluster_1.csv
   563 cluster_2.csv
   559 cluster_3.csv
   532 cluster_4.csv
   483 cluster_5.csv
   425 cluster_6.csv
   366 cluster_7.csv
   360 cluster_8.csv
   338 cluster_9.csv
  5336 clusters.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;use-bamtools&#34;&gt;use bamtools&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;time bamtools filter -tag CB:Z:AAACTGCAGAGCAGCT-1 -in atac_v1_pbmc_5k_possorted_bam.bam -out AAACTGCAGAGCAGCT-1.bam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This takes a little over 1 hour for one barcode! And there is no easy way
to specify a group of barcodes.&lt;/p&gt;

&lt;h3 id=&#34;use-the-linux-tricks&#34;&gt;use the linux tricks&lt;/h3&gt;

&lt;p&gt;inspired partly by this post &lt;a href=&#34;https://www.biostars.org/p/263346/&#34; target=&#34;_blank&#34;&gt;https://www.biostars.org/p/263346/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wc -l clusters.csv
5336 clusters.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and let&amp;rsquo;s see how fast each regular expression takes for &lt;code&gt;awk&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;time samtools view atac_v1_pbmc_5k_possorted_bam.bam | awk -v tag=&amp;quot;CB:Z:AAACTGCAGAGCAGCT-1&amp;quot; &#39;index($0,tag)&amp;gt;0&#39; &amp;gt;&amp;gt; AAACTGCAGAGCAGCT-1.sam

real    27m14.332s
user    48m36.883s
sys     4m37.908s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is not too bad, but if we loops over the &lt;code&gt;clusters.csv&lt;/code&gt; files for 5335 times,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;samtools view -H atac_v1_pbmc_5k_possorted_bam.bam &amp;gt; header.txt

cat clusters.csv \
| sed &#39;1d&#39; \
| while IFS=&#39;,&#39; read -r barcode cluster
    do samtools view atac_v1_pbmc_5k_possorted_bam.bam |  awk -v tag=&amp;quot;CB:Z:$barcode&amp;quot; &#39;index($0,tag)&amp;gt;0&#39; &amp;gt;&amp;gt; &amp;quot;$cluster.sam&amp;quot;
    done

## then cat the header with the sam.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;it will take ~30min * 5335 = ~100 days to finish.&lt;/p&gt;

&lt;p&gt;we can do better to parallize by GNU parallel&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## not tested...
cat clusters.csv \
| sed &#39;1d&#39; \
| parallel --colsep &#39;,&#39; -j 40 &#39;samtools view atac_v1_pbmc_5k_possorted_bam.bam |awk -v tag=&amp;quot;CB:Z:{1}&amp;quot; &#39;index(\$0,tag)&amp;gt;0&#39; &amp;gt;&amp;gt; {2}.sam&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, using 40 cores may reduce our time to &lt;sup&gt;100&lt;/sup&gt;&amp;frasl;&lt;sub&gt;40&lt;/sub&gt; = 5 days.&lt;/p&gt;

&lt;h3 id=&#34;use-pysam&#34;&gt;use pysam&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s only loop over the sam file once&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pysam
import csv

cluster_dict = {}
with open(&#39;clusters.csv&#39;) as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=&#39;,&#39;)
    #skip header
    header = next(csv_reader)
    for row in csv_reader:
        cluster_dict[row[0]] = row[1]

clusters = set(x for x in cluster_dict.values())


fin = pysam.AlignmentFile(&amp;quot;atac_v1_pbmc_5k_possorted_bam.bam&amp;quot;, &amp;quot;rb&amp;quot;)

# open the number of bam files as the same number of clusters, and map the out file handler to the cluster id, write to a bam with wb
fouts_dict = {}
for cluster in clusters:
    fout = pysam.AlignmentFile(&amp;quot;cluster&amp;quot; + cluster + &amp;quot;.bam&amp;quot;, &amp;quot;wb&amp;quot;, template = fin)
    fouts_dict[cluster] = fout

for read in fin:
    tags = read.tags
    CB_list = [ x for x in tags if x[0] == &amp;quot;CB&amp;quot;]
    if CB_list:
        cell_barcode = CB_list[0][1]
    # the bam files may contain reads not in the final clustered barcodes
    # will be None if the barcode is not in the clusters.csv file
    else: 
        continue
    cluster_id = cluster_dict.get(cell_barcode)
    if cluster_id:
        fouts_dict[cluster_id].write(read)

## do not forget to close the files
fin.close()
for fout in fouts_dict.values():
    fout.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;real    172m58.758s
user    172m10.678s
sys     0m46.071s&lt;/p&gt;

&lt;p&gt;Note, some read record in the bam file do not have &lt;code&gt;CB&lt;/code&gt; but only &lt;code&gt;CR&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;from &lt;a href=&#34;https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/output/bam&#34; target=&#34;_blank&#34;&gt;https://support.10xgenomics.com/single-cell-atac/software/pipelines/latest/output/bam&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CB&lt;/td&gt;
&lt;td&gt;Chromium cellular barcode sequence that is error-corrected and confirmed against a list of known-good barcode sequences.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;CR&lt;/td&gt;
&lt;td&gt;Chromium cellular barcode sequence as reported by the sequencer.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;How many of those reads with &lt;code&gt;CR&lt;/code&gt;?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# every read has a CR tag
samtools view atac_v1_pbmc_5k_possorted_bam.bam| grep -v &amp;quot;CR&amp;quot; | wc -l
0 

# not every read has a CB tag.
samtools view atac_v1_pbmc_5k_possorted_bam.bam| grep -v &amp;quot;CB&amp;quot; | wc -l
10647804

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;use-pybam&#34;&gt;use pybam&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.biostars.org/p/186732/&#34; target=&#34;_blank&#34;&gt;https://www.biostars.org/p/186732/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/JohnLonginotto/pybam&#34; target=&#34;_blank&#34;&gt;https://github.com/JohnLonginotto/pybam&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note that pybam is python2.x&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source activate py27
cd ~/apps
git clone https://github.com/JohnLonginotto/pybam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;inside python:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pybam
import csv

cluster_dict = {}
with open(&#39;clusters.csv&#39;) as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=&#39;,&#39;)
    #skip header
    header = next(csv_reader)
    for row in csv_reader:
        cluster_dict[row[0]] = row[1]

clusters = set(x for x in cluster_dict.values())


# open the number of bam files as the same number of clusters, and map the out file handler to the cluster id

header = pybam.read(&#39;atac_v1_pbmc_5k_possorted_bam.bam&#39;).file_header
fouts_dict = {}
for cluster in clusters:
    fout = open(&amp;quot;cluster&amp;quot; + cluster + &amp;quot;.sam&amp;quot;, &amp;quot;w&amp;quot;)
    fout.write(header)
    fouts_dict[cluster] = fout

for read in pybam.read(&#39;possorted_bam.bam&#39;):
        ## not always the same position in the list for the CB tag
        ## there could be no CB tag for a certian read as well
        ## it will return empty list
        CB_list = [ x for x in read.sam_tags_list if x[0] == &amp;quot;CB&amp;quot;]
        if CB_list:
            cell_barcode = CB_list[0][2]
            cluster_id = cluster_dict.get(cell_barcode)
            if cluster_id:
                fouts_dict[cluster_id].write(read.sam + &#39;\n&#39;)
        
## do not forget to close the files
for fout in fouts_dict.values():
    fout.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;real    1262m30.849s
user    1240m11.906s
sys     22m9.325s&lt;/p&gt;

&lt;p&gt;Did not find how to write to a bam file, so I have to write to a sam file. I asked on github issues but no responses. The author is not actively maintaining the library anymore.&lt;/p&gt;

&lt;h3 id=&#34;use-hts-nim&#34;&gt;use hts-nim&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/brentp/hts-nim-tools/issues/5&#34; target=&#34;_blank&#34;&gt;https://github.com/brentp/hts-nim-tools/issues/5&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks Brent for providing the code.&lt;/p&gt;

&lt;h4 id=&#34;htslib-need-to-be-in-ld-library-path&#34;&gt;htslib need to be in &lt;code&gt;$LD_LIBRARY_PATH&lt;/code&gt;:&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/samtools/htslib/releases/download/1.6/htslib-1.6.tar.bz2
tar xjf htslib-1.6.tar.bz2
cd htslib-1.6
./configure ~/bin/

make

# add this to .bashrc and source ~/.bashrc
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/n/home02/mtang/apps/htslib-1.6
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;install-nim-and-hts-nim&#34;&gt;install &lt;code&gt;nim&lt;/code&gt; and &lt;code&gt;hts-nim&lt;/code&gt;&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://raw.githubusercontent.com/brentp/hts-nim/master/scripts/simple-install.sh

chmod u+x simple-install.sh
./simple-install.sh

# add nim to PATH

git clone https://github.com/brentp/hts-nim
cd hts-nim
nimble install -y
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import hts
import os
import strutils
import tables
var ibam:Bam

# lookup from cb -&amp;gt; cluster
var clusterTbl = initTable[string,string]()
# lookup from cluster -&amp;gt; bam
var tbl = initTable[string, Bam]()

for x in paramStr(1).lines:
  var toks = x.strip().split(&amp;quot;,&amp;quot;)
  clusterTbl[toks[0]] = toks[1]

if not open(ibam, paramStr(2)):
   quit &amp;quot;couldn&#39;t open bam&amp;quot;

for aln in ibam:
  var cb = tag[string](aln, &amp;quot;CB&amp;quot;).get
  if cb.isNullOrEmpty: continue
  if cb notin clusterTbl: continue
  var cluster = clusterTbl[cb]
  if cluster notin tbl:
    var obam: Bam
    if not open(obam, &amp;quot;out-cluster-&amp;quot; &amp;amp; cluster &amp;amp; &amp;quot;.bam&amp;quot;, mode=&amp;quot;w&amp;quot;):
      quit &amp;quot;couldn&#39;t open bam for writing&amp;quot;
    obam.write_header(ibam.hdr)
    tbl[cluster] = obam
  tbl[cluster].write(aln)

for k, bam in tbl:
  bam.close()
ibam.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;compile&#34;&gt;compile&lt;/h4&gt;

&lt;p&gt;save it to &lt;code&gt;split_scATAC_bam.nim&lt;/code&gt; and compile:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nim compile -d:release scATAC_split_scATAC_bam.nim
split_scATAC_bam clusters.csv atac_v1_pbmc_5k_possorted_bam.bam
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;real    105m17.140s
user    102m17.214s
sys     2m58.312s&lt;/p&gt;

&lt;p&gt;it is &lt;sup&gt;172&lt;/sup&gt;&amp;frasl;&lt;sub&gt;105&lt;/sub&gt; &lt;strong&gt;~1.6 times faster&lt;/strong&gt; in &lt;code&gt;hts-nim&lt;/code&gt; than in &lt;code&gt;pysam&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;speed-up&#34;&gt;speed up&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;parallize by chromosome&lt;/li&gt;
&lt;li&gt;pysam parallization&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hts-nim&lt;/code&gt; from Brent:&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;you can add &lt;code&gt;threads=2&lt;/code&gt; (or 3) to the &lt;code&gt;open&lt;/code&gt; calls to get a bit more speed on de/compressing the bam which will be the most CPU time&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I tested using &lt;code&gt;threads = 3&lt;/code&gt; for the same bam file, it took&lt;/p&gt;

&lt;p&gt;real    92m11.205s
user    100m11.622s
sys     6m3.067s&lt;/p&gt;

&lt;p&gt;one saved another 105-92 = 13 mins using multi-thread &lt;code&gt;hts-nim&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;C htslib, I expect the speed will be similar to &lt;code&gt;hts-nim&lt;/code&gt; since &lt;code&gt;hts-nim&lt;/code&gt; is a wrapper around it.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;lessons-learned&#34;&gt;Lessons learned&lt;/h3&gt;

&lt;p&gt;I had &lt;a href=&#34;https://github.com/brentp/hts-nim-tools/issues/5#issuecomment-464114496&#34; target=&#34;_blank&#34;&gt;a bug&lt;/a&gt; in my &lt;code&gt;pysam&lt;/code&gt; code and it pulls out some reads without the &lt;code&gt;CB&lt;/code&gt; tag. Thanks Brent for catching it. I spent some time to debug and could not find it.&lt;/p&gt;

&lt;p&gt;Lessons that I have learned:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How to make sure the output of the software is correct is very difficult. unit testing is important.&lt;/li&gt;
&lt;li&gt;It is good to have someone else with more programming experience to look at the code for you. You are so used to the code that you write and can not find the &amp;ldquo;obvious&amp;rdquo; problem.&lt;/li&gt;
&lt;li&gt;Do not use libraries that are not well maintained. The &lt;code&gt;pybam&lt;/code&gt; author is not maintaining the library now and it is written in python2.x. I am writing all my python code in python3.x&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have put the python and nim code at &lt;a href=&#34;https://github.com/crazyhottommy/scATACutils&#34; target=&#34;_blank&#34;&gt;scATACutils&lt;/a&gt;. The &lt;code&gt;pysam&lt;/code&gt; code and &lt;code&gt;hts-nim&lt;/code&gt; code generate exactly the same results.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>understand 10x scRNAseq and scATAC fastqs</title>
      <link>/post/understand-10x-scrnaseq-and-scatac-fastqs/</link>
      <pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/understand-10x-scrnaseq-and-scatac-fastqs/</guid>
      <description>

&lt;h3 id=&#34;single-cell-rnaseq&#34;&gt;single cell RNAseq&lt;/h3&gt;

&lt;p&gt;Please read the following posts by Dave Tang. When I google, I always find his posts on top of the pages. Thanks for sharing your knowledge.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://davetang.org/muse/2018/06/06/10x-single-cell-bam-files/&#34; target=&#34;_blank&#34;&gt;https://davetang.org/muse/2018/06/06/10x-single-cell-bam-files/&lt;/a&gt;
&lt;a href=&#34;https://davetang.org/muse/2018/08/09/getting-started-with-cell-ranger/&#34; target=&#34;_blank&#34;&gt;https://davetang.org/muse/2018/08/09/getting-started-with-cell-ranger/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From the 10x manual:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The final Single Cell 3’ Libraries contain the P5 and P7 primers used in Illumina bridge amplification PCR. The 10x Barcode and Read 1 (primer site for sequencing read 1) is added to the molecules during the GEMRT incubation. The P5 primer, Read 2 (primer site for sequencing read 2), Sample Index and P7 primer will be added during library construction&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/rnaseq_library.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A Single Cell 3’ Library comprises standard Illumina paired-end constructs which begin and end with P5 and P7. The Single Cell 3’ v2 16 bp 10x Barcodes are encoded at the start of Read 1, while sample index sequences are incorporated as the i7 index read. Read 1 and Read 2 are standard Illumina sequencing primer sites used in paired-end sequencing. Read 1 is used to sequence the 16 bp 10x Barcode and 10 bp UMI, while Read 2 is used to sequence the cDNA fragment.&lt;/p&gt;

&lt;p&gt;Each sample index provided in the Chromium i7 Sample Index Kit combines 4 different sequences in order to balance across all 4 nucleotides.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After &lt;code&gt;cellranger mkfastq&lt;/code&gt;, three &lt;code&gt;fastq.gz&lt;/code&gt; files will be produced: &lt;code&gt;I1&lt;/code&gt;, &lt;code&gt;R1&lt;/code&gt; and &lt;code&gt;R2&lt;/code&gt;. &lt;code&gt;I1&lt;/code&gt; is the 8 bp sample barcode, &lt;code&gt;R1&lt;/code&gt; is the 16bp &lt;code&gt;feature barcode&lt;/code&gt; + 10 bp &lt;code&gt;UMI&lt;/code&gt;, &lt;code&gt;R2&lt;/code&gt; is the reads mapped to the transcriptome.&lt;/p&gt;

&lt;p&gt;Feature barcode whitelist can be found at the cellranger installation path:  &lt;code&gt;cellranger-2.1.0/cellranger-cs/2.1.0/lib/python/cellranger/barcodes/737K-august-2016.txt&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;single-cell-atac&#34;&gt;single cell ATAC&lt;/h3&gt;

&lt;p&gt;after &lt;code&gt;cellranger-atac mkfastq&lt;/code&gt;, there four &lt;code&gt;fastq.gz&lt;/code&gt; files will be generated. &lt;code&gt;I1&lt;/code&gt;, &lt;code&gt;R1&lt;/code&gt;, &lt;code&gt;R2&lt;/code&gt; and &lt;code&gt;R3&lt;/code&gt;.
&lt;code&gt;I1&lt;/code&gt; is the 8 bp sample barcode, &lt;code&gt;R1&lt;/code&gt; is the forward read, &lt;code&gt;R2&lt;/code&gt; is the 16 bp &lt;code&gt;10x feature barcode&lt;/code&gt; and &lt;code&gt;R3&lt;/code&gt; is the reverse read. Thanks &lt;a href=&#34;https://twitter.com/Itti_Q&#34; target=&#34;_blank&#34;&gt;Aditi Qamra&lt;/a&gt; for pointing it out.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/posts_img/atac_library.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The 16bp feature barcode whitelist can be found at cellranger-atac installation path:&lt;code&gt;cellranger-atac-1.0.1/cellranger-atac-cs/1.0.1/lib/python/barcodes/737K-cratac-v1.txt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that I have put the sample barcodes and feature barcodes files at &lt;a href=&#34;https://osf.io/2z9gj/files/&#34; target=&#34;_blank&#34;&gt;https://osf.io/2z9gj/files/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;are-the-feature-barcode-whitelist-the-same-for-scrnaseq-and-scatac&#34;&gt;Are the feature barcode whitelist the same for scRNAseq and scATAC?&lt;/h3&gt;

&lt;p&gt;In theory, 16bp barcode can have &lt;code&gt;4^16&lt;/code&gt; (4,294,967,296) combinations, but you will want some diversities of the sequences to better distinguish 2 barcodes.Keep in mind that there are PCR or sequencing errors for the barcodes.&lt;/p&gt;

&lt;p&gt;Both are 737K, but are the sequences the same?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat 737K-august-2016.txt | sort | uniq &amp;gt; scRNAseq_barcode.txt
cat 737K-cratac-v1.txt | sort | uniq &amp;gt; scATAC_barcode.txt

wc -l scRNAseq_barcode.txt
737280 scRNAseq_barcode.txt

wc -l scATAC_barcode.txt
737280 scATAC_barcode.txt

## only 10812 cell barcodes are common  
comm -12 scRNAseq_barcode.txt scATAC_barcode.txt  | wc -l
10812
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, 10x uses quite different cell barcodes for scRNAseq and scATACseq applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluating single cell RNAseq cluster stability</title>
      <link>/project/evaluating-scrnaseq-cluster/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 -0500</pubDate>
      
      <guid>/project/evaluating-scrnaseq-cluster/</guid>
      <description>&lt;p&gt;The goal of scclusteval(Single Cell Cluster Evaluation) is to evaluate the single cell clustering stability by boostrapping/subsampling the cells and provide many visualization methods for comparing clusters.&lt;/p&gt;

&lt;p&gt;for Theory behind the method, see Christian Henning, “Cluster-wise assessment of cluster stability,” Research Report 271, Dept. of Statistical Science, University College London, December 2006)&lt;/p&gt;

&lt;p&gt;You can find the package at my &lt;a href=&#34;https://github.com/crazyhottommy/scclusteval&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/raincloud_cluster.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to make a transcript to gene mapping file</title>
      <link>/post/how-to-make-a-transcript-to-gene-mapping-file/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-make-a-transcript-to-gene-mapping-file/</guid>
      <description>&lt;p&gt;I need a transcript to gene mapping file for &lt;code&gt;Salmon&lt;/code&gt;. I am aware of annotation &lt;code&gt;bioconductor&lt;/code&gt; packages that can do this job. However, I was working on a species which does not have the annotation in a package format (I am going to use Drosphila as an example for this blog post). I had to go and got the gtf file and made such a file from scratch.&lt;/p&gt;
&lt;p&gt;Please read the &lt;a href=&#34;https://useast.ensembl.org/info/website/upload/gff.html&#34;&gt;specifications&lt;/a&gt; of those two file formats.&lt;/p&gt;
&lt;div id=&#34;download-drosophila-gtf-file-from-ensemble-and-gff-file-from-ncbi&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Download drosophila gtf file from ENSEMBLE and gff file from NCBI&lt;/h3&gt;
&lt;p&gt;Find the &lt;code&gt;gff&lt;/code&gt; file at &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/genome/?term=drosophila+melanogaster&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/genome/?term=drosophila+melanogaster&lt;/a&gt;&lt;br /&gt;
Find the &lt;code&gt;gtf&lt;/code&gt; file at &lt;a href=&#34;ftp://ftp.ensembl.org/pub/release-95/gtf/drosophila_melanogaster/&#34; class=&#34;uri&#34;&gt;ftp://ftp.ensembl.org/pub/release-95/gtf/drosophila_melanogaster/&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;#gtf file
zless -S ~/Downloads/Drosophila_melanogaster.BDGP6.95.gtf.gz | grep -v &amp;quot;#&amp;quot; | cut -f3 | sort | uniq -c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 160859 CDS
##    4 Selenocysteine
## 187373 exon
## 46299 five_prime_utr
## 17737 gene
## 30492 start_codon
## 33892 three_prime_utr
## 34767 transcript&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;#gff file
zless -S ~/Downloads/GCF_000001215.4_Release_6_plus_ISO1_MT_genomic.gff.gz| grep -v &amp;quot;#&amp;quot; | cut -f3 | sort | uniq -c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 160949 CDS
##    1 RNase_MRP_RNA
##    2 RNase_P_RNA
##    2 SRP_RNA
##  584 antisense_RNA
## 187809 exon
## 17421 gene
## 2275 lnc_RNA
## 30480 mRNA
##  479 miRNA
## 5416 mobile_genetic_element
##   77 ncRNA
##  263 primary_transcript
##  308 pseudogene
##  134 rRNA
## 1870 region
##    1 sequence_feature
##   32 snRNA
##  289 snoRNA
##  319 tRNA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;use-unix-command-to-make-a-transcripts-to-gene-mapping-file-from-gtf-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use unix command to make a transcripts to gene mapping file from gtf file&lt;/h3&gt;
&lt;p&gt;We see the feature types are quite different although they are both annotation files for the same species.
The &lt;code&gt;gtf&lt;/code&gt; file is relatively well formatted, and we can make a transcripts to gene mapping file easily using
unix command line.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;zless -S ~/Downloads/Drosophila_melanogaster.BDGP6.95.gtf.gz | grep -v &amp;quot;#&amp;quot; | awk &amp;#39;$3==&amp;quot;transcript&amp;quot;&amp;#39; | cut -f9 | tr -s &amp;quot;;&amp;quot; &amp;quot; &amp;quot; | awk &amp;#39;{print$4&amp;quot;\t&amp;quot;$2}&amp;#39; | sort | uniq |  sed &amp;#39;s/\&amp;quot;//g&amp;#39; | tee tx2gene_ensemble.tsv| head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## FBgn0013687  FBgn0013687
## FBtr0005088  FBgn0260439
## FBtr0006151  FBgn0000056
## FBtr0070000  FBgn0031081
## FBtr0070001  FBgn0052826
## FBtr0070002  FBgn0031085
## FBtr0070003  FBgn0062565
## FBtr0070006  FBgn0031089
## FBtr0070007  FBgn0031092
## FBtr0070008  FBgn0031094&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;hmm…why the first line has both genes in the two columns?…
sanity check:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;zless -S ~/Downloads/Drosophila_melanogaster.BDGP6.95.gtf.gz | grep &amp;quot;FBgn0013687&amp;quot; | less -S&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## mitochondrion_genome FlyBase gene    14917   19524   .   +   .   gene_id &amp;quot;FBgn0013687&amp;quot;; gene_name &amp;quot;mt:ori&amp;quot;; gene_source &amp;quot;FlyBase&amp;quot;; gene_biotype &amp;quot;pseudogene&amp;quot;;
## mitochondrion_genome FlyBase transcript  14917   19524   .   +   .   gene_id &amp;quot;FBgn0013687&amp;quot;; transcript_id &amp;quot;FBgn0013687&amp;quot;; gene_name &amp;quot;mt:ori&amp;quot;; gene_source &amp;quot;FlyBase&amp;quot;; gene_biotype &amp;quot;pseudogene&amp;quot;; transcript_source &amp;quot;FlyBase&amp;quot;; transcript_biotype &amp;quot;pseudogene&amp;quot;;
## mitochondrion_genome FlyBase exon    14917   19524   .   +   .   gene_id &amp;quot;FBgn0013687&amp;quot;; transcript_id &amp;quot;FBgn0013687&amp;quot;; exon_number &amp;quot;1&amp;quot;; gene_name &amp;quot;mt:ori&amp;quot;; gene_source &amp;quot;FlyBase&amp;quot;; gene_biotype &amp;quot;pseudogene&amp;quot;; transcript_source &amp;quot;FlyBase&amp;quot;; transcript_biotype &amp;quot;pseudogene&amp;quot;; exon_id &amp;quot;FBgn0013687-E1&amp;quot;;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed it is in the original gtf file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-gffutilsto-make-a-transcripts-to-gene-mapping-file-from-gff-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use &lt;code&gt;gffutils&lt;/code&gt;to make a transcripts to gene mapping file from gff file&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;gff&lt;/code&gt; file is not that well defined. One may still be able to use some unix tricks to get the tx2gene.tsv file from a gff file, but it can be rather awkward especially for gff files from other not well annotated species. Instead, let’s use &lt;code&gt;gffutils&lt;/code&gt;, a python package to do the same.&lt;/p&gt;
&lt;p&gt;install &lt;code&gt;gffutils&lt;/code&gt; in terminal:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;source activate snakemake&lt;/code&gt;&lt;br /&gt;
&lt;code&gt;conda install gffutils&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note, I am running python through Rsutdio/ First read how to set python path for &lt;code&gt;reticulate&lt;/code&gt; at &lt;a href=&#34;https://rstudio.github.io/reticulate/articles/versions.html&#34; class=&#34;uri&#34;&gt;https://rstudio.github.io/reticulate/articles/versions.html&lt;/a&gt;
read more on &lt;a href=&#34;https://cran.r-project.org/web/packages/reticulate/vignettes/versions.html&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/reticulate/vignettes/versions.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Somehow, I have to create a &lt;code&gt;.Rprofile&lt;/code&gt; in the same folder of &lt;code&gt;.Rproj&lt;/code&gt; file with the following line to use my snakemake conda environment which is python3:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Sys.setenv(PATH = paste(&amp;quot;/anaconda3/envs/snakemake/bin/&amp;quot;, Sys.getenv(&amp;quot;PATH&amp;quot;), sep=&amp;quot;:&amp;quot;))&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reticulate)

# check which python I am using
py_discover_config()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## python:         /anaconda3/envs/snakemake/bin//python
## libpython:      /anaconda3/envs/snakemake/lib/libpython3.6m.dylib
## pythonhome:     /anaconda3/envs/snakemake:/anaconda3/envs/snakemake
## version:        3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 14:01:38)  [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
## numpy:          /anaconda3/envs/snakemake/lib/python3.6/site-packages/numpy
## numpy_version:  1.15.3
## 
## python versions found: 
##  /anaconda3/envs/snakemake/bin//python
##  /usr/bin/python
##  /anaconda3/envs/py27/bin/python
##  /anaconda3/envs/snakemake/bin/python&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# these did not work for me...
# use_condaenv(&amp;quot;snakemake&amp;quot;, required = TRUE)
# use_python(&amp;quot;/anaconda3/envs/snakemake/bin/python&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import sys
print(sys.version)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 14:05:31) 
## [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import gffutils
import itertools
import os
os.listdir()
db = gffutils.create_db(&amp;quot;GCF_000001215.4_Release_6_plus_ISO1_MT_genomic.gff.gz&amp;quot;, &amp;quot;:memory:&amp;quot;, force = True,merge_strategy=&amp;quot;merge&amp;quot;, id_spec={&amp;#39;gene&amp;#39;: &amp;#39;Dbxref&amp;#39;})
list(db.featuretypes())
# one can do it for one type of features, say mRNA
for mRNA in itertools.islice(db.features_of_type(&amp;#39;mRNA&amp;#39;), 10):
        print(mRNA[&amp;#39;transcript_id&amp;#39;][0], mRNA[&amp;#39;gene&amp;#39;][0])
        #print(mRNA.attributes.items())
        
## but I then have to do the same for lnc_RNA and others.        
## instead, loop over all features in the database&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NM_001103384.3 CG17636
## NM_001258513.2 CG17636
## NM_001258512.2 CG17636
## NM_001297796.1 RhoGAP1A
## NM_001297795.1 RhoGAP1A
## NM_001103385.2 RhoGAP1A
## NM_001103386.2 RhoGAP1A
## NM_001169155.1 RhoGAP1A
## NM_001297797.1 RhoGAP1A
## NM_001297801.1 tyn&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;tx_and_gene=[]
with open(&amp;quot;tx2gene_NCBI.tsv&amp;quot;, &amp;quot;w&amp;quot;) as f:
        for feature in db.all_features():
                transcript = feature.attributes.get(&amp;#39;transcript_id&amp;#39;, [None])[0]
                gene = feature.attributes.get(&amp;#39;gene&amp;#39;, [None])[0]
                if gene and transcript and ([transcript, gene] not in tx_and_gene):
                        tx_and_gene.append([transcript, gene])
                        f.write(transcript + &amp;quot;\t&amp;quot; + gene + &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These lines of codes are not hard to write. It takes more time to read the package documentation and understand how to use the package. One problem with bioinFORMATics is that there are so many different file formats. To make things worse, even for gff file format, many files do not follow the exact specification. You can have a taste of that at &lt;a href=&#34;http://daler.github.io/gffutils/examples.html&#34; class=&#34;uri&#34;&gt;http://daler.github.io/gffutils/examples.html&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding p value, multiple comparisons, FDR and q value</title>
      <link>/post/understanding-p-value-multiple-comparisons-fdr-and-q-value/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/understanding-p-value-multiple-comparisons-fdr-and-q-value/</guid>
      <description>&lt;p&gt;UPDATE 01/29/2019.
Read this awesome paper &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877414/&#34;&gt;Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This was an &lt;a href=&#34;http://crazyhottommy.blogspot.com/2015/03/understanding-p-value-multiple.html&#34;&gt;old post&lt;/a&gt; I wrote 3 years ago after I took HarvardX: &lt;a href=&#34;https://courses.edx.org/courses/course-v1:HarvardX+PH525.3x+1T2018/0b42cffa7c6e4c559bf74f93fb864a59/&#34;&gt;PH525.3x Advanced Statistics for the Life Sciences on edx&lt;/a&gt; taught by &lt;a href=&#34;http://rafalab.github.io/&#34;&gt;Rafael Irizarry&lt;/a&gt;. It is still one of the best courses to get you started using R for genomics. I am very thankful to have those high quality classes available to me when I started to learn. I am reposting it here using blogdown to give myself a refresh.&lt;/p&gt;
&lt;p&gt;I am writing this post for my own later references. Deep understanding of p-value, FDR and q-value is not trivial, and many biologists are misusing and/or misinterpreting them. Please also read this Nature Biotech primer &lt;a href=&#34;https://www.nature.com/articles/nbt1209-1135&#34;&gt;How does multiple testing correction work?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For biologists’ sake, I will use an example of gene expression. Suppose we have two groups of cells: control and treatment (can be anything like chemical treatment, radiation treatment etc..). We are looking if Gene A is deferentially expressed or not under treatment. Each group we have 12 replicates.&lt;/p&gt;
&lt;p&gt;What we usually do is take the average of 12 replicates of each group and do a t-test to compare if the difference is significant or not (assume normal distribution). We then get a p-value, say p = 0.035. We know it is smaller than 0.05 (a threshold we set), and we conclude that after treatment, expression of Gene A is significantly changed. However, what does it mean by saying a p value of 0.035?&lt;/p&gt;
&lt;p&gt;Everything starts with a null hypothesis:&lt;br /&gt;
&lt;strong&gt;H0 : There are no difference of gene expression for Gene A after treatment.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;and an alternative hypothesis:&lt;br /&gt;
&lt;strong&gt;H1: After treatment, expression of Gene A changes.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The definition of every P value begins by assuming a null hypothesis is True. &lt;span class=&#34;citation&#34;&gt;Motulsky (&lt;a href=&#34;#ref-motulsky2014intuitive&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; Third edition page 127. With a p-value of 0.035, it means that under the Null, the probability that we see the difference of gene expression after treatment is 0.035, which is very low. If we choose a significant level of alpha=0.05, we then reject the Null hypothesis and accept the alternative hypothesis. So, if you can not state what the null hypothesis is, you can not understand the P value. &lt;span class=&#34;citation&#34;&gt;Motulsky (&lt;a href=&#34;#ref-motulsky2014intuitive&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; Third edition page 127.&lt;/p&gt;
&lt;p&gt;For a typical genomic study, there are thousands of genes we want to compare. How do we report the gene list containing the genes that are differentially expressed? We can perform a-test for each single gene and if the p-value is smaller than 0.05, we report it. However, it will give us a lot of false positives because we did not consider multiple tests.&lt;/p&gt;
&lt;p&gt;Let’s start using a microarray data set in which thousands of genes are assayed at the same time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### This part is from the Edx online Harvard course 
## HarvardX: PH525.3x Advanced Statistics for the Life Sciences, week1

library(devtools)
library(qvalue)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;qvalue&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install_github(&amp;quot;genomicsclass/GSE5859Subset&amp;quot;)

library(GSE5859Subset)
data(GSE5859Subset)
dim(geneExpression)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8793   24&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Have a look at the data and objects available&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;geneExpression[1:6, 1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           GSM136508.CEL.gz GSM136530.CEL.gz GSM136517.CEL.gz
## 1007_s_at         6.543954         6.401470         6.298943
## 1053_at           7.546708         7.263547         7.201699
## 117_at            5.402622         5.050546         5.024917
## 121_at            7.892544         7.707754         7.461886
## 1255_g_at         3.242779         3.222804         3.185605
## 1294_at           7.531754         7.090270         7.466018
##           GSM136576.CEL.gz GSM136566.CEL.gz GSM136574.CEL.gz
## 1007_s_at         6.837899         6.470689         6.450220
## 1053_at           7.052761         6.980207         7.096195
## 117_at            5.304313         5.214149         5.173731
## 121_at            7.558130         7.819013         7.641136
## 1255_g_at         3.195363         3.251915         3.324934
## 1294_at           7.122145         7.058973         6.992396&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(sampleInfo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 24  4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(sampleInfo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     ethnicity       date         filename group
## 107       ASN 2005-06-23 GSM136508.CEL.gz     1
## 122       ASN 2005-06-27 GSM136530.CEL.gz     1
## 113       ASN 2005-06-27 GSM136517.CEL.gz     1
## 163       ASN 2005-10-28 GSM136576.CEL.gz     1
## 153       ASN 2005-10-07 GSM136566.CEL.gz     1
## 161       ASN 2005-10-07 GSM136574.CEL.gz     1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sampleInfo$filename&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;GSM136508.CEL.gz&amp;quot; &amp;quot;GSM136530.CEL.gz&amp;quot; &amp;quot;GSM136517.CEL.gz&amp;quot;
##  [4] &amp;quot;GSM136576.CEL.gz&amp;quot; &amp;quot;GSM136566.CEL.gz&amp;quot; &amp;quot;GSM136574.CEL.gz&amp;quot;
##  [7] &amp;quot;GSM136575.CEL.gz&amp;quot; &amp;quot;GSM136569.CEL.gz&amp;quot; &amp;quot;GSM136568.CEL.gz&amp;quot;
## [10] &amp;quot;GSM136559.CEL.gz&amp;quot; &amp;quot;GSM136565.CEL.gz&amp;quot; &amp;quot;GSM136573.CEL.gz&amp;quot;
## [13] &amp;quot;GSM136523.CEL.gz&amp;quot; &amp;quot;GSM136509.CEL.gz&amp;quot; &amp;quot;GSM136727.CEL.gz&amp;quot;
## [16] &amp;quot;GSM136510.CEL.gz&amp;quot; &amp;quot;GSM136515.CEL.gz&amp;quot; &amp;quot;GSM136522.CEL.gz&amp;quot;
## [19] &amp;quot;GSM136507.CEL.gz&amp;quot; &amp;quot;GSM136524.CEL.gz&amp;quot; &amp;quot;GSM136514.CEL.gz&amp;quot;
## [22] &amp;quot;GSM136563.CEL.gz&amp;quot; &amp;quot;GSM136564.CEL.gz&amp;quot; &amp;quot;GSM136572.CEL.gz&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(geneAnnotation)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      PROBEID  CHR     CHRLOC SYMBOL
## 1  1007_s_at chr6   30852327   DDR1
## 30   1053_at chr7  -73645832   RFC2
## 31    117_at chr1  161494036  HSPA6
## 32    121_at chr2 -113973574   PAX8
## 33 1255_g_at chr6   42123144 GUCA1A
## 34   1294_at chr3  -49842638   UBA7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;let’s look at one single gene&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g&amp;lt;- sampleInfo$group

e&amp;lt;- geneExpression[25,]

# t-test, expression should be normal distribution
qqnorm(e[g==1])
qqline(e[g==1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqnorm(e[g==0])
qqline(e[g==1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# perform t-test
t.test(e[g==1], e[g==0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  e[g == 1] and e[g == 0]
## t = 0.28382, df = 21.217, p-value = 0.7793
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.1431452  0.1884244
## sample estimates:
## mean of x mean of y 
##  10.52505  10.50241&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;do t-test for all the genes&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mytest&amp;lt;- function(x) t.test(x[g==1], x[g==0], var.equal=T)$p.value

## or we can use the genefilter package from bioconductor
## library(genefilter)
## results&amp;lt;- rowttests(geneExpression, factor(g))

pvals&amp;lt;- apply(geneExpression, 1, mytest)

sum(pvals&amp;lt; 0.05)  # how many pvalues are smaller than 0.05&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1383&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;have a look at the p-value distribution&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# there are 1383 genes with p value smaller than 0.05
# are all of them statistically different?
hist(pvals)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;simulate-multiple-comparisons-with-random-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;simulate multiple comparisons with random data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m&amp;lt;- nrow(geneExpression)
n&amp;lt;- ncol(geneExpression)

# generate random numbers
randomData&amp;lt;- matrix(rnorm(n*m), m, n)
nullpvalues&amp;lt;- apply(randomData, 1, mytest)
hist(nullpvalues)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;compare this histogram with the histogram above. what do you see?
Even if we randomly generated the data, you still see some pvalues are smaller than 0.05!! We randomly generated data, there should be no genes that deferentially expressed. However, we see a flat line across different p values.&lt;/p&gt;
&lt;p&gt;p values are random variables. Mathematically, one can &lt;a href=&#34;https://joyeuserrance.wordpress.com/2011/04/22/proof-that-p-values-under-the-null-are-uniformly-distributed/&#34;&gt;demonstrate&lt;/a&gt; that under the null hypothesis (and some assumptions are met, in this case, the test statistic T follows standard normal distribution), p-values follow a uniform (0,1) distribution, which means that P(p &amp;lt; p1) = p1. This means that the probability see a p value smaller than p1 is equal to p1. That being said, with a 100 t-tests, under the null (no difference between control and treatment), we will see 1 test with a p value smaller than 0.01. And we will see 2 tests with a p value smaller than 0.02 etc…
This explains why we see some p-values are smaller than 0.05 in our randomly generated numbers.&lt;/p&gt;
&lt;p&gt;In fact, checking the p-value distribution by histogram is a very important step during data analysis.
You may want to read a blog post by David Robinson &lt;a href=&#34;http://varianceexplained.org/statistics/interpreting-pvalue-histogram/&#34;&gt;How to interpret a p-value histogram&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-do-we-control-the-false-positives-for-multiple-comparisons&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How do we control the false positives for multiple comparisons?&lt;/h3&gt;
&lt;p&gt;One way is to use the Bonferroni correction to correct the familywise error rate (FWER):
define a particular comparison as statistically significant only when the P value is less than alpha(often 0.05) divided by the number of comparisons (p &amp;lt; alpha/m) &lt;span class=&#34;citation&#34;&gt;Motulsky (&lt;a href=&#34;#ref-motulsky2014intuitive&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; Third edition page 187. Say we computed 100 t-tests, and got 100 p values, we only consider the genes with a p value smaller than 0.05/100 as significant. This approach is very conservative and is used in Genome-wide association studies (GWAS). Since we often compare millions of genetic variations between (tens of thousands) cases and controls, this threshold will be very small! &lt;span class=&#34;citation&#34;&gt;Motulsky (&lt;a href=&#34;#ref-motulsky2014intuitive&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; Third edition page 188.&lt;/p&gt;
&lt;p&gt;Alternatively, we can use False Discovery Rate (FDR) to report the gene list.
&lt;strong&gt;FDR = #false positives/# called significant.&lt;/strong&gt;&lt;br /&gt;
This approach does not use the term statistically significant but instead use the term discovery.
Let’s control FDR for a gene list with &lt;code&gt;FDR = 0.05&lt;/code&gt;.
&lt;strong&gt;It means that of all the discoveries, 5% of them is expected to be false positives.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Benjamini &amp;amp; Hochberg (BH method) in 1995 proposed a way to control FDR:
Let k be the largest i such that &lt;code&gt;p(i) &amp;lt;= (i/m) * alpha&lt;/code&gt;, (m is the number of comparisons)
then reject H(i) for i =1, 2, …k&lt;/p&gt;
&lt;p&gt;This process controls the FDR at level alpha. The method sets a different threshold p value for each comparison. Say we computed 100 t-tests, and got 100 p values, and we want to control the FDR =0.05. We then rank the p values from small to big.
if p(1) &amp;lt;= 1/100 * 0.05, we then reject null hypothesis and accept the alternative.
if p(2) &amp;lt; = 2/100 * 0.05, we then reject the null and accept the alternative..
…..&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## order the pvals computed above and plot it.
alpha = 0.05
m = length(pvals)
#m is the number of 8793 comparisons 

plot(x=seq(1,100), y=pvals[order(pvals)][1:100])
abline(a=0, b=alpha/m)
title(&amp;quot;slop is alpha/m&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# let&amp;#39;s zoom in to look at the first 15 p values from small to big

plot(x=seq(1,100), y=pvals[order(pvals)][1:100], xlim=c(1,15))
abline(a=0, b=alpha/m)
title(&amp;quot;slop is alpha/m&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-13-understanding-p-value-multiple-comparisons-fdr-and-q-value_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we can see that the 14th p value is bigger than its own threshold 
# which is computed by (0.05/m) * 14 = 7.960878e-05

# we will use p.adjust function and the method &amp;quot;fdr&amp;quot; or &amp;quot;BH&amp;quot; to
# correct the p value, what the p.adjust function does to to
# recalculate the p-value. ?p.adjust to see more
# p(i)&amp;lt;= (i/m) * alpha 
# p(i) * m/i &amp;lt;= alpha
# we can then only accept the returned if p.adjust(pvals) &amp;lt;= alpha
# number of p values smaller than their own thresholds after controlling FDR=0.05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we can see that the 14th p value is bigger than its own threshold ,which is computed by (0.05/m) * 14 = 7.960878e-05
we will use p.adjust function and the method “fdr” or “BH” to correct the p value, what the p.adjust function does is to recalculate the p-values.
p(i)&amp;lt;= (i/m) * alpha
p(i) * m/i &amp;lt;= alpha
we can then only accept the returned the p values if p.adjust(pvals) &amp;lt;= alpha&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum( p.adjust(pvals, method=&amp;quot;fdr&amp;quot;) &amp;lt; 0.05 )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;it is 13, the same as we saw from the figure.&lt;/p&gt;
&lt;p&gt;Another method by Storey in 2002 is the direct approach to FDR:
Let K be the largest i such that pi_0 * p(i) &amp;lt; (i/m) * alpha
then reject H(i) for i =1,2,…k
pi_0 is the estimate of the proportion of null hypothesis in the gene list is true, range from 0 to 1.
so when pi_0 is 1, then we have the Benjamini &amp;amp; Hochberg correction.
This method is less conservative than the BH method.
Use the qvalue function in the bioconductor package “qvalue”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum( qvalue(pvals)$qvalues &amp;lt; 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 22&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;it is 22, less conservative than the BH method.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note that FDR is a property of a list of genes. q value is defined for a specific gene:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;HarvardX: PH525.3x Advanced Statistics for the Life Sciences, week1, video lecture for FDR.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“But if you do want to assign a number to each gene, a simple thing you can do, is you can go gene by gene, and decide what would be the smallest FDR I would consider, that would include this gene in the list. And once you do that, then you have defined a q-value. And this is something that is very often reported in the list of genes”[4]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;HarvardX: PH525.3x Advanced Statistics for the Life Sciences, week1, quiz for FDR:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“To define the q-value we order features we tested by p-value then compute the FDRs for a list with the most significant, the two most significant, the three most significant, etc… The FDR of the list with the, say, m most significant tests is defined as the q-value of the m-th most significant feature. In other words, the q-value of a feature, is the FDR of the biggest list that includes that gene” [5]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I hope this post helps you better understand p values, FDR and q values. Sadly, many biologists do not understand them well and try to do p-hacking.&lt;/p&gt;
&lt;p&gt;Further read &lt;a href=&#34;https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106&#34;&gt;The Extent and Consequences of P-Hacking in Science&lt;/a&gt; and &lt;a href=&#34;https://www.thermofisher.com/blog/proteomics/whats-true-whats-false-proteostats-and-the-fdr/&#34;&gt;What’s True? What’s False? ProteoStats and the FDR&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-motulsky2014intuitive&#34;&gt;
&lt;p&gt;Motulsky, Harvey. 2014. &lt;em&gt;Intuitive Biostatistics: A Nonmathematical Guide to Statistical Thinking&lt;/em&gt;. Oxford University Press, USA.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>permutation test for PCA components</title>
      <link>/post/permute-test-for-pca-components/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/permute-test-for-pca-components/</guid>
      <description>&lt;p&gt;PCA is a critical method for dimension reduction for high-dimensional data.
High-dimensional data are data with features (p) a lot more than observations (n).
However, this is changing with single-cell RNAseq data. Now, we can sequence millions (n)
of single cells and each cell has ~20,000 genes/features (p).&lt;/p&gt;
&lt;p&gt;I suggest you read my &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/post/pca-in-action/&#34;&gt;previous blog post&lt;/a&gt; on using &lt;code&gt;svd&lt;/code&gt; to calculate PCs.&lt;/p&gt;
&lt;div id=&#34;single-cell-expression-data-pca&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Single-cell expression data PCA&lt;/h3&gt;
&lt;p&gt;In single-cell RNAseq analysis, feature selection will be performed first. e.g. In &lt;a href=&#34;https://github.com/satijalab/seurat&#34;&gt;&lt;code&gt;Seruat&lt;/code&gt;&lt;/a&gt;, most variable genes will be calculated by &lt;code&gt;FindVariableGenes&lt;/code&gt; and will be used for downstream analysis. The number of variable genes is in
the range of a couple of thousands (~2000). This further reduced number of features(p).&lt;/p&gt;
&lt;p&gt;Let’s take a look at the &lt;a href=&#34;https://github.com/satijalab/seurat/blob/master/R/dimensional_reduction.R#L70&#34;&gt;source code of &lt;code&gt;Seurat&lt;/code&gt;&lt;/a&gt; for
PCA:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if (rev.pca) {
    pcs.compute &amp;lt;- min(pcs.compute, ncol(x = data.use)-1)
    pca.results &amp;lt;- irlba(A = data.use, nv = pcs.compute, ...)
    sdev &amp;lt;- pca.results$d/sqrt(max(1, nrow(data.use) - 1))
    if(weight.by.var){
      gene.loadings &amp;lt;- pca.results$u %*% diag(pca.results$d)
    } else{
      gene.loadings &amp;lt;- pca.results$u
    }
    cell.embeddings &amp;lt;- pca.results$v
  }
  else {
    pcs.compute &amp;lt;- min(pcs.compute, nrow(x = data.use)-1)
    pca.results &amp;lt;- irlba(A = t(x = data.use), nv = pcs.compute, ...)
    gene.loadings &amp;lt;- pca.results$v
    sdev &amp;lt;- pca.results$d/sqrt(max(1, ncol(data.use) - 1))
    if(weight.by.var){
      cell.embeddings &amp;lt;- pca.results$u %*% diag(pca.results$d)
    } else {
      cell.embeddings &amp;lt;- pca.results$u
    }
  }
  rownames(x = gene.loadings) &amp;lt;- rownames(x = data.use)
  colnames(x = gene.loadings) &amp;lt;- paste0(reduction.key, 1:pcs.compute)
  rownames(x = cell.embeddings) &amp;lt;- colnames(x = data.use)
  colnames(x = cell.embeddings) &amp;lt;- colnames(x = gene.loadings)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the help page for &lt;code&gt;{Seruat::RunPCA}&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pc.genes    
Genes to use as input for PCA. Default is object@var.genes

rev.pca 
By default computes the PCA on the cell x gene matrix. Setting to true will compute it on gene x cell matrix.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Seurat&lt;/code&gt; uses &lt;a href=&#34;https://cran.r-project.org/web/packages/irlba/index.html&#34;&gt;irlba&lt;/a&gt; (Fast Truncated Singular Value Decomposition and Principal Components Analysis for Large Dense and Sparse Matrices) for PCA.
The &lt;code&gt;irlba&lt;/code&gt; is both faster and more memory efficient than the usual R &lt;code&gt;svd&lt;/code&gt; function for computing a few of the largest singular vectors and corresponding singular values of a matrix.&lt;/p&gt;
&lt;p&gt;By default, &lt;code&gt;RunPCA&lt;/code&gt; computes the PCA on the &lt;code&gt;cell (n) x gene (p)&lt;/code&gt; matrix.
One thing to note is that in linear algebra, a matrix is coded as n (rows are observations) X p (columns are features). That’s why by default, the &lt;code&gt;gene x cell&lt;/code&gt; original matrix is transposed first to &lt;code&gt;cell x gene&lt;/code&gt;: &lt;code&gt;irlba(A = t(x = data.use), nv = pcs.compute, ...)&lt;/code&gt;.
After &lt;code&gt;irlba&lt;/code&gt;, the &lt;code&gt;v&lt;/code&gt; matrix is the gene loadings, the &lt;code&gt;u&lt;/code&gt; matrix is the cell embeddings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;number-of-significant-pcs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;number of significant PCs&lt;/h3&gt;
&lt;p&gt;For downstream analysis, e.g. &lt;code&gt;{Seurat::FindClusters}&lt;/code&gt; only the PCs that significantly contribute to the variation of the data are used. &lt;code&gt;Seruat&lt;/code&gt; uses &lt;code&gt;JackStraw&lt;/code&gt; and &lt;code&gt;JackStrawplot&lt;/code&gt; function to achieve it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;JackStraw&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Randomly permutes a subset of data, and calculates projected PCA scores for these ‘random’ genes. Then compares the PCA scores for the ‘random’ genes with the observed PCA scores to determine statistical significance. End result is a &lt;strong&gt;p-value for each gene’s association with each principal component&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;JackStrawplot&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Plots the results of the JackStraw analysis for PCA significance. For each PC, plots a QQ-plot comparing the distribution of p-values for all genes across each PC, compared with a uniform distribution. Also determines a p-value for the overall significance of each PC.The p-value for each PC is based on a proportion test comparing the number of genes with a p-value below a particular threshold (score.thresh), compared with the proportion of genes expected under a uniform distribution of p-values.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The other day, I saw a tweet on permute the original matrix to calculate the significance of the PCs.
I forget the original tweet, but this is from a retweet: &lt;a href=&#34;https://twitter.com/MattOldach/status/1075037756563382274&#34; class=&#34;uri&#34;&gt;https://twitter.com/MattOldach/status/1075037756563382274&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;references:
This is called Horn’s Parallel Analysis (original paper &lt;a href=&#34;https://link.springer.com/article/10.1007%2FBF02289447&#34; class=&#34;uri&#34;&gt;https://link.springer.com/article/10.1007%2FBF02289447&lt;/a&gt; and a modification &lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/0013164495055003002?journalCode=epma&#34; class=&#34;uri&#34;&gt;https://journals.sagepub.com/doi/abs/10.1177/0013164495055003002?journalCode=epma&lt;/a&gt;. It’s a great method for removing noisy components.&lt;/p&gt;
&lt;p&gt;This is not exactly the same as what &lt;code&gt;Seurat&lt;/code&gt; is doing, but the idea is similar.
I want to put it down here for my future reference.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;permutation-test-for-pca&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;permutation “test” for PCA&lt;/h3&gt;
&lt;p&gt;The code below is copied from that tweet, credit goes to the author.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_eigenperm&amp;lt;- function(data, nperm = 1000){
        pca_out&amp;lt;- prcomp(data, scale. = T)
        eigenperm&amp;lt;- data.frame(matrix(NA, nperm, ncol(data)))
        n&amp;lt;- ncol(data)
        data_i&amp;lt;- data.frame(matrix(NA, nrow(data), ncol(data)))
        for (j in 1: nperm){
        for (i in 1:n){
                data_i[,i]&amp;lt;- sample(data[,i], replace = F)
        }
        pca.perm&amp;lt;- prcomp(data_i, scale. = T)
        eigenperm[j,]&amp;lt;- pca.perm$sdev^2
        }
        colnames(eigenperm)&amp;lt;- colnames(pca_out$rotation)
        eigenperm
        
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use the same &lt;code&gt;NCI60&lt;/code&gt; data set for demonstration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ──────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.1.0     ✔ purrr   0.2.5
## ✔ tibble  1.4.2     ✔ dplyr   0.7.8
## ✔ tidyr   0.8.2     ✔ stringr 1.3.1
## ✔ readr   1.3.1     ✔ forcats 0.3.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ─────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

library(ISLR)

ncidat&amp;lt;- NCI60$data
rownames(ncidat)&amp;lt;- NCI60$labs

dim(ncidat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]   64 6830&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fa_pca_perm&amp;lt;- pca_eigenperm(t(ncidat))
fa_pca&amp;lt;- prcomp(t(ncidat))
fa_pca_rand95&amp;lt;- 
        data.frame(Random_Eigenvalues = sapply(fa_pca_perm, quantile, 0.95)) %&amp;gt;%
        #95% percentile of randome eigenvalues
        mutate(PC = colnames(fa_pca$rotation)) %&amp;gt;%
        #add PC IDs as discrete var
        cbind(Eigenvalues = fa_pca$sdev^2)
#combine rand95 with real eigenvals

## only the first 9 PCs
fa_pca_rand95_long&amp;lt;-
        gather(fa_pca_rand95[1:9, ], key = Variable, value = Value, -PC)

ggplot(fa_pca_rand95_long, aes(PC, Value, fill = Variable)) +
        geom_bar(stat = &amp;quot;identity&amp;quot;, position = position_dodge())+
        labs(y=&amp;quot;Eigenvalue&amp;quot;, x=&amp;quot;&amp;quot;, fill= &amp;quot;&amp;quot;) +
        theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-04-permutate-test-for-pca-components_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see after PC6, the Eigenvalues are almost the same with the permuted data.
For single cell data, permutation can take a long time, that’s why in &lt;code&gt;JackStraw&lt;/code&gt; there is an
option &lt;code&gt;prop.freq&lt;/code&gt; (Proportion of the data to randomly permute for each replicate) to
permute only a subset of the data matrix.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The end of 2018</title>
      <link>/post/the-end-of-2018/</link>
      <pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-end-of-2018/</guid>
      <description>

&lt;p&gt;It is almost the end of 2018. It is a good time to review what I have achieved during the year
and look forward to a brand new 2019. I wrote a similar post for 2017 &lt;a href=&#34;http://crazyhottommy.blogspot.com/2017/12/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;some-highlights-of-the-year-2018&#34;&gt;Some highlights of the year 2018:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;My son Noah Tang was born in April. He is so lovely and we love him so much. Can&amp;rsquo;t believe he is
almost 9 months old.
&lt;img src=&#34;/img/noah.jpg&#34; alt=&#34;&#34; /&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Our epigenomic project was selected by the &lt;a href=&#34;https://bigdatau.ini.usc.edu/roadtrip&#34; target=&#34;_blank&#34;&gt;Data Science Road-Trip program&lt;/a&gt; by USC. I spent 2 weeks in PNNL and worked closely with &lt;a href=&#34;https://www.pnnl.gov/science/staff/staff_info.asp?staff_num=8785&#34; target=&#34;_blank&#34;&gt;Lisa Bramer&lt;/a&gt; and developed a pipeline to do feature selection using machine learning from a lot of chromHMM data sets. You can find the github repo &lt;a href=&#34;https://github.com/crazyhottommy/pyflow-chromForest/tree/vsurf_merge&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. I kept a note for everyday what I did as well at &lt;a href=&#34;https://github.com/crazyhottommy/Epigenome_RoadTrip&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. I will think about writing it up.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I finally migrated my &lt;a href=&#34;http://crazyhottommy.blogspot.com/&#34; target=&#34;_blank&#34;&gt;previous blog&lt;/a&gt; to blogdown which you are reading now :) Oh my, I love it. It makes blogging so much fun.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I taught the ChIP-seq lesson for &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/talk/2018-dibsi-course/&#34; target=&#34;_blank&#34;&gt;2018 ANGUS Next-Gen Sequence Analysis Workshop&lt;/a&gt; held in UC Davis from 7/1/2018 to 7/14/2018, and TAed for the rest of the sessions. It was a great teaching experience for me. I got to know many people and built connections. Most importantly, I enjoyed the teaching very much! Thanks &lt;a href=&#34;https://biology.ucdavis.edu/people/c-titus-brown&#34; target=&#34;_blank&#34;&gt;Titus Brown&lt;/a&gt; for the invitation. I highly recommend you to attend this workshop if you want to start learning sequencing data analysis. The environment is so welcoming and Titus is so hilarious:) I was in the workshop to learn in 2014 and now I am back to teach!&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I went back to University of Florida where I did my PhD to give a &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/talk/2018-uf-talk/&#34; target=&#34;_blank&#34;&gt;talk&lt;/a&gt;. It was very nice to be back home and catch up with my supervisor, other professors and some church friends!&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Several co-author &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/&#34; target=&#34;_blank&#34;&gt;papers/see the publications section&lt;/a&gt; are out in 2018. My first video shot for JOVE can be found at &lt;a href=&#34;https://www.jove.com/video/56972/an-integrated-platform-for-genome-wide-mapping-chromatin-states-using&#34; target=&#34;_blank&#34;&gt;https://www.jove.com/video/56972/an-integrated-platform-for-genome-wide-mapping-chromatin-states-using&lt;/a&gt;. I was nervous but It was fun.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In addition, two papers are out in Biorxiv at the end of 2018. I am co-first author in one of them. Both papers describe how epigenetic regulator KMT2D mediate tumor progression in melanoma and lung cancer, respectively. We are in the process submitting those papers to journals, but you can read more details at &lt;a href=&#34;https://www.biorxiv.org/content/early/2018/12/28/507202&#34; target=&#34;_blank&#34;&gt;https://www.biorxiv.org/content/early/2018/12/28/507202&lt;/a&gt; and &lt;a href=&#34;https://www.biorxiv.org/content/early/2018/12/27/507327&#34; target=&#34;_blank&#34;&gt;https://www.biorxiv.org/content/early/2018/12/27/507327&lt;/a&gt;.&lt;br /&gt;
The work was done in &lt;a href=&#34;http://railab.org/people.html&#34; target=&#34;_blank&#34;&gt;Kunal Rai&amp;rsquo;s lab&lt;/a&gt; where I had a chance to play with large amount of ChIP-seq data sets. My &lt;a href=&#34;https://divingintogeneticsandgenomics.rbind.io/project/snakemake-pipelines/&#34; target=&#34;_blank&#34;&gt;Snakemake pipeline&lt;/a&gt; is being used in the lab by others and has processed thousands of ChIP-seq data sets.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;My three &lt;a href=&#34;https://github.com/crazyhottommy&#34; target=&#34;_blank&#34;&gt;most stared github repos&lt;/a&gt; were cited in a paper &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fbioe.2018.00198/full&#34; target=&#34;_blank&#34;&gt;GitHub Statistics as a Measure of the Impact of Open-Source Bioinformatics Software&lt;/a&gt; by &lt;a href=&#34;https://medschool.vcu.edu/expertise/detail.html?id=mdozmorov&#34; target=&#34;_blank&#34;&gt;Mikhail G. Dozmorov&lt;/a&gt;. The table summarizing the popular github repos can be found at &lt;a href=&#34;https://github.com/mdozmorov/bioinformatics-impact/blob/master/tables/table_1.md&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. It&amp;rsquo;s over three years&amp;rsquo; cumulative work for those repos.  I am so glad that my notes are helpful for other researchers. I am always supportive for open science and believe sharing knowledge is the way to promote science progress.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I joined Harvard FAS Informatics as a bioinformatics scientist in October and started working on single-cell RNAseq with &lt;a href=&#34;https://www.dulaclab.com/&#34; target=&#34;_blank&#34;&gt;Dulac lab&lt;/a&gt; and will have a chance to play with other single molecule transcriptome data generated from &lt;a href=&#34;http://zhuang.harvard.edu/&#34; target=&#34;_blank&#34;&gt;Xiaowei Zhuang&amp;rsquo;s lab&lt;/a&gt;. I am really excited about learning more!&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I have my green card approved. This is important so I can work in the US without worrying about my visa status. Thanks to my previous postdoc adviser &lt;a href=&#34;https://www.jax.org/research-and-faculty/faculty/roel-verhaak&#34; target=&#34;_blank&#34;&gt;Roel Verhaak&lt;/a&gt;, &lt;a href=&#34;http://pinellolab.org/&#34; target=&#34;_blank&#34;&gt;Luca Pinello&lt;/a&gt;, &lt;a href=&#34;https://www.ialbert.me/&#34; target=&#34;_blank&#34;&gt;Istvan Albert&lt;/a&gt;,  &lt;a href=&#34;https://blogs.cornell.edu/sethupathylab/&#34; target=&#34;_blank&#34;&gt;Praveen Sethupathy&lt;/a&gt; and &lt;a href=&#34;https://medicine.iu.edu/faculty/14584/cheng-liang/&#34; target=&#34;_blank&#34;&gt;Liang Cheng&lt;/a&gt; for writing recommendation letters. I will need to visit Luca&amp;rsquo;s lab to say thanks personally, I have not had a chance to meet him after I moved to Harvard.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I finally got a chance to write my frist R packagefor single cell cluster stability testing. It is in github: &lt;a href=&#34;https://github.com/crazyhottommy/scclusteval&#34; target=&#34;_blank&#34;&gt;scclusteval&lt;/a&gt;. I implemented some functions for visualizing single cell data and evaluating cluster stability. I will make it public once I clean up a bit. I was so satisfied to have it installed by &lt;code&gt;devtools::install_github()&lt;/code&gt; and all functions and help pages are readily available as a package. I mean I am gradually transiting myself from an R user to R programmer. I know I still have a lot to learn, but this is exciting! By the way, I highly recommend the &lt;a href=&#34;https://github.com/r-lib/usethis&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;usethis&lt;/code&gt;&lt;/a&gt; package for writing R packages and read the &lt;a href=&#34;http://r-pkgs.had.co.nz/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;R pacakges book&lt;/code&gt;&lt;/a&gt; by Hadley Wickham. read &lt;a href=&#34;https://blog.methodsconsultants.com/posts/developing-r-packages-using-gitlab-ci-part-i/&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; and &lt;a href=&#34;https://www.hvitfeldt.me/blog/usethis-workflow-for-package-development/&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; blog post to get started on how to write a minimal functional R package.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-to-expect-in-2019&#34;&gt;What to expect in 2019&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;I will have a lot of opportunities to teach workshops on R, Unix and single-cell RNAseq at my current position.&lt;/li&gt;
&lt;li&gt;A lot to learn on neuroscience. I will audit classes taught by Catherine Dulac.&lt;/li&gt;
&lt;li&gt;I will guest lecture a few lessons for &lt;a href=&#34;https://canvas.harvard.edu/courses/39391&#34; target=&#34;_blank&#34;&gt;STAT 115: Introduction to Computational Biology and Bioinformatics&lt;/a&gt; taught by &lt;a href=&#34;http://liulab.dfci.harvard.edu/&#34; target=&#34;_blank&#34;&gt;Sheirly Liu&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I would love to learn more on deep learning and apply it to single cell data analysis. Currently, I am taking classes from Coursera.&lt;/li&gt;
&lt;li&gt;Attend several conferences. Would love to catch up with the twitter-verse in person.&lt;/li&gt;
&lt;li&gt;A few more papers to write. I have at least 2 first author papers to finish. It&amp;rsquo;s hanging there forever.&lt;/li&gt;
&lt;li&gt;Of course, spend time with the family.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A tale of two heatmap functions</title>
      <link>/post/a-tale-of-two-heatmap-functions/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-tale-of-two-heatmap-functions/</guid>
      <description>&lt;p&gt;You probably do not understand heatmap! Please read &lt;a href=&#34;http://www.opiniomics.org/you-probably-dont-understand-heatmaps/&#34;&gt;You probably don’t understand heatmaps by Mick Watson&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the blog post, Mick used &lt;code&gt;heatmap&lt;/code&gt; function in the &lt;code&gt;stats&lt;/code&gt; package, I will try to walk you through comparing &lt;code&gt;heatmap&lt;/code&gt;, and &lt;code&gt;heatmap.2&lt;/code&gt; from &lt;code&gt;gplots&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;Before I start, I want to quote this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The defaults of almost every heat map function in R does the hierarchical clustering first, then scales the rows then displays the image”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;see these two posts in biostar: &lt;a href=&#34;https://www.biostars.org/p/85527/&#34;&gt;post1&lt;/a&gt;&lt;br /&gt;
and &lt;a href=&#34;https://www.biostars.org/p/15285/&#34;&gt;post2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In other words, the &lt;code&gt;scale&lt;/code&gt; parameter inside the &lt;code&gt;heatmap&lt;/code&gt; functions only plays a role in displaying the colors, but does not involve clustering. This is critical to know! We will test to see if this hold true.&lt;/p&gt;
&lt;div id=&#34;heatmap-function-in-stats-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;heatmap function in stats package&lt;/h3&gt;
&lt;p&gt;Simulate the data. The example is exactly the same as in the Mick Watson’s blog post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stats)
library(gplots)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gplots&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:stats&amp;#39;:
## 
##     lowess&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h1 &amp;lt;- c(10,20,10,20,10,20,10,20)
h2 &amp;lt;- c(20,10,20,10,20,10,20,10)

l1 &amp;lt;- c(1,3,1,3,1,3,1,3)
l2 &amp;lt;- c(3,1,3,1,3,1,3,1)

mat &amp;lt;- rbind(h1,h2,l1,l2)

par(mfrow =c(1,1), mar=c(4,4,1,1))
plot(1:8,rep(0,8), ylim=c(0,35), pch=&amp;quot;&amp;quot;, xlab=&amp;quot;Time&amp;quot;, ylab=&amp;quot;Gene Expression&amp;quot;)

for (i in 1:nrow(mat)) {
lines(1:8,mat[i,], lwd=3, col=i)
}

legend(1,35,rownames(mat), 1:4, cex=0.7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this dummy example, we have four genes (l1, l2, h1, h2) that are measured in 8 time points.&lt;/p&gt;
&lt;p&gt;when we do clustering, we want to cluster l1 h1 together, and l2 h2 together as they have the same trend across the time points. However, you will notice that the scale of the expression levels of these four genes are different: with h1 and h2 are high, and l1 l2 are low.&lt;/p&gt;
&lt;p&gt;If we calculate the distance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#?dist to see other distance measures
dist(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           h1        h2        l1
## h2 28.284271                    
## l1 38.470768 40.496913          
## l2 40.496913 38.470768  5.656854&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## I will use the default for linkage method: complete
plot(hclust(dist(mat)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The default distance measure is &lt;a href=&#34;https://en.wikipedia.org/wiki/Euclidean_distance&#34;&gt;Eucledian distance&lt;/a&gt;, and you will see h1 and h2 are closer (28.284271), l1 and l2 are closer. This simply because how euclidean distance is defined.&lt;/p&gt;
&lt;p&gt;Let’s check the help for &lt;code&gt;?heatmap&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;scale character indicating if the values should be centered and scaled in either the row direction or the column direction, or none. The default is “row” if symm false, and “none” otherwise.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;symm
logical indicating if x should be treated symmetrically; can only be true when x is a square matrix.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;So the default is scale row inside the &lt;code&gt;heatmap&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## default, I will give parameters explicitly
heatmap(mat, Colv=NA, col=greenred(10), scale = &amp;quot;row&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We do see h1,h2 cluster together; l1 l2 cluster together. Inside heatmap function, the default distance measure is the same as default of &lt;code&gt;dist&lt;/code&gt;, the linkage method is the same as &lt;code&gt;hclust&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you read the heatmap carefully, you will find that h1,h2 are with large values, but they have the same red color as l1,l2. &lt;strong&gt;This confirms that heatmap does clustering first, and then scale the row for representing the color!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;How about if we turn off scale inside &lt;code&gt;heatmap&lt;/code&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap(mat, Colv = NA, col=greenred(10), scale = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You will see the &lt;strong&gt;clustering does not change, but the color changed!&lt;/strong&gt; l1 and l2 are all green now (small values)&lt;/p&gt;
&lt;p&gt;How about if we scale the genes before we feed into heatmap? scale works on columns, transpose for rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat.scaled&amp;lt;- t(scale(t(mat), center=TRUE, scale = TRUE))
mat.scaled&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]       [,2]       [,3]       [,4]       [,5]       [,6]
## h1 -0.9354143  0.9354143 -0.9354143  0.9354143 -0.9354143  0.9354143
## h2  0.9354143 -0.9354143  0.9354143 -0.9354143  0.9354143 -0.9354143
## l1 -0.9354143  0.9354143 -0.9354143  0.9354143 -0.9354143  0.9354143
## l2  0.9354143 -0.9354143  0.9354143 -0.9354143  0.9354143 -0.9354143
##          [,7]       [,8]
## h1 -0.9354143  0.9354143
## h2  0.9354143 -0.9354143
## l1 -0.9354143  0.9354143
## l2  0.9354143 -0.9354143
## attr(,&amp;quot;scaled:center&amp;quot;)
## h1 h2 l1 l2 
## 15 15  2  2 
## attr(,&amp;quot;scaled:scale&amp;quot;)
##       h1       h2       l1       l2 
## 5.345225 5.345225 1.069045 1.069045&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see how the distance change among genes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dist(mat.scaled)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          h1       h2       l1
## h2 5.291503                  
## l1 0.000000 5.291503         
## l2 5.291503 0.000000 5.291503&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclust(dist(mat.scaled)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Wow! h1 and l1 are clustered together; l2 and h2 are clustered together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap(mat.scaled, Colv = NA, col=greenred(10), scale = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I hope you understand now how scale the data &lt;strong&gt;before or after&lt;/strong&gt; can affect the looking of your heatmaps.&lt;/p&gt;
&lt;p&gt;If we do not scale the data beforehand, but we still want l1 and h1 cluster together; l2 and h2 cluster together, we can use a different distance measure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## correlation among the genes
cor(t(mat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    h1 h2 l1 l2
## h1  1 -1  1 -1
## h2 -1  1 -1  1
## l1  1 -1  1 -1
## l2 -1  1 -1  1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 1- correation to define the distance
1- cor(t(mat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    h1 h2 l1 l2
## h1  0  2  0  2
## h2  2  0  2  0
## l1  0  2  0  2
## l2  2  0  2  0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc &amp;lt;- hclust(as.dist(1-cor(t(mat))))
plot(hc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Indeed, h1 and l1 are together; h2 and l2 are together.&lt;/p&gt;
&lt;p&gt;Now, we plot the heatmap, but set scale = “none” inside heatmap&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap(mat, Colv = NA, Rowv=as.dendrogram(hc), col=greenred(10), scale = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Is this what you expect?! Yes, l1 and h2 are clustered together; l1 and h1 clustered together. but because the value range are different, you see l1 and l2 are green (small values); h1 and h2 are red (big values).&lt;/p&gt;
&lt;p&gt;The magic will happen if we set scale =“row” which is the default:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap(mat, Colv = NA, Rowv=as.dendrogram(hc), col=greenred(10), scale = &amp;quot;row&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I hope I have clarified a bit for the complications of heatmaps.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;heatmap.2-function-in-gplots-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;heatmap.2 function in gplots package&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;scale character indicating if the values should be centered and scaled in either the row direction or the column direction, or none. The default is “none”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;The default is none!!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Please also pay attention to the Color Key of the heatmap.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## defaults of heatmap.2, scale is none
heatmap.2(mat, trace = &amp;quot;none&amp;quot;, Colv= NA, dendrogram = &amp;quot;row&amp;quot;, scale = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## FACT of heatmap functions in R: it does clustering first and then use the scale argument (if set) to represent the data.
heatmap.2(mat, trace = &amp;quot;none&amp;quot;, Colv= NA, dendrogram = &amp;quot;row&amp;quot;, scale = &amp;quot;row&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;how about we scale the data explicitly first and use euclidean distance. works fine!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap.2(t(scale(t(mat), center=TRUE, scale=TRUE)), trace = &amp;quot;none&amp;quot;, Colv= NA, dendrogram = &amp;quot;row&amp;quot;, scale = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-1--corx-as-distance-and-do-not-scale-before-hand&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;use 1- cor(x) as distance and do not scale before hand&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap.2(mat, trace = &amp;quot;none&amp;quot;, 
          Colv= NA, dendrogram = &amp;quot;row&amp;quot;,
          scale = &amp;quot;none&amp;quot;,
          hclust=function(x) hclust(x, method=&amp;#39;complete&amp;#39;), distfun=function(x) as.dist(1-cor(t(x))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-1--corx-as-distance-and-do-not-scale-before-hand-but-use-scale-in-the-heatmap.2-function-to-represent-the-colors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;use 1- cor(x) as distance and do not scale before hand, but use scale in the heatmap.2 function to represent the colors&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap.2(mat, trace = &amp;quot;none&amp;quot;, 
          Colv= NA, dendrogram = &amp;quot;row&amp;quot;,
          scale = &amp;quot;row&amp;quot;,
          hclust=function(x) hclust(x, method=&amp;#39;complete&amp;#39;), distfun=function(x) as.dist(1-cor(t(x))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scale-before-hand-and-use-1--corx-as-distance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;scale before hand and use 1- cor(x) as distance&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heatmap.2(t(scale(t(mat), center=TRUE, scale=TRUE)), trace = &amp;quot;none&amp;quot;, 
          Colv= NA, dendrogram = &amp;quot;row&amp;quot;,
          hclust=function(x) hclust(x, method=&amp;#39;complete&amp;#39;), distfun=function(x) as.dist(1-cor(t(x))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-19-a-tale-of-two-heatmap-functions_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that the dendrogram can be rotated and without changing the clustering. Check &lt;a href=&#34;https://cran.r-project.org/web/packages/dendsort/index.html&#34;&gt;Dendersort&lt;/a&gt; if you want to specify the order of the dendrogram.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://jokergoo.github.io/ComplexHeatmap-reference/book/&#34;&gt;ComplexHeatmap&lt;/a&gt; package which now I am using mainly &lt;strong&gt;does not&lt;/strong&gt; do any scaling inside the Heatmap function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-messages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Take home messages&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Generate heatmap is easy, but make sure to understand the parameters in each heatmap function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Understand your data and what you are looking for. Do you need to scale your data before clustering?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distance measure and linkage method can drastically affect your clustering. Choose the right one for your data!. Please also read my &lt;a href=&#34;https://rpubs.com/crazyhottommy/heatmap_demystified&#34;&gt;last post&lt;/a&gt; on this theme.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PCA in action</title>
      <link>/post/pca-in-action/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/pca-in-action/</guid>
      <description>&lt;div id=&#34;pca-in-practice.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PCA in practice.&lt;/h3&gt;
&lt;p&gt;Principle Component Analysis(PCA) is a very important skill for dimention reduction to analyze high-dimentional data. High-dimentional data are data with features (p) a lot more than observations (n). This types of data are very commonly generated from high-throuput sequencing experiments. For example, an RNA-seq or microarry experiment measures expression of tens of thousands of genes for only 8 samples (4 controls and 4 treatments).&lt;/p&gt;
&lt;p&gt;Let’s use a microarray data for demonstration. One thing to note is that in linear algebra, a matrix is coded as &lt;code&gt;n (rows are observations) X p (columns are features)&lt;/code&gt;.However, in the microarray/RNA-seq case, the matrix is represented as &lt;code&gt;p (rows are features/genes) X n (columns are observations/samples)&lt;/code&gt;. That’s why we need to transpose the matrix before feeding the matrix to &lt;code&gt;prcomp&lt;/code&gt; or &lt;code&gt;SVD&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ISLR)

# transpose the data
ncidat&amp;lt;- t(NCI60$data)
colnames(ncidat)&amp;lt;- NCI60$labs

dim(ncidat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6830   64&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## it is a data matrix with 64 columns (different tissues) and 6830 rows (genes)
ncidat[1:6,1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      CNS      CNS    CNS         RENAL BREAST    CNS
## 1  0.300 0.679961  0.940  2.800000e-01  0.485  0.310
## 2  1.180 1.289961 -0.040 -3.100000e-01 -0.465 -0.030
## 3  0.550 0.169961 -0.170  6.800000e-01  0.395 -0.100
## 4  1.140 0.379961 -0.040 -8.100000e-01  0.905 -0.460
## 5 -0.265 0.464961 -0.605  6.250000e-01  0.200 -0.205
## 6 -0.070 0.579961  0.000 -1.387779e-17 -0.005 -0.540&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;use-prcomp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use prcomp&lt;/h3&gt;
&lt;p&gt;The default R package stats comes with function &lt;code&gt;prcomp()&lt;/code&gt; to perform principal component analysis. This means that we don’t need to install anything (although there are other options using external packages).&lt;/p&gt;
&lt;p&gt;We take the transpose of ncidat because &lt;strong&gt;&lt;code&gt;prcomp&lt;/code&gt; assumes: units/samples in row and features (genes) in columns&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## please look at the help page to see the meanings of  center and scale. parameters.
## center and scale can affect the result a lot. Usually we center the data.

pca_prcomp&amp;lt;- prcomp(t(ncidat), center = TRUE, scale. = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How much variantion is explained by each component:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pca_prcomp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-18-pca-in-action_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pca_prcomp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Importance of components:
##                            PC1      PC2      PC3      PC4      PC5
## Standard deviation     25.1638 18.78637 16.73078 13.53082 12.78895
## Proportion of Variance  0.1489  0.08301  0.06584  0.04306  0.03847
## Cumulative Proportion   0.1489  0.23194  0.29777  0.34083  0.37930
##                             PC6      PC7      PC8      PC9    PC10   PC11
## Standard deviation     12.21052 11.05840 10.94492 10.59140 9.57657 9.4493
## Proportion of Variance  0.03507  0.02876  0.02817  0.02638 0.02157 0.0210
## Cumulative Proportion   0.41437  0.44313  0.47130  0.49769 0.51926 0.5403
##                           PC12   PC13    PC14    PC15   PC16    PC17
## Standard deviation     9.22659 8.8220 8.66863 8.43185 8.2746 8.19031
## Proportion of Variance 0.02002 0.0183 0.01767 0.01672 0.0161 0.01578
## Cumulative Proportion  0.56028 0.5786 0.59626 0.61298 0.6291 0.64486
##                           PC18    PC19    PC20    PC21    PC22   PC23
## Standard deviation     7.86272 7.84561 7.74753 7.64167 7.41462 7.3207
## Proportion of Variance 0.01454 0.01448 0.01412 0.01373 0.01293 0.0126
## Cumulative Proportion  0.65940 0.67388 0.68799 0.70173 0.71466 0.7273
##                           PC24   PC25    PC26    PC27   PC28    PC29
## Standard deviation     7.09512 7.0817 6.86791 6.71857 6.6190 6.57295
## Proportion of Variance 0.01184 0.0118 0.01109 0.01062 0.0103 0.01016
## Cumulative Proportion  0.73910 0.7509 0.76199 0.77261 0.7829 0.79307
##                           PC30    PC31    PC32    PC33    PC34    PC35
## Standard deviation     6.50142 6.38411 6.31688 6.10274 6.07004 5.96433
## Proportion of Variance 0.00994 0.00959 0.00938 0.00876 0.00867 0.00837
## Cumulative Proportion  0.80302 0.81260 0.82199 0.83075 0.83941 0.84778
##                           PC36    PC37    PC38    PC39    PC40   PC41
## Standard deviation     5.93322 5.87286 5.82866 5.67923 5.63268 5.5707
## Proportion of Variance 0.00828 0.00811 0.00799 0.00759 0.00746 0.0073
## Cumulative Proportion  0.85606 0.86417 0.87216 0.87975 0.88721 0.8945
##                           PC42   PC43    PC44    PC45   PC46    PC47
## Standard deviation     5.51265 5.4555 5.37942 5.32142 5.1743 5.14470
## Proportion of Variance 0.00715 0.0070 0.00681 0.00666 0.0063 0.00623
## Cumulative Proportion  0.90165 0.9086 0.91546 0.92212 0.9284 0.93464
##                           PC48    PC49    PC50    PC51   PC52    PC53
## Standard deviation     5.01790 4.82436 4.77879 4.69168 4.5637 4.49039
## Proportion of Variance 0.00592 0.00547 0.00537 0.00518 0.0049 0.00474
## Cumulative Proportion  0.94057 0.94604 0.95141 0.95659 0.9615 0.96623
##                           PC54   PC55    PC56    PC57    PC58    PC59
## Standard deviation     4.41142 4.2741 4.21355 4.08613 3.91956 3.78810
## Proportion of Variance 0.00458 0.0043 0.00418 0.00393 0.00361 0.00337
## Cumulative Proportion  0.97081 0.9751 0.97928 0.98320 0.98682 0.99019
##                           PC60    PC61    PC62   PC63      PC64
## Standard deviation     3.52405 3.22882 3.15278 2.9856 1.341e-14
## Proportion of Variance 0.00292 0.00245 0.00234 0.0021 0.000e+00
## Cumulative Proportion  0.99311 0.99557 0.99790 1.0000 1.000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#sdev refers to the standard deviation of principal components.
pca_prcomp$sdev&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 2.516378e+01 1.878637e+01 1.673078e+01 1.353082e+01 1.278895e+01
##  [6] 1.221052e+01 1.105840e+01 1.094492e+01 1.059140e+01 9.576574e+00
## [11] 9.449313e+00 9.226590e+00 8.821954e+00 8.668634e+00 8.431849e+00
## [16] 8.274578e+00 8.190308e+00 7.862721e+00 7.845612e+00 7.747529e+00
## [21] 7.641665e+00 7.414624e+00 7.320674e+00 7.095120e+00 7.081674e+00
## [26] 6.867907e+00 6.718573e+00 6.618968e+00 6.572955e+00 6.501420e+00
## [31] 6.384107e+00 6.316878e+00 6.102743e+00 6.070035e+00 5.964333e+00
## [36] 5.933221e+00 5.872856e+00 5.828663e+00 5.679232e+00 5.632684e+00
## [41] 5.570718e+00 5.512649e+00 5.455510e+00 5.379416e+00 5.321422e+00
## [46] 5.174272e+00 5.144699e+00 5.017899e+00 4.824356e+00 4.778789e+00
## [51] 4.691679e+00 4.563740e+00 4.490394e+00 4.411423e+00 4.274070e+00
## [56] 4.213548e+00 4.086132e+00 3.919560e+00 3.788098e+00 3.524054e+00
## [61] 3.228818e+00 3.152782e+00 2.985601e+00 1.341106e-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## variance explained by each PC cumulatively
cumsum(pca_prcomp$sdev^2)/sum(pca_prcomp$sdev^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.1489294 0.2319364 0.2977720 0.3408323 0.3793002 0.4143671 0.4431287
##  [8] 0.4713030 0.4976867 0.5192567 0.5402571 0.5602793 0.5785838 0.5962576
## [15] 0.6129791 0.6290826 0.6448598 0.6594001 0.6738773 0.6879947 0.7017289
## [22] 0.7146592 0.7272638 0.7391037 0.7508988 0.7619925 0.7726091 0.7829132
## [29] 0.7930745 0.8030158 0.8126016 0.8219866 0.8307461 0.8394120 0.8477786
## [36] 0.8560582 0.8641702 0.8721606 0.8797465 0.8872086 0.8945074 0.9016548
## [43] 0.9086548 0.9154609 0.9221211 0.9284180 0.9346431 0.9405652 0.9460392
## [50] 0.9514103 0.9565874 0.9614860 0.9662284 0.9708055 0.9751019 0.9792776
## [57] 0.9832045 0.9868178 0.9901928 0.9931137 0.9955657 0.9979035 1.0000000
## [64] 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;what’s in the prca_prcomp object&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(pca_prcomp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;sdev&amp;quot;     &amp;quot;rotation&amp;quot; &amp;quot;center&amp;quot;   &amp;quot;scale&amp;quot;    &amp;quot;x&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## the first two PCs for the first 6 tissues
head(pca_prcomp$x[,1:2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              PC1         PC2
## CNS    -19.79578   0.1152691
## CNS    -21.54610  -1.4573503
## CNS    -25.05662   1.5260929
## RENAL  -37.40954 -11.3894784
## BREAST -50.21864  -1.3461737
## CNS    -26.43520   0.4629819&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PC1_and_PC2&amp;lt;- data.frame(PC1=pca_prcomp$x[,1], PC2= pca_prcomp$x[,2], type = rownames(pca_prcomp$x))

## plot PCA plot
library(ggplot2)
ggplot(PC1_and_PC2, aes(x=PC1, y=PC2, col=type)) + geom_point() + geom_text(aes(label = type), hjust=0, vjust=0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-18-pca-in-action_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#This returns each principal components loadings
pca_prcomp$rotation[1:6, 1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            PC1           PC2         PC3          PC4          PC5
## 1 -0.005096247  0.0009839929 0.002116058  0.007628801 -0.012118316
## 2 -0.001642354  0.0034355664 0.008049350  0.004910196 -0.007412249
## 3 -0.002509243 -0.0015838271 0.004746350  0.008769557 -0.012426296
## 4  0.004940063  0.0078435347 0.013716214  0.011378816 -0.014851587
## 5 -0.003365039 -0.0002680479 0.010677453 -0.005249648 -0.003317312
## 6  0.001382038 -0.0034431320 0.003167842  0.007425971 -0.002879251
##             PC6
## 1  0.0061392242
## 2  0.0114429879
## 3 -0.0002860562
## 4 -0.0065009935
## 5 -0.0003513787
## 6  0.0003210365&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;use-singluar-value-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use singluar value decomposition&lt;/h3&gt;
&lt;p&gt;in a svd analysis, a matrix n x p matrix X is decomposed by &lt;code&gt;X = U*D*V&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;1.U is an m×n orthogonal matrix.&lt;br /&gt;
2.V is an n×n orthogonal matrix.&lt;br /&gt;
3.D is an n×n diagonal matrix.&lt;/p&gt;
&lt;p&gt;PCs: &lt;strong&gt;Z = XV or Z = UD (U are un-scaled PCs)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some facts of PCA:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;k th column of Z, Z(k), is the k th PC.(the k th pattern)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;PC loadings: V k th column of V, V(k) is the k th PC loading (feature weights). aka. &lt;strong&gt;the k th column of V encodes the associated k th pattern in feature space.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;PC loadings: U k th column of U, U(k) is the k th PC loading (observation weights). aka. &lt;strong&gt;the k th column of U encodes the associated k th pattern in observation space.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Diagnal matrix: D diagnals in D: &lt;strong&gt;d(k) gives the strength of the k th pattern.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Variance explained by k th PC: d(k)^2 Total variance of the data: &lt;code&gt;sum(d(k1)^2 + d(k2)^2 + …..d(k)^2+….)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;proportion of variane explained by k th PC: &lt;code&gt;d(k)^2 / sum(d(k1)^2 + d(k2)^2 + …..d(k)^2+….)&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X&amp;lt;- t(scale(t(ncidat),center=TRUE,scale=FALSE))
# we transpose X again for svd
# usually there is no need to transpose the matrix and then transpose it back, but svd was written that rows are observations and columns are 
# features.however, most genomic data represent observations (e.g. samples) in columns and features (e.g. genes) in columns.

sv&amp;lt;- svd(t(X))
U&amp;lt;- sv$u
V&amp;lt;- sv$v
D&amp;lt;- sv$d


## U are un-scaled PC, Z is scaled
Z&amp;lt;- t(X)%*%V

## PCs
Z[1:6, 1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]        [,2]       [,3]      [,4]       [,5]     [,6]
## CNS    -19.79578   0.1152691  -5.968917  4.753293  -4.882164 18.92591
## CNS    -21.54610  -1.4573503  -9.019584  6.767942  -2.247604 17.07273
## CNS    -25.05662   1.5260929  -6.959653  2.785913 -10.819648 16.45389
## RENAL  -37.40954 -11.3894784  -5.407097 15.442094 -16.011475 33.09651
## BREAST -50.21864  -1.3461737 -17.599944 15.099862 -13.852847 16.94340
## CNS    -26.43520   0.4629819 -16.931456 11.389195  -6.742920 11.85838&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## is the same as
pca_prcomp$x[1:6, 1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              PC1         PC2        PC3       PC4        PC5      PC6
## CNS    -19.79578   0.1152691  -5.968917  4.753293  -4.882164 18.92591
## CNS    -21.54610  -1.4573503  -9.019584  6.767942  -2.247604 17.07273
## CNS    -25.05662   1.5260929  -6.959653  2.785913 -10.819648 16.45389
## RENAL  -37.40954 -11.3894784  -5.407097 15.442094 -16.011475 33.09651
## BREAST -50.21864  -1.3461737 -17.599944 15.099862 -13.852847 16.94340
## CNS    -26.43520   0.4629819 -16.931456 11.389195  -6.742920 11.85838&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## PC loadings
V[1:6, 1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              [,1]          [,2]        [,3]         [,4]         [,5]
## [1,] -0.005096247  0.0009839929 0.002116058  0.007628801 -0.012118316
## [2,] -0.001642354  0.0034355664 0.008049350  0.004910196 -0.007412249
## [3,] -0.002509243 -0.0015838271 0.004746350  0.008769557 -0.012426296
## [4,]  0.004940063  0.0078435347 0.013716214  0.011378816 -0.014851587
## [5,] -0.003365039 -0.0002680479 0.010677453 -0.005249648 -0.003317312
## [6,]  0.001382038 -0.0034431320 0.003167842  0.007425971 -0.002879251
##               [,6]
## [1,]  0.0061392242
## [2,]  0.0114429879
## [3,] -0.0002860562
## [4,] -0.0065009935
## [5,] -0.0003513787
## [6,]  0.0003210365&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## is the same as 
pca_prcomp$rotation[1:6, 1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            PC1           PC2         PC3          PC4          PC5
## 1 -0.005096247  0.0009839929 0.002116058  0.007628801 -0.012118316
## 2 -0.001642354  0.0034355664 0.008049350  0.004910196 -0.007412249
## 3 -0.002509243 -0.0015838271 0.004746350  0.008769557 -0.012426296
## 4  0.004940063  0.0078435347 0.013716214  0.011378816 -0.014851587
## 5 -0.003365039 -0.0002680479 0.010677453 -0.005249648 -0.003317312
## 6  0.001382038 -0.0034431320 0.003167842  0.007425971 -0.002879251
##             PC6
## 1  0.0061392242
## 2  0.0114429879
## 3 -0.0002860562
## 4 -0.0065009935
## 5 -0.0003513787
## 6  0.0003210365&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot PC1 vs PC2

pc_dat&amp;lt;- data.frame(type = rownames(Z), PC1 = Z[,1], PC2= Z[,2])
ggplot(pc_dat,aes(x=PC1, y=PC2, col=type)) + geom_point() + geom_text(aes(label = type), hjust=0, vjust=0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-18-pca-in-action_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We get the same results as from the &lt;code&gt;prcomp&lt;/code&gt; function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-explained-for-each-pc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variance explained for each PC&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;varex = 0
cumvar = 0
denom = sum(D^2)
for(i in 1:length(D)){
  varex[i] = D[i]^2/denom
  cumvar[i] = sum(D[1:i]^2)/denom
}

## variance explained by each PC cumulatively
cumvar&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.1489294 0.2319364 0.2977720 0.3408323 0.3793002 0.4143671 0.4431287
##  [8] 0.4713030 0.4976867 0.5192567 0.5402571 0.5602793 0.5785838 0.5962576
## [15] 0.6129791 0.6290826 0.6448598 0.6594001 0.6738773 0.6879947 0.7017289
## [22] 0.7146592 0.7272638 0.7391037 0.7508988 0.7619925 0.7726091 0.7829132
## [29] 0.7930745 0.8030158 0.8126016 0.8219866 0.8307461 0.8394120 0.8477786
## [36] 0.8560582 0.8641702 0.8721606 0.8797465 0.8872086 0.8945074 0.9016548
## [43] 0.9086548 0.9154609 0.9221211 0.9284180 0.9346431 0.9405652 0.9460392
## [50] 0.9514103 0.9565874 0.9614860 0.9662284 0.9708055 0.9751019 0.9792776
## [57] 0.9832045 0.9868178 0.9901928 0.9931137 0.9955657 0.9979035 1.0000000
## [64] 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is the same as the result of &lt;code&gt;cumsum(pca_prcomp$sdev^2)/sum(pca_prcomp$sdev^2)&lt;/code&gt; above.&lt;/p&gt;
&lt;p&gt;Screeplot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,2))
plot(1:length(D),varex,type=&amp;quot;l&amp;quot;,lwd=2,xlab=&amp;quot;PC&amp;quot;,ylab=&amp;quot;% Variance Explained&amp;quot;)
plot(1:length(D),cumvar,type=&amp;quot;l&amp;quot;,lwd=2,xlab=&amp;quot;PC&amp;quot;,ylab=&amp;quot;Cummulative Variance Explained&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-18-pca-in-action_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Merge featureCount table from RNAseq</title>
      <link>/post/merge-featurecount-table-from-rnaseq/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/merge-featurecount-table-from-rnaseq/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://bioinf.wehi.edu.au/featureCounts/&#34; target=&#34;_blank&#34;&gt;featureCounts&lt;/a&gt; is a program to fast summarize counts from sequencing data. I use it to get gene-level RNAseq counts by&lt;/p&gt;

&lt;p&gt;&lt;code&gt;featureCounts -p -t exon -g gene_id -a annotation.gtf -o mysample_featureCount.txt mapping_results_PE.bam&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you have a lot of samples, you will get a lot of &lt;code&gt;*featureCount.txt&lt;/code&gt; and you will
need to merge them for downstream analysis.&lt;/p&gt;

&lt;p&gt;I will show you how to merge the tables using &lt;code&gt;R&lt;/code&gt;, &lt;code&gt;python&lt;/code&gt; and &lt;code&gt;unix&lt;/code&gt; below.&lt;/p&gt;

&lt;h4 id=&#34;r-solution&#34;&gt;R solution&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(purrr)
library(tidyverse)
f_files&amp;lt;- list.files(&amp;quot;results/superEnhancer/rna_expression/MSTC&amp;quot;, pattern = &amp;quot;featureCount.txt&amp;quot;, full.names = T)

read_in_feature_counts&amp;lt;- function(file){
        cnt&amp;lt;- read_tsv(file, col_names =T, comment = &amp;quot;#&amp;quot;)
        cnt&amp;lt;- cnt %&amp;gt;% dplyr::select(-Chr, -Start, -End, -Strand, -Length)
        return(cnt)
}
        
raw_counts&amp;lt;- map(f_files, read_in_feature_counts)
raw_counts_df&amp;lt;- purrr::reduce(raw_counts, inner_join) 
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;python-solution&#34;&gt;python solution&lt;/h4&gt;

&lt;p&gt;I am still very crude with python :)
It works for python2.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import csv
import glob

## after some google https://mail.python.org/pipermail/tutor/2004-November/033475.html
## The idea is to keep the count column into a list.

files = glob.glob(&amp;quot;*featureCount.txt&amp;quot;)
list_column = []
n = 1
for file in files:
    print n
    column_data = []
    with open(file, &#39;r&#39;) as f:
        reader = csv.reader(f, delimiter = &amp;quot;\t&amp;quot;)
            # skip the comment line
        comment = next(reader)
        if n &amp;lt;= 1:
            for row in reader:
                # for the first file, keep the gene column as well
                column_data.append(row[0] + &#39;\t&#39; + row[6])
        else:
            for row in reader:
                column_data.append(row[6])
        n = n + 1
    list_column.append(column_data)


# This creates a list of row lists from the list of column lists
# If any of the column lists are too short they will be padded with None
# map function is a gem :)
rows = map(None, *list_column)

with open(&#39;output.txt&#39;,&#39;w&#39;) as f_out:
     for row in rows:
         f_out.write(&#39;\t&#39;.join(row))
         f_out.write(&#39;\n&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;unix-command-line-solution&#34;&gt;unix command line solution&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# get the count
ls -1  *featureCount.txt | parallel &#39;cat {} | sed &#39;1d&#39; | cut -f7 {} &amp;gt; {/.}_clean.txt&#39; 
ls -1  *featureCount.txt | head -1 | xargs cut -f1 &amp;gt; genes.txt
paste genes.txt *featureCount_clean.txt &amp;gt; output.txt

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Three gotchas when using R for Genomic data analysis</title>
      <link>/post/three-gotchas-when-using-r-for-genomic-data-analysis/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/three-gotchas-when-using-r-for-genomic-data-analysis/</guid>
      <description>

&lt;p&gt;During my daily work with R for genomic data analysis, I encountered several instances that R gives me some (bad) surprises.&lt;/p&gt;

&lt;h4 id=&#34;1-the-devil-1-and-0-coordinate-system&#34;&gt;1. The devil 1 and 0 coordinate system&lt;/h4&gt;

&lt;p&gt;read detail here &lt;a href=&#34;https://github.com/crazyhottommy/DNA-seq-analysis#tips-and-lessons-learned-during-my-dna-seq-data-analysis-journey&#34; target=&#34;_blank&#34;&gt;https://github.com/crazyhottommy/DNA-seq-analysis#tips-and-lessons-learned-during-my-dna-seq-data-analysis-journey&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;some files such as &lt;code&gt;bed&lt;/code&gt; file is 0 based. Two genomic regions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chr1    0    1000
chr1    1001    2000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;when you import that bed file into R using &lt;code&gt;rtracklayer::import()&lt;/code&gt;, it will become&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;chr1     1    1000
chr1    1002    2000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The function convert it to 1 based internally (R is 1 based unlike python).&lt;/p&gt;

&lt;p&gt;The problem is that when you read the bed file with &lt;code&gt;read.table&lt;/code&gt; and use &lt;code&gt;GenomicRanges::makeGRangesFromDataFrame()&lt;/code&gt; to convert it to a GRanges object, do not forget to add 1 to the start before doing it.&lt;/p&gt;

&lt;p&gt;Similarily, when you write a GRanges object to disk using &lt;code&gt;rtracklayer::export&lt;/code&gt;, you do not need to worry, R will convert it back to 0 based in file.&lt;/p&gt;

&lt;p&gt;However, if you make a dataframe out of the GRanges object, you need to remember do &lt;code&gt;start -1&lt;/code&gt; before writing to a file.&lt;/p&gt;

&lt;h4 id=&#34;2-read-tsv-column-types&#34;&gt;2. &lt;code&gt;read_tsv&lt;/code&gt; column types&lt;/h4&gt;

&lt;p&gt;If you use &lt;code&gt;read_tsv&lt;/code&gt; from &lt;a href=&#34;https://github.com/tidyverse/readr&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;readr&lt;/code&gt;&lt;/a&gt;, it will use the first 1000 rows to determine the column types (integer, charater etc). For genomic data, however, especially for the chromosome column, you may or may not have &lt;code&gt;chr&lt;/code&gt; prepending.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1    0    1000    
1    1000    2000
.
.
.
X
Y
MT

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;you may fail to read rows for chromosome X, Y and MT. (To make things worse, UCSC uses chrM rather than chrMT&amp;hellip;)&lt;/p&gt;

&lt;p&gt;The solution is that read in all the data as characters.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(readr)
challenge2 &amp;lt;- read_tsv(&amp;quot;my.bed&amp;quot;, 
  col_types = cols(.default = col_character())
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;see &lt;a href=&#34;http://r4ds.had.co.nz/data-import.html&#34; target=&#34;_blank&#34;&gt;http://r4ds.had.co.nz/data-import.html&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-scientific-notation-for-genomic-coordinates&#34;&gt;3. Scientific notation for genomic coordinates&lt;/h4&gt;

&lt;p&gt;This is kind of related to 2. &lt;code&gt;1200000&lt;/code&gt; will be written as &lt;code&gt;1.2e6&lt;/code&gt; in a dataframe if R thinks it is an integer. So, you will need to read in the columns all as characters, or if you convert the character to numeric and wants to write to a file,
add &lt;code&gt;options(scipen=500)&lt;/code&gt; on the top of your script.&lt;/p&gt;

&lt;p&gt;The scientific notation can not be disabled in &lt;code&gt;write_csv&lt;/code&gt;: &lt;a href=&#34;https://github.com/tidyverse/readr/issues/671&#34; target=&#34;_blank&#34;&gt;https://github.com/tidyverse/readr/issues/671&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;one-more-gotcha-for-rownames-and-colnames&#34;&gt;One more gotcha for rownames and colnames&lt;/h4&gt;

&lt;p&gt;base R will change the name with &lt;code&gt;-&lt;/code&gt; to a &lt;code&gt;.&lt;/code&gt;. e.g. TCGA-06-ABCD will be changed to TCGA.06.ABCD. this can cause troubles when you use the name of the columns to match samples. &lt;code&gt;readr&lt;/code&gt; will maintain the &lt;code&gt;-&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>open files on remote with sublime by ssh</title>
      <link>/post/open-files-on-remote-with-sublime-by-ssh/</link>
      <pubDate>Wed, 10 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/open-files-on-remote-with-sublime-by-ssh/</guid>
      <description>

&lt;p&gt;I am still suck at &lt;code&gt;vim&lt;/code&gt; or &lt;code&gt;emacs&lt;/code&gt;. I use &lt;code&gt;nano&lt;/code&gt; to edit files on remote machines. But for more complicated editing, I prefer to use sublime.&lt;/p&gt;

&lt;p&gt;use this &lt;a href=&#34;https://github.com/randy3k/RemoteSubl&#34; target=&#34;_blank&#34;&gt;https://github.com/randy3k/RemoteSubl&lt;/a&gt; for editing remote files.&lt;/p&gt;

&lt;p&gt;Steps:&lt;/p&gt;

&lt;h3 id=&#34;on-remote-machine-install-rmate&#34;&gt;on remote machine, install &lt;code&gt;rmate&lt;/code&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ssh bio1
curl -o ~/bin/rmate https://raw.githubusercontent.com/aurora/rmate/master/rmate

chmod u+x bin/rmate
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;on-your-local-computer-install-remotesubl&#34;&gt;on your local computer, install &lt;code&gt;RemoteSubl&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;on your &lt;strong&gt;local&lt;/strong&gt; computer, open &lt;code&gt;sublime&lt;/code&gt;, click &lt;code&gt;tools&lt;/code&gt; &amp;ndash;&amp;gt; &lt;code&gt;Command Palette&lt;/code&gt; &amp;ndash;&amp;gt; type Package control:Install Package &amp;ndash;&amp;gt; type &lt;code&gt;RemoteSubl&lt;/code&gt; to install.&lt;/p&gt;

&lt;h3 id=&#34;change-your-ssh-config-file&#34;&gt;change your ssh config file&lt;/h3&gt;

&lt;p&gt;add &lt;code&gt;RemoteForward 52698 localhost:52698&lt;/code&gt; to your &lt;code&gt;~/.ssh/config&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Now,  ssh to remote, and you can do &lt;code&gt;rmate my.txt&lt;/code&gt; in your remote and open sublime in your local machine.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
