---
title: 'marker gene selection using logistic regression and regularization for scRNAseq '
author: Ming Tang
date: '2022-05-15'
slug: marker-gene-selection-using-logistic-regression-and-regularization-for-scrnaseq
categories:
  - bioinformatics
  - single-cell
  - machine learning
tags:
  - single-cell
  - bioinformatics
  - machine learning
header:
  caption: ''
  image: ''
editor_options: 
  chunk_output_type: console
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="why-this-blog-post" class="section level3">
<h3>why this blog post?</h3>
<p>I saw a biorxiv paper titled <a href="https://www.biorxiv.org/content/10.1101/2022.05.09.490241v1">A comparison of marker gene selection methods for single-cell RNA sequencing data</a></p>
<blockquote>
<p>Our results highlight the efficacy of simple methods, especially the Wilcoxon rank-sum test, Student‚Äôs t-test and logistic regression</p>
</blockquote>
<p>I am interested in using logistic regression to find marker genes and want to try fitting the model in the <a href="https://www.tidymodels.org/">tidymodel</a> ecosystem and using different regularization methods.</p>
<p>I highly recommend you to watch the <a href="https://app.learney.me/maps/StatQuest?topic=Regularization">statquest videos</a> on those topics.</p>
<p>Let‚Äôs use PBMC single-cell RNAseq data as an example.</p>
<p>Load libraries</p>
<pre class="r"><code>library(Seurat)
library(tidyverse)
library(tidymodels)
library(scCustomize) # for plotting</code></pre>
<p>Preprocess the data</p>
<pre class="r"><code># Load the PBMC dataset
pbmc.data &lt;- Read10X(data.dir = &quot;~/blog_data/filtered_gene_bc_matrices/hg19/&quot;)
# Initialize the Seurat object with the raw (non-normalized data).
pbmc &lt;- CreateSeuratObject(counts = pbmc.data, project = &quot;pbmc3k&quot;, min.cells = 3, min.features = 200)

## routine processing
pbmc&lt;- pbmc %&gt;% 
  NormalizeData(normalization.method = &quot;LogNormalize&quot;, scale.factor = 10000) %&gt;%
  FindVariableFeatures(selection.method = &quot;vst&quot;, nfeatures = 2000) %&gt;%
  ScaleData() %&gt;%
  RunPCA(verbose = FALSE) %&gt;%
  FindNeighbors(dims = 1:10, verbose = FALSE) %&gt;%
  FindClusters(resolution = 0.5, verbose = FALSE) %&gt;%
  RunUMAP(dims = 1:10, verbose = FALSE)

## the annotation borrowed from Seurat tutorial
new.cluster.ids &lt;- c(&quot;Naive CD4 T&quot;, &quot;CD14+ Mono&quot;, &quot;Memory CD4 T&quot;, &quot;B&quot;, &quot;CD8 T&quot;, &quot;FCGR3A+ Mono&quot;, 
    &quot;NK&quot;, &quot;DC&quot;, &quot;Platelet&quot;)
names(new.cluster.ids) &lt;- levels(pbmc)
pbmc &lt;- RenameIdents(pbmc, new.cluster.ids)
pbmc$cell_type&lt;- Idents(pbmc)
DimPlot(pbmc)</code></pre>
<p><img src="/post/2022-05-15-marker-gene-selection-using-logistic-regression-and-regularization-for-scrnaseq.en_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="marker-gene-detection-using-differential-expression-analysis-between-clusters." class="section level3">
<h3>Marker gene detection using differential expression analysis between clusters.</h3>
<pre class="r"><code>pbmc_subset&lt;- pbmc[, pbmc$cell_type %in% c(&quot;NK&quot;, &quot;B&quot;)]
DimPlot(pbmc_subset)</code></pre>
<p><img src="/post/2022-05-15-marker-gene-selection-using-logistic-regression-and-regularization-for-scrnaseq.en_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>Idents(pbmc_subset)&lt;- pbmc_subset$cell_type

diff_markers&lt;- FindMarkers(pbmc_subset, ident.1 = &quot;B&quot;, ident.2 = &quot;NK&quot;, test.use = &quot;wilcox&quot;)

diff_markers %&gt;%
        arrange(p_val_adj, desc(abs(avg_log2FC))) %&gt;%
        head(n = 20)</code></pre>
<pre><code>#&gt;                 p_val avg_log2FC pct.1 pct.2    p_val_adj
#&gt; GZMB     7.646318e-98  -5.553778 0.017 0.966 1.048616e-93
#&gt; GZMA     2.929923e-95  -4.519094 0.011 0.939 4.018097e-91
#&gt; PRF1     9.684091e-95  -4.635264 0.029 0.959 1.328076e-90
#&gt; NKG7     8.032681e-92  -6.310372 0.083 0.986 1.101602e-87
#&gt; CST7     4.498511e-91  -4.389481 0.037 0.946 6.169258e-87
#&gt; CTSW     1.747566e-89  -4.144031 0.066 0.966 2.396612e-85
#&gt; FGFBP2   3.777986e-87  -4.555937 0.034 0.912 5.181130e-83
#&gt; GNLY     8.666212e-85  -6.080298 0.080 0.939 1.188484e-80
#&gt; FCGR3A   1.278458e-79  -3.635596 0.034 0.858 1.753278e-75
#&gt; CD247    1.428149e-77  -3.763002 0.040 0.851 1.958563e-73
#&gt; GZMM     3.440488e-77  -3.248116 0.017 0.818 4.718285e-73
#&gt; CD7      4.578399e-74  -3.330969 0.054 0.851 6.278817e-70
#&gt; TYROBP   2.340536e-73  -3.560921 0.103 0.899 3.209811e-69
#&gt; FCER1G   9.343826e-72  -3.499007 0.069 0.845 1.281412e-67
#&gt; SPON2    7.917147e-70  -3.506623 0.011 0.743 1.085758e-65
#&gt; CD74     1.880522e-69   3.659825 1.000 0.824 2.578947e-65
#&gt; HLA-DRA  2.083409e-69   4.597057 1.000 0.365 2.857187e-65
#&gt; SRGN     1.501958e-68  -3.151188 0.201 0.932 2.059785e-64
#&gt; HLA-DPB1 3.257314e-66   3.595454 0.986 0.345 4.467080e-62
#&gt; HLA-DRB1 2.058185e-65   3.710470 0.980 0.189 2.822594e-61</code></pre>
<p>Note that p-values from this type of analysis are inflated as we clustered the cells first and then find the differences between the clusters. In other words, we double-dip the data.</p>
<p>see slide 27 from my ABRF2022 talk <a href="https://divingintogeneticsandgenomics.rbind.io/files/slides/2022-03-29_single-cell-101.pdf" class="uri">https://divingintogeneticsandgenomics.rbind.io/files/slides/2022-03-29_single-cell-101.pdf</a></p>
</div>
<div id="lets-use-logistic-regression-to-find-marker-genes" class="section level3">
<h3>Let‚Äôs use logistic regression to find marker genes</h3>
<p>We can use logistic regression to classify B-cells and NK cells.</p>
<pre class="r"><code>## re-process the data, as the most-variable genes will change when you only have NK and B cells vs all cells. use log normalized data
pbmc_subset&lt;- pbmc_subset %&gt;% 
  NormalizeData(normalization.method = &quot;LogNormalize&quot;, scale.factor = 10000) %&gt;%
  FindVariableFeatures(selection.method = &quot;vst&quot;, nfeatures = 2000) %&gt;%
  ScaleData() %&gt;%
  RunPCA(verbose = FALSE) %&gt;%
  FindNeighbors(dims = 1:10, verbose = FALSE) %&gt;%
  FindClusters(resolution = 0.1, verbose = FALSE) %&gt;%
  RunUMAP(dims = 1:10, verbose = FALSE)</code></pre>
<p>we use scaled data, because it was z-score transformed, so the highly expressed genes will not dominate the model</p>
<pre class="r"><code>data&lt;- pbmc_subset@assays$RNA@scale.data
# let&#39;s transpose the matrix and make it to a dataframe
dim(data)</code></pre>
<pre><code>#&gt; [1] 2000  497</code></pre>
<pre class="r"><code>data&lt;- t(data) %&gt;%
        as.data.frame()

# now, every row is a cell with 2000 genes/features/predictors. 
dim(data)</code></pre>
<pre><code>#&gt; [1]  497 2000</code></pre>
<pre class="r"><code>## add the cell type/the outcome/y to the dataframe
data$cell_type&lt;- pbmc_subset$cell_type
data$cell_barcode&lt;- rownames(data)
## it is important to turn it to a factor for classification
data$cell_type&lt;- factor(data$cell_type)</code></pre>
</div>
<div id="prepare-the-data" class="section level3">
<h3>prepare the data</h3>
<pre class="r"><code>set.seed(123)

data_split &lt;- initial_split(data, strata = &quot;cell_type&quot;)
data_train &lt;- training(data_split)
data_test &lt;- testing(data_split)

# 10 fold cross validation
data_fold &lt;- vfold_cv(data_train, v = 10)</code></pre>
<p>We will not look at the test data at all until we test our model.</p>
</div>
<div id="ridge-regression" class="section level3">
<h3>Ridge Regression</h3>
<p>In our training data, we have 2000 genes/features (<code>p</code>) and 273 cells/observations (<code>n</code>)
and <code>p</code> &gt;&gt; <code>n</code>, so we will need to enforce sparsity of the model by regularization.</p>
<p>We‚Äôll set the penalty argument to <code>tune()</code> as a placeholder for now. This is a model hyper parameter that we will tune to find the best value for making predictions with our data. Setting mixture to a value of one means that the glmnet model will potentially remove irrelevant predictors and choose a simpler model.</p>
<p>Let‚Äôs use L2 regularization/norm for ridge regression by specifying <code>mixture = 0</code>.
and we need to tune the <code>penalty</code> hyper parameter.</p>
<pre class="r"><code>ridge_spec&lt;-
        logistic_reg(penalty = tune(), mixture = 0) %&gt;%
        set_engine(&quot;glmnet&quot;) %&gt;%
        set_mode(&quot;classification&quot;)

ridge_recipe &lt;- 
  recipe(formula = cell_type ~ ., data = data_train) %&gt;% 
  update_role(cell_barcode, new_role = &quot;ID&quot;)  %&gt;%
  step_zv(all_predictors())

# step_normalize(all_predictors())
## the expression is already scaled, no need to do step_normalize

ridge_workflow &lt;- workflow() %&gt;% 
  add_recipe(ridge_recipe) %&gt;% 
  add_model(ridge_spec)</code></pre>
<p>tune the <code>penalty</code> hyper-parameter.</p>
<pre class="r"><code>penalty_grid &lt;- grid_regular(penalty(range = c(-5, 5)), levels = 50)
#penalty_grid
tune_res &lt;- tune_grid(
  ridge_workflow,
  resamples = data_fold, 
  grid = penalty_grid
)

# tune_res
# check which penalty is the best 
autoplot(tune_res)</code></pre>
<p><img src="/post/2022-05-15-marker-gene-selection-using-logistic-regression-and-regularization-for-scrnaseq.en_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>best_penalty &lt;- select_best(tune_res, metric = &quot;accuracy&quot;)
best_penalty</code></pre>
<pre><code>#&gt; # A tibble: 1 √ó 2
#&gt;   penalty .config              
#&gt;     &lt;dbl&gt; &lt;chr&gt;                
#&gt; 1 0.00001 Preprocessor1_Model01</code></pre>
<pre class="r"><code>ridge_final &lt;- finalize_workflow(ridge_workflow, best_penalty)
ridge_final_fit &lt;- fit(ridge_final, data = data_train)

tidy(ridge_final_fit) %&gt;% arrange(desc(abs(estimate)))</code></pre>
<pre><code>#&gt; # A tibble: 2,000 √ó 3
#&gt;    term        estimate penalty
#&gt;    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 (Intercept)  -1.21   0.00001
#&gt;  2 NKG7          0.0318 0.00001
#&gt;  3 GZMB          0.0307 0.00001
#&gt;  4 HLA-DRA      -0.0307 0.00001
#&gt;  5 CTSW          0.0305 0.00001
#&gt;  6 GNLY          0.0297 0.00001
#&gt;  7 PRF1          0.0296 0.00001
#&gt;  8 GZMA          0.0295 0.00001
#&gt;  9 CD74         -0.0295 0.00001
#&gt; 10 FGFBP2        0.0295 0.00001
#&gt; # ‚Ä¶ with 1,990 more rows</code></pre>
<pre class="r"><code># augment(ridge_final_fit, new_data = data_test) </code></pre>
<p>we see the NKG7, GZMB, CTSW GNLY, PRF1 etc on the top as the marker genes for NK cells which makes sense!</p>
<pre class="r"><code>## confusion matrix
predict(ridge_final_fit, new_data = data_test) %&gt;%
        bind_cols(data_test %&gt;% select(cell_type)) %&gt;%
        conf_mat(truth = cell_type, estimate = .pred_class)</code></pre>
<pre><code>#&gt;           Truth
#&gt; Prediction  B NK
#&gt;         B  88  0
#&gt;         NK  0 37</code></pre>
</div>
<div id="lasso-regression" class="section level3">
<h3>Lasso regression</h3>
<p>You need to use <code>logistic_reg()</code> and set <code>mixture = 1</code> to specify a lasso model. The mixture argument specifies the amount of different types of regularization, <code>mixture = 0</code> specifies only ridge regularization and <code>mixture = 1</code> specifies only lasso regularization.</p>
<p>Lasso will remove highly correlated features which may not be what we want here!</p>
<pre class="r"><code>lasso_spec&lt;-
        logistic_reg(penalty = tune(), mixture = 1) %&gt;%
        set_engine(&quot;glmnet&quot;) %&gt;%
        set_mode(&quot;classification&quot;)

lasso_recipe &lt;- 
  recipe(formula = cell_type ~ ., data = data_train) %&gt;% 
  update_role(cell_barcode, new_role = &quot;ID&quot;)  %&gt;%
  step_zv(all_predictors())

# step_normalize(all_predictors())
## the expression is already scaled, no need to do step_normalize

lasso_workflow &lt;- workflow() %&gt;% 
  add_recipe(lasso_recipe) %&gt;% 
  add_model(lasso_spec)

penalty_grid &lt;- grid_regular(penalty(range = c(-5, 5)), levels = 50)
#penalty_grid
tune_res &lt;- tune_grid(
  lasso_workflow,
  resamples = data_fold, 
  grid = penalty_grid
)

# tune_res
# check which penalty is the best 
autoplot(tune_res)</code></pre>
<p><img src="/post/2022-05-15-marker-gene-selection-using-logistic-regression-and-regularization-for-scrnaseq.en_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>best_penalty &lt;- select_best(tune_res, metric = &quot;accuracy&quot;)
best_penalty</code></pre>
<pre><code>#&gt; # A tibble: 1 √ó 2
#&gt;   penalty .config              
#&gt;     &lt;dbl&gt; &lt;chr&gt;                
#&gt; 1 0.00001 Preprocessor1_Model01</code></pre>
<pre class="r"><code>lasso_final &lt;- finalize_workflow(lasso_workflow, best_penalty)
lasso_final_fit &lt;- fit(lasso_final, data = data_train)

## confusion matrix
predict(lasso_final_fit, new_data = data_test) %&gt;%
        bind_cols(data_test %&gt;% select(cell_type)) %&gt;%
        conf_mat(truth = cell_type, estimate = .pred_class)</code></pre>
<pre><code>#&gt;           Truth
#&gt; Prediction  B NK
#&gt;         B  88  0
#&gt;         NK  0 37</code></pre>
<pre class="r"><code>lasso_features&lt;- tidy(lasso_final_fit) %&gt;% 
        arrange(desc(abs(estimate))) %&gt;%
        filter(estimate != 0) 

print(lasso_features, n = Inf)</code></pre>
<pre><code>#&gt; # A tibble: 21 √ó 3
#&gt;    term        estimate penalty
#&gt;    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 (Intercept) -2.48    0.00001
#&gt;  2 NKG7         1.70    0.00001
#&gt;  3 CTSW         0.582   0.00001
#&gt;  4 GZMA         0.561   0.00001
#&gt;  5 HLA-DRA     -0.561   0.00001
#&gt;  6 CD74        -0.485   0.00001
#&gt;  7 CD79A       -0.423   0.00001
#&gt;  8 CDC6         0.422   0.00001
#&gt;  9 CDKN3        0.251   0.00001
#&gt; 10 GNLY         0.245   0.00001
#&gt; 11 CST7         0.195   0.00001
#&gt; 12 GZMB         0.174   0.00001
#&gt; 13 TYMS         0.155   0.00001
#&gt; 14 PRF1         0.151   0.00001
#&gt; 15 RRM2         0.142   0.00001
#&gt; 16 KIFC1        0.0690  0.00001
#&gt; 17 LTB         -0.0612  0.00001
#&gt; 18 FGFBP2       0.0588  0.00001
#&gt; 19 MGLL         0.0577  0.00001
#&gt; 20 RPL13A      -0.0456  0.00001
#&gt; 21 CDCA8        0.00842 0.00001</code></pre>
<p>Because <code>Lasso regression</code> enforce sparsity, we can select the features with the coefficients/estimates not equal to 0, but we will miss the correlated genes which were thrown out by the model.</p>
<p>Also, if you are wondering how can I get a p-value for the coefficient, read <a href="https://stats.stackexchange.com/questions/410173/lasso-regression-p-values-and-coefficients" class="uri">https://stats.stackexchange.com/questions/410173/lasso-regression-p-values-and-coefficients</a></p>
</div>
<div id="elastic-net" class="section level3">
<h3>Elastic-net</h3>
<p>Setting mixture to a value between 0 and 1 lets us use both Lasso and Ridge (Elastic-net!!)</p>
<p>Elastic-net gets the best world of both Lasso and Ridge.
Thanks, <a href="https://twitter.com/Matthew_N_B">Matt Bernstein</a> for pointing the original elastic-net paper to me!</p>
<p><a href="https://hastie.su.domains/Papers/elasticnet.pdf" class="uri">https://hastie.su.domains/Papers/elasticnet.pdf</a></p>
<blockquote>
<p>In addition, the elastic net encourages a
grouping effect, where strongly correlated predictors tend to be in
(out) the model together. The elastic net is particularly useful when
the number of predictors (p) is much bigger than the number of observations (n)</p>
</blockquote>
<pre class="r"><code>elastic_recipe &lt;- 
  recipe(formula = cell_type ~ ., data = data_train) %&gt;% 
  update_role(cell_barcode, new_role = &quot;ID&quot;) %&gt;%
  step_zv(all_predictors())

# we will tune both penalty and the mixture
elastic_spec &lt;- 
  logistic_reg(penalty = tune(), mixture = tune()) %&gt;% 
  set_engine(&quot;glmnet&quot;) %&gt;%
  set_mode(&quot;classification&quot;)

elastic_workflow &lt;- workflow() %&gt;% 
  add_recipe(elastic_recipe) %&gt;% 
  add_model(elastic_spec)

## tune both the penalty and the mixture from 0-1
penalty_grid &lt;- grid_regular(penalty(range = c(-2, 2)), mixture(), levels = 50)

doParallel::registerDoParallel()
tune_res &lt;- tune_grid(
  elastic_workflow,
  resamples = data_fold, 
  grid = penalty_grid
)

autoplot(tune_res)</code></pre>
<p><img src="/post/2022-05-15-marker-gene-selection-using-logistic-regression-and-regularization-for-scrnaseq.en_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>best_penalty &lt;- select_best(tune_res, metric = &quot;accuracy&quot;)
elastic_final &lt;- finalize_workflow(elastic_workflow, best_penalty)
elastic_final_fit &lt;- fit(elastic_final, data = data_train)

## confusion matrix
predict(elastic_final_fit, new_data = data_test) %&gt;%
        bind_cols(data_test %&gt;% select(cell_type)) %&gt;%
        conf_mat(truth = cell_type, estimate = .pred_class)</code></pre>
<pre><code>#&gt;           Truth
#&gt; Prediction  B NK
#&gt;         B  88  0
#&gt;         NK  0 37</code></pre>
<pre class="r"><code>elastic_features&lt;- tidy(elastic_final_fit) %&gt;%
  arrange(desc(abs(estimate))) %&gt;%
  filter(estimate != 0) 

head(elastic_features, n = 20)</code></pre>
<pre><code>#&gt; # A tibble: 20 √ó 3
#&gt;    term        estimate penalty
#&gt;    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 (Intercept)  -1.84      0.01
#&gt;  2 NKG7          0.105     0.01
#&gt;  3 CTSW          0.100     0.01
#&gt;  4 HLA-DRA      -0.0993    0.01
#&gt;  5 GZMB          0.0971    0.01
#&gt;  6 GNLY          0.0971    0.01
#&gt;  7 CD74         -0.0949    0.01
#&gt;  8 GZMA          0.0940    0.01
#&gt;  9 FGFBP2        0.0932    0.01
#&gt; 10 CST7          0.0929    0.01
#&gt; 11 PRF1          0.0922    0.01
#&gt; 12 CD79A        -0.0901    0.01
#&gt; 13 GZMM          0.0867    0.01
#&gt; 14 LTB          -0.0838    0.01
#&gt; 15 LRRIQ3        0.0808    0.01
#&gt; 16 CD247         0.0792    0.01
#&gt; 17 TYROBP        0.0778    0.01
#&gt; 18 CD7           0.0766    0.01
#&gt; 19 FCGR3A        0.0765    0.01
#&gt; 20 FCER1G        0.0712    0.01</code></pre>
<pre class="r"><code>head(lasso_features, n = 20)</code></pre>
<pre><code>#&gt; # A tibble: 20 √ó 3
#&gt;    term        estimate penalty
#&gt;    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
#&gt;  1 (Intercept)  -2.48   0.00001
#&gt;  2 NKG7          1.70   0.00001
#&gt;  3 CTSW          0.582  0.00001
#&gt;  4 GZMA          0.561  0.00001
#&gt;  5 HLA-DRA      -0.561  0.00001
#&gt;  6 CD74         -0.485  0.00001
#&gt;  7 CD79A        -0.423  0.00001
#&gt;  8 CDC6          0.422  0.00001
#&gt;  9 CDKN3         0.251  0.00001
#&gt; 10 GNLY          0.245  0.00001
#&gt; 11 CST7          0.195  0.00001
#&gt; 12 GZMB          0.174  0.00001
#&gt; 13 TYMS          0.155  0.00001
#&gt; 14 PRF1          0.151  0.00001
#&gt; 15 RRM2          0.142  0.00001
#&gt; 16 KIFC1         0.0690 0.00001
#&gt; 17 LTB          -0.0612 0.00001
#&gt; 18 FGFBP2        0.0588 0.00001
#&gt; 19 MGLL          0.0577 0.00001
#&gt; 20 RPL13A       -0.0456 0.00001</code></pre>
<pre class="r"><code>merged_markers&lt;- left_join(elastic_features, lasso_features, by = c(&quot;term&quot; = &quot;term&quot;)) %&gt;%
        dplyr::rename(estimate.elastic = estimate.x, estimate.lasso= estimate.y) %&gt;%
        select(-penalty.x, - penalty.y) 

head(merged_markers, n = 20)</code></pre>
<pre><code>#&gt; # A tibble: 20 √ó 3
#&gt;    term        estimate.elastic estimate.lasso
#&gt;    &lt;chr&gt;                  &lt;dbl&gt;          &lt;dbl&gt;
#&gt;  1 (Intercept)          -1.84          -2.48  
#&gt;  2 NKG7                  0.105          1.70  
#&gt;  3 CTSW                  0.100          0.582 
#&gt;  4 HLA-DRA              -0.0993        -0.561 
#&gt;  5 GZMB                  0.0971         0.174 
#&gt;  6 GNLY                  0.0971         0.245 
#&gt;  7 CD74                 -0.0949        -0.485 
#&gt;  8 GZMA                  0.0940         0.561 
#&gt;  9 FGFBP2                0.0932         0.0588
#&gt; 10 CST7                  0.0929         0.195 
#&gt; 11 PRF1                  0.0922         0.151 
#&gt; 12 CD79A                -0.0901        -0.423 
#&gt; 13 GZMM                  0.0867        NA     
#&gt; 14 LTB                  -0.0838        -0.0612
#&gt; 15 LRRIQ3                0.0808        NA     
#&gt; 16 CD247                 0.0792        NA     
#&gt; 17 TYROBP                0.0778        NA     
#&gt; 18 CD7                   0.0766        NA     
#&gt; 19 FCGR3A                0.0765        NA     
#&gt; 20 FCER1G                0.0712        NA</code></pre>
<p>we see GZMM is missed by Lasso but picked up by elastic-net. GZMM is probably highly correlated with other granzymes such as GZMB, GZMA which are already in the model.</p>
<p>FCRR3A (CD16) picked up as an NK cell marker by elastic too.</p>
<p>From this <a href="https://ashpublications.org/bloodadvances/article/4/7/1388/454300/Diversity-of-peripheral-blood-human-NK-cells">paper</a>. CD16 is a pretty important marker for NK cell subytyping. It is sad that Lasso missed it.</p>
<p>Let‚Äôs plot the gene expression level to check the raw data.</p>
<pre class="r"><code>Idents(pbmc_subset)&lt;- pbmc_subset$cell_type
scCustomize::Stacked_VlnPlot(pbmc_subset, features = merged_markers %&gt;% slice(-1) %&gt;% pull(term) %&gt;% head(n = 20),
                             colors_use = c(&quot;blue&quot;, &quot;red&quot;) )</code></pre>
<p><img src="/post/2022-05-15-marker-gene-selection-using-logistic-regression-and-regularization-for-scrnaseq.en_files/figure-html/unnamed-chunk-13-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Many of the top genes picked up by elastic-net are very convincing to me. LRRIQ3 is low in both cell types.</p>
</div>
<div id="things-learned" class="section level3">
<h3>Things learned</h3>
<ul>
<li><p>the tidymodel ecosystem makes the interface to different machine learning methods consistent. You can tell how similar the codes are for all three regularization methods (Time to write a function!).</p></li>
<li><p>classification for B cells and NK cells are petty easy task as they are very different. All models perform well. Again, the purpose here is not to use the model to do prediction, but use the model to do feature selection.</p></li>
<li><p>Seurat has an <a href="https://github.com/satijalab/seurat/blob/f1b2593ea72f2e6d6b16470dc7b9e9b645179923/R/differential_expression.R#L1670">implementation</a> of the logistic regression for marker gene selection: <code>glm(formula = fmla, data = model.data, family = "binomial")</code></p></li>
</ul>
<p>Other readings:</p>
<ul>
<li><a href="https://www.tmwr.org/">Tidy Modeling with R</a></li>
<li><a href="https://stats.stackexchange.com/questions/108614/regression-in-pn-setting-how-to-choose-regularization-method-lasso-pls-pc">Regression in ùëù&gt;ùëõ setting: how to choose regularization method (Lasso, PLS, PCR, ridge)?</a></li>
<li><a href="https://towardsdatascience.com/bias-variance-and-regularization-in-linear-regression-lasso-ridge-and-elastic-net-8bf81991d0c5">Bias, Variance, and Regularization in Linear Regression: Lasso, Ridge, and Elastic Net ‚Äî Differences and uses</a></li>
<li><a href="https://uc-r.github.io/regularized_regression">Regularized Regression</a></li>
</ul>
</div>
